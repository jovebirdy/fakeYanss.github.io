<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[MacOS-tomcat启动80端口]]></title>
    <url>%2Farchives%2F9b118094.html</url>
    <content type="text"><![CDATA[Mac OS上非root用户是不允许启动小于1024的端口，所以在mac上使用IDE开发，比如在Eclipse或者Intellij中，启动服务用80端口，会报错Permission Denied。可以通过修改Tomcat的用户为root来解决，但是更简单的是设置一个端口监听转发，将其他端口比如8080转发到80，这样在浏览器中输入地址就不用写端口号了。 命令： sudo vim /etc/pf.conf 在rdr-anchor &quot;com.apple/*&quot;这一行后面添加rdr on lo0 inet proto tcp from any to 127.0.0.1 port 80 -&gt; 127.0.0.1 port 8080，保存退出 sudo pfctl -f /etc/pf.conf sudo pfctl -e 这样就完成了，如果想关闭转发，输入sudo pfctl -d。 重启后转发需要重新开启生效，即输入3，4命令 如果要重启后自动生效，首先关闭系统完整性保护机制，需重启到安全模式在终端中执行下述命令关闭文件系统保护1csrutil enable --without fs 然后sudo vim /System/Library/LaunchDaemons/com.apple.pfctl.plist, 添加一行 &lt;string&gt;-e&lt;/string&gt;1234&lt;string&gt;pfctl&lt;/string&gt;&lt;string&gt;-e&lt;/string&gt;&lt;string&gt;-f&lt;/string&gt;&lt;string&gt;/etc/pf.conf&lt;/string&gt; 保存退出。 最后要说的是：博客源码 ， 欢迎 star]]></content>
      <categories>
        <category>Solution</category>
      </categories>
      <tags>
        <tag>mac</tag>
        <tag>tomcat</tag>
        <tag>port</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spring自动注入静态属性]]></title>
    <url>%2Farchives%2F7eedff02.html</url>
    <content type="text"><![CDATA[stackoverflow问题Can you use @Autowired with static fields? Spring中有时需要在静态方法中使用自动注入的属性，例如Service或者Mapper，而@autiwored是不能注解静态属性的，这是因为静态属性是类的属性，而spring注入是对象层面的依赖注入，所以spring是不支持注入静态属性的，这时候如果非得用，就要曲线救国了123456789@Component(&quot;NewClass&quot;)public class NewClass&#123; private static SomeThing someThing; @Autowired public void setSomeThing(SomeThing someThing)&#123; NewClass.someThing = someThing; &#125;&#125; 最后要说的是：博客源码 ， 欢迎 star]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>spring</tag>
        <tag>autowired</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[正则表达式]]></title>
    <url>%2Farchives%2F2f57a694.html</url>
    <content type="text"><![CDATA[待编辑 最后要说的是：博客源码 ， 欢迎 star]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>regular expression</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos搭建Jupyter-Notebook]]></title>
    <url>%2Farchives%2F40396ba0.html</url>
    <content type="text"><![CDATA[centos搭建Jupyter Notebook, 添加多语言支持(R,Ruby,Octave,JS,Java,C++). 常用命令主要熟悉常用命令，wget [url]下载， tar -xvf解压.tar.gz， 使用yum包管理工具yum search xx搜索, yum install xx下载。 whereis，which查找文件， find [path] -name xxx查找指定目录下的文件 ps -ef | grep jupyter 查看jupyter运行的pid 安装jupyter 下载Anaconda最新版本，省事。在官网找到下载链接，python3.6版本，然后 1wget https://repo.anaconda.com/archive/Anaconda3-5.2.0-Linux-x86_64.sh 然后运行脚本 1bash Anaconda3-3-5.2.0-Linux-x86_64.sh 中间需要确定时确定即可，等待安装完成，会自动写入环境变量。 12conda --versionjupyter --version 然后按照官网步骤，设置config 12jupyter notebook --generate-config# 会生成config文件为 .jupyter/jupyter_notebook_config.py 设置密码 123from notebook.auth import passwdpasswd()# 会生成类似 sha1:xxxxxxxxxxxxxxxxxxx， 需要记一下 然后编辑 config文件 1vim .jupyter/jupyter_notebook_config.py 修改这些行，去掉注释，填相应值。其他选项也可以自己设置 1234c.NotebookApp.password = &apos;sha1:xxxxxxxxxxxxxxxxxxx&apos;c.NotebookApp.ip = &apos;*&apos;c.NotebookApp.open_browser = Falsec.NotebookApp.port = 8888 开放和重启防火墙 centos7： 123firewall-cmd --zone=public --add-port=8888/tcp --permanent # 永久开放8888端口firewall-cmd --reload # 重启firewallfirewall-cmd --list-ports # 查看开放端口 centos6： 123/sbin/iptables -I INPUT -p tcp --dport 8888 -j ACCEPT # 开放8888端口/etc/rc.d/init.d/iptables save # 保存service iptables status # 查看防火墙状态 启动server，jupyter notebook。如果需要后台启动，运行nohup jupyter notebook &gt; jupyter.log 2&gt;&amp;1 &amp; 安装其他kernel主要根据 [Jupyter kernels wiki](https://github.com/jupyter/jupyter/wiki/Jupyter-kernels） py2.7123conda create -n ipykernel_py2 python=2 ipykernelsource activate ipykernel_py2python -m ipykernel install --user rubyiruby, ruby需要版本大于2.1 准备工作，安装环境。yum 下载的可能不是最新的ruby，所以用 1sudo yum install -y git-core ruby-devel ruby zlib zlib-devel gcc-c++ patch readline readline-devel libyaml-devel libffi-devel openssl-devel make ruby kernel需要ZeroMQ 12sudo yum install zeromq-devel zeromq czmqgem install cztop rbczmq ffi_rzmq 安装iruby kernel 12gem install cztop irubyiruby register --force R按照irkernel的安装步骤，或者直接1conda install -c r r-essentials octavepip 1pip install octave_kernel 或者conda 12conda config --add channels conda-forgeconda install octave_kernel js首先安装nodejs和npm，然后 12npm install -g ijavascriptijsinstall javaIJava ,需要安装java 9或10，设置好环境变量， 123export JAVA_HOME=/usr/java/jdk-10.0.1/export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jarexport PATH=$JAVA_HOME/bin:$PATH 然后 1234git clone https://github.com/SpencerPark/IJava.git --depth 1cd IJava/chmod u+x gradlew./gradlew installKernel c++xues-cling 123conda create -n clingsource activate clingconda install xeus-cling notebook -c QuantStack -c conda-forge 有个问题是需要切换到cling环境启动jupyter才会有c++的kernel，找了一下cling 的kernel文件 1find anaconda3/envs/cling -name kernels 输出 123anaconda3/envs/cling/lib/python3.6/site-packages/notebook/services/kernelsanaconda3/envs/cling/lib/python3.6/site-packages/notebook/static/services/kernelsanaconda3/envs/cling/share/jupyter/kernels 进入到/share/jupyter/kernels，发现了 12cd anaconda3/envs/cling/share/jupyter/kernelsll 输出 12345total 16drwxr-xr-x 2 root root 4096 Jul 5 14:48 python3drwxr-xr-x 2 root root 4096 Jul 5 14:48 xeus-cling-cpp11drwxr-xr-x 2 root root 4096 Jul 5 14:48 xeus-cling-cpp14drwxr-xr-x 2 root root 4096 Jul 5 14:48 xeus-cling-cpp17 现在，看一下jupyter kernel的文件目录在哪 1jupyter kernelspec list 输出 123456789Available kernels: java /root/.ipython/kernels/java ruby /root/.ipython/kernels/ruby ir /root/.local/share/jupyter/kernels/ir javascript /root/.local/share/jupyter/kernels/javascript python2 /root/.local/share/jupyter/kernels/python2 python3 /root/.local/share/jupyter/kernels/python3 octave /root/anaconda3/share/jupyter/kernels/octave bash /usr/local/share/jupyter/kernels/bash 确定是.ipython/kernels/的这一个。 将xeus-cling-cpp11 xeus-cling-cpp14 xeus-cling-cpp17这三个文件夹复制到.ipython/kernels/下 123cp xeus-cling-cpp11 .ipython/kernels/cp xeus-cling-cpp14 .ipython/kernels/cp xeus-cling-cpp17 .ipython/kernels/ 完成。 最后要说的是：博客源码 ， 欢迎 star]]></content>
      <categories>
        <category>Solution</category>
      </categories>
      <tags>
        <tag>Jupyter NoteBook</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ZooKeeper的分布式锁实现]]></title>
    <url>%2Farchives%2Fbfb69c25.html</url>
    <content type="text"><![CDATA[原生ZooKeeper APICurator待编辑 最后要说的是：博客源码 ， 欢迎 star]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>ZooKeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java主线程等待子线程的几种方法]]></title>
    <url>%2Farchives%2F3e629a3.html</url>
    <content type="text"><![CDATA[在很多时候, 都需要在主线程中等待所有线程执行完毕, 再进行其他的操作. 在这种情况下, 显然如下的写法是不行的.123456789101112131415161718192021public class Main &#123; public static void main(String[] args) &#123; long start = System.currentTimeMillis(); Thread thread = new Thread() &#123; public void run() &#123; System.out.println(this.getName() + " start"); try &#123; Thread.sleep(5000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(this.getName() + " end"); &#125; &#125;; thread.start(); long end = System.currentTimeMillis(); System.out.println("runtime: " + (end - start)); &#125;&#125; 这时候的输出是123runtime: 0Thread-0 startThread-0 end 这时候需要阻塞主线程, 让其等待子线程执行完毕, 方法有几种, 下面开始介绍. 准备工作先创建一个类实现Runnable接口.123456789101112131415public class MyRunnable implements Runnable &#123; @Override public void run() &#123; System.out.println(Thread.currentThread().getName() + " start"); try &#123; Thread.sleep(5000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName() + " end"); &#125;&#125; Thread.join()等待一个子线程123456789101112131415161718public class Main &#123; public static void main(String[] args) &#123; long start = System.currentTimeMillis(); MyRunnable runnable = new MyRunnable(); Thread thread = new Thread(runnable, "thread-0"); thread.start(); try &#123; thread.join(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; long end = System.currentTimeMillis(); System.out.println("runtime: " + (end - start)); &#125;&#125; 等待多个子线程1234567891011121314151617181920public class Main &#123; public static void main(String[] args) &#123; long start = System.currentTimeMillis(); for(int i = 0; i &lt; 5; i++) &#123; MyRunnable runnable = new MyRunnable(); Thread thread = new Thread(runnable, "thread-" + i); thread.start(); try &#123; thread.join(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; long end = System.currentTimeMillis(); System.out.println("runtime: " + (end - start)); &#125;&#125; 输出, 显然是串行执行的5个线程.1234567891011thread-0 startthread-0 endthread-1 startthread-1 endthread-2 startthread-2 endthread-3 startthread-3 endthread-4 startthread-4 endruntime: 25004 如果想异步并发执行多个子线程, 可在循环体外join12345678910111213141516171819202122232425262728import java.util.ArrayList;import java.util.List;public class Main &#123; public static void main(String[] args) &#123; long start = System.currentTimeMillis(); List&lt;Thread&gt; list = new ArrayList&lt;Thread&gt;(); for(int i = 0; i &lt; 5; i++) &#123; MyRunnable runnable = new MyRunnable(); Thread thread = new Thread(runnable, "thread-" + i); thread.start(); list.add(thread); &#125; try &#123; for(Thread thread : list) &#123; thread.join(); &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; long end = System.currentTimeMillis(); System.out.println("runtime: " + (end - start)); &#125;&#125; 输出.1234567891011thread-1 startthread-3 startthread-4 startthread-0 startthread-2 startthread-1 endthread-3 endthread-4 endthread-2 endthread-0 endruntime: 5004 由于每个线程都会抢占cpu执行, 执行的顺序是随机的, 所以每次输出都会不同. CountDownLatchCountDownLatch是java.util.concurrent下的一个类, 作用是允许一个或多个线程等待其他线程执行完毕. A synchronization aid that allows one or more threads to wait until a set of operations being performed in other threads completes. CountDownLatch源码如下.123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051public class CountDownLatch &#123; /** * Synchronization control For CountDownLatch. The details are not * writted. please read the official docs. */ private static final class Sync extends AbstractQueuedSynchronizer &#123;...&#125; private final Sync sync; // Constructs a CountDownLatch initialized with the given count. public CountDownLatch(int count) &#123; if (count &lt; 0) throw new IllegalArgumentException("count &lt; 0"); this.sync = new Sync(count); &#125; /** * Causes the current thread to wait until the latch has counted down to * zero, unless the thread is interrupted. */ public void await() throws InterruptedException &#123; sync.acquireSharedInterruptibly(1); &#125; /** * Causes the current thread to wait until the latch has counted down to * zero, unless the thread is interrupted, or the specified waiting time * elapses. */ public boolean await(long timeout, TimeUnit unit) throws InterruptedException &#123; return sync.tryAcquireSharedNanos(1, unit.toNanos(timeout)); &#125; /** * Decrements the count of the latch, releasing all waiting threads if the * count reaches zero. */ public void countDown() &#123; sync.releaseShared(1); &#125; public long getCount() &#123; return sync.getCount(); //Returns the current count. &#125; public String toString() &#123; return super.toString() + "[Count = " + sync.getCount() + "]"; &#125;&#125; 有一个构造器和几个方法, 构造时传参用于定义CountDownLatch大小, 且不可修改. 具体应用时, 每次执行一个线程后, 就countdown()一次. 在所有线程开始执行后, 立即await()等待, 直到所有线程执行完, 再执行await()后的代码段. 使用CountDownLatch实现主线程等待子线程如下.1234567891011121314151617181920212223242526272829303132import java.util.concurrent.CountDownLatch;public class Main &#123; public static void main(String[] args) &#123; long start = System.currentTimeMillis(); CountDownLatch latch = new CountDownLatch(5); for(int i = 0; i &lt; 5; i++) &#123; new Thread() &#123; public void run() &#123; System.out.println(Thread.currentThread().getName() + " start"); try &#123; Thread.sleep(5000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; latch.countDown(); &#125; System.out.println(Thread.currentThread().getName() + " end"); &#125; &#125; .start(); &#125; try &#123; latch.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; long end = System.currentTimeMillis(); System.out.println("runtime: " + (end - start)); &#125;&#125; 输出1234567891011Thread-2 startThread-3 startThread-0 startThread-1 startThread-4 startThread-2 endThread-3 endThread-0 endThread-1 endThread-4 endruntime: 5004 线程池java.util.concurrent.ExecutorService是java线程池的一个接口, 通过ExecutorService实现主线程等待子线程的方法很多, 比如submit()的返回Future对象判断提交的任务是否执行完, 或者在线程池中使用CountDownLatch, 或者用isTerminated()或awiatTermination(long, TimeUnit)判断线程池shutdown后所有任务是否完成. 具体可以查一下ExecutorService的文档 这里, 讲一个最简单的isTerminated().12345678910111213141516171819import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;public class Main &#123; public static void main(String[] args) &#123; long start = System.currentTimeMillis(); ExecutorService pool = Executors.newFixedThreadPool(5); for(int i = 0; i &lt; 5; i++) &#123; MyRunnable runnable = new MyRunnable(); pool.execute(runnable); &#125; pool.shutdown(); while(!pool.isTerminated()); long end = System.currentTimeMillis(); System.out.println("runtime: " + (end - start)); &#125;&#125; 输出1234567891011pool-1-thread-4 startpool-1-thread-2 startpool-1-thread-3 startpool-1-thread-5 startpool-1-thread-1 startpool-1-thread-5 endpool-1-thread-4 endpool-1-thread-2 endpool-1-thread-1 endpool-1-thread-3 endruntime: 5003 或者用awaitTermination(long, TimeUnit)更好, long传一个长整型, TimeUnit传时间单位, 常用的有MILLISECONDS, SECONDS, MINUTES等等, long和TimeUnit组合表示超时时间. 当线程池所有任务执行完,返回true. 未执行完前超时返回false. 如下.12345678910111213141516171819202122232425import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;import java.util.concurrent.TimeUnit;public class Main &#123; public static void main(String[] args) &#123; long start = System.currentTimeMillis(); ExecutorService pool = Executors.newFixedThreadPool(5); for(int i = 0; i &lt; 5; i++) &#123; MyRunnable runnable = new MyRunnable(); pool.execute(runnable); &#125; pool.shutdown(); try &#123; //可以让while循环每2s执行一次, 而不是一直循环消耗性能 while(!pool.awaitTermination(2, TimeUnit.SECONDS)); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; long end = System.currentTimeMillis(); System.out.println("runtime: " + (end - start)); &#125;&#125; 输出1234567891011pool-1-thread-2 startpool-1-thread-5 startpool-1-thread-1 startpool-1-thread-4 startpool-1-thread-3 startpool-1-thread-2 endpool-1-thread-5 endpool-1-thread-1 endpool-1-thread-3 endpool-1-thread-4 endruntime: 5003 最后CountDownLatch相对于join()来说, 在复杂场景下更能体现出优势. 比如需要主线程在其他线程执行一半或执行到某个阶段时开始, 这种情况是join()没法做到的. 最后要说的是：博客源码 ， 欢迎 star]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>concurrent</tag>
        <tag>Thread</tag>
        <tag>CountDownLatch</tag>
        <tag>ExecutorService</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spring-boot的Mapper与数据库表字段映射]]></title>
    <url>%2Farchives%2F4a57f35e.html</url>
    <content type="text"><![CDATA[本文讲述Spring Boot如何通过mybatis-spring-boot-starter集成Mybatis，并且在Mapper中如何映射Model属性和表的字段。下面给出一个简单的示例。 pom.xml首先要引入mybatis的依赖12345&lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.1.1&lt;/version&gt;&lt;/dependency&gt; table_user数据库建表123456CREATE TABLE IF NOT EXISTS `tb_user`( `id` INT(11) NOT NULL, `name` VARCHAR(100) NOT NULL, `created_by` VARCHAR(100) NOT NULL, PRIMARY KEY ( `id` ))ENGINE=InnoDB DEFAULT CHARSET=utf8; User实体类12345678public class User &#123; private Integer id; private String name; private String createdBy; //省略了setters &amp; getters&#125; UserMapperMapper接口中可以通过注解的形式直接写sql，比将sql分离到xml中的方式更方便12345678910111213141516171819202122232425262728@Mapperpublic interface UserMapper &#123; @Select("select * from tb_user where id = #&#123;id&#125;") User selectById(@Param("id") int id); @Select("select * from tb_user") List&lt;User&gt; selectAll(); @Insert("insert into tb_user(id, name, created_by) values(#&#123;id&#125;, #&#123;name&#125;, #&#123;createdBy&#125;)") @Results(id = "user", value = &#123; @Result(column = "id",property = "id"), @Result(column = "name",property = "name"), @Result(column = "created_by",property = "createdBy") &#125; ) //@Options(useGeneratedKeys = true, keyProperty = "id", keyColumn = "id") void insertUser(User user); @Update("update tb_user set id = #&#123;id&#125;, name = #&#123;name&#125;, created_by = #&#123;createdBy&#125;") @ResultMap(value = "user") void updateUser(User user); @Delete("delete from tb_user where id = #&#123;id&#125;") void deleteById(@Param("id") int id);&#125; @Select, @Insert, @Update, @Delete显然就是sql语句的注解了. @Param是根据别名取参数的. @Results和@Result配合使用, 就可以将实体类属性和表字段进行一一映射. @Results的参数id表示这个映射的别名, 可以配合@ResultMap使用. @Result的参数column表示表字段名, property表示实体属性名. @Options可以在插入时返回主键值, 在这里没什么用. 一般用于在主键id自增的情况下, 插入操作不定义id, 可以在插入数据库表后返回该条插入信息的主键id. UserService1234567891011121314151617181920212223242526272829303132333435363738394041@Servicepublic class ScriptService &#123; @Autowired private UserMapper userMapper; public User selectById(int id) &#123; User user = userMapper.selectById(id); return user; &#125; public List&lt;User&gt; selectAll() &#123; List&lt;User&gt; list = userMapper.selectAll(); return list; &#125; public int insertUser(User user) &#123; userMapper.insertUser(user); retyrn user.getId(); &#125; public boolean updateUser(User user) &#123; int id = user.getId(); user check = userMapper.selectById(id); if (check == null) &#123; return false; &#125; userMapper.updateUser(user); return true; &#125; public boolean deleteById(int id) &#123; user check = userMapper.selectById(id); if (check == null) &#123; return false; &#125; userMapper.deleteById(id); return true; &#125;&#125; UserController1234567891011121314151617181920212223242526272829303132333435363738@Controller@RequestMapping("/user")public class UserController &#123; @Autowired private UserService userService; @RequestMapping(value = "/selectById/&#123;id&#125;", method = RequestMethod.GET) @ResponseBody public User selectById(@PathVariable("id") int id) &#123; return userService.selectById(id); &#125; @RequestMapping(value = "/selectAll", method = RequestMethod.GET) @ResponseBody public List&lt;User&gt; selectAll() &#123; return userService.selectAll(); &#125; @RequestMapping(value = "/insertUser", method = RequestMethod.POST) @ResponseBody public int insertUser(@RequestBody User user) &#123; return userService.insertUser(user); &#125; @RequestMapping(value = "/updateUser", method = RequestMethod.POST) @ResponseBody public boolean updateUser(@RequestBody User user) &#123; return userService.updateUser(user); &#125; @RequestMapping(value = "/deleteById/&#123;id&#125;", method = RequestMethod.POST) @ResponseBody public boolean deleteById(@PathVariable("id") int id) &#123; return userService.deleteById(id); &#125;&#125; 最后要说的是：博客源码 ， 欢迎 star]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>spring-boot</tag>
        <tag>ResultMap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Thymeleaf的简单使用]]></title>
    <url>%2Farchives%2Ff5d54990.html</url>
    <content type="text"><![CDATA[Thymeleaf是一款用于渲染XML/XHTML/HTML5内容的Java模板引擎库，可以通过HTML的标签属性渲染标签内容。 举个例子，1&lt;p th:text="$&#123;home.welcome&#125;"&gt;Welcome to our grocery store!&lt;/p&gt; 这里的th:text的内容就是需要后台渲染的，假如没有后台渲染，html会将无法识别的部分直接过滤掉，那么输出就是1&lt;p &gt;Welcome to our grocery store!&lt;/p&gt; 假如后台传过来的home.welcome的值是Welcome, Yanss!，那么输出就是1&lt;p &gt;Welcome, Yanss!&lt;/p&gt; 这就是Thymeleaf的用法和作用了，其他的地方也差不多。 记录几个常用的语法。 URL1&lt;a th:href="@&#123;http://www.thymeleaf.org&#125;"&gt;Thymeleaf&lt;/a&gt; 如果需要传参12&lt;a th:href="@&#123;http://www.thymeleaf.org(id=$&#123;id&#125;)&#125;"&gt;Thymeleaf&lt;/a&gt;&lt;a th:href="@&#123;http://www.thymeleaf.org(id=$&#123;id&#125;,name=$&#123;name&#125;)&#125;"&gt;Thymeleaf&lt;/a&gt; 字符串替换1&lt;span th:text="'Welcome to our application, ' + $&#123;user.name&#125; + '!'"&gt; 条件选择式类似于java的三元表达式1&lt;p th:text="true?'真':'假'"&gt;&lt;/p&gt; 循环loop 创建表格 123456789101112&lt;table&gt; &lt;tr&gt; &lt;th&gt;NAME&lt;/th&gt; &lt;th&gt;PRICE&lt;/th&gt; &lt;th&gt;IN STOCK&lt;/th&gt; &lt;/tr&gt; &lt;tr th:each="prod : $&#123;prods&#125;"&gt; &lt;td th:text="$&#123;prod.name&#125;"&gt;Onions&lt;/td&gt; &lt;td th:text="$&#123;prod.price&#125;"&gt;2.41&lt;/td&gt; &lt;td th:text="$&#123;prod.inStock&#125;? #&#123;true&#125; : #&#123;false&#125;"&gt;yes&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; 创建下拉框 在项目启动访问index页面的时候，把要需要的列表集合存到session作用域 123456@RequestMapping("index")public String index(HttpSession session)&#123; List&lt;Classes&gt; list = userService.findAllClasses(); session.setAttribute("list",list); return "index";&#125; 前台取值 1234&lt;select name="className" class="form-control"&gt; &lt;option&gt;请选择班级&lt;/option&gt; &lt;option th:each="list:$&#123;session.list&#125;" th:value="$$&#123;list.cid&#125;" th:text="$&#123;list.cname&#125;"&gt;&lt;/option&gt;&lt;/select&gt; 遍历Map和List 123456789&lt;table class="table"&gt; &lt;thead&gt; &lt;th th:each="entry : $&#123;map&#125;" th:text="$&#123;entry.key&#125;"&gt;&lt;/th&gt; &lt;th th:each="entry : $&#123;map&#125;" th:text="$&#123;entry.value&#125;"&gt;&lt;/th&gt; &lt;/thead&gt; &lt;tbody &gt; &lt;td th:each="ele : $&#123;list&#125;" th:text="$&#123;ele&#125;"&gt;&lt;/td&gt; &lt;/tbody&gt; &lt;/table&gt; 遍历List 1234&lt;ul th:each="lm : $&#123;listmap&#125;"&gt; &lt;li th:each="entry : $&#123;lm&#125;" th:text="$&#123;entry.key&#125;" &gt;&lt;/li&gt; &lt;li th:each="entry : $&#123;lm&#125;" th:text="$&#123;entry.value&#125;"&gt;&lt;/li&gt; &lt;/ul&gt; if/unless, switch/case下面&lt;a&gt;标签只有在th:if中条件成立时才显示，th:unless只有不成立时才显示1&lt;a th:href="@&#123;/login&#125;" th:if=$&#123;session.user != null&#125;&gt;Login&lt;/a&gt; switch/case也很好理解，默认属性default可以用*表示12345&lt;div th:switch="$&#123;user.role&#125;"&gt; &lt;p th:case="'admin'"&gt;User is an administrator&lt;/p&gt; &lt;p th:case="#&#123;roles.manager&#125;"&gt;User is a manager&lt;/p&gt; &lt;p th:case="*"&gt;User is some other thing&lt;/p&gt;&lt;/div&gt; inline举个例子, 当我想在一个两层的标签中同时渲染外层和内层的属性, 可能会出现下面这种错误.1&lt;p th:text="$&#123;Hello.world&#125;"&gt;&lt;span th:if="$&#123;user == 'yanss'&#125;"&gt;yanss&lt;/span&gt;&lt;/p&gt; 这种写法等同于1&lt;p th:text="$&#123;Hello.world&#125;"&gt;&lt;/p&gt; 因为外层的th:text会将内层的覆盖掉, 如果要同时渲染, 可以使用inline属性, 也可以将内外层隔离开.1234567&lt;p th:inline="text"&gt;[[$&#123;Hello.world&#125;]]&lt;span th:if="$&#123;user == 'yanss'&#125;"&gt;yanss&lt;/span&gt;&lt;/p&gt;&lt;!-- or --&gt;&lt;p &gt; &lt;span th:text="$&#123;Hello.world&#125;"&gt;&lt;/span&gt; &lt;span th:if="$&#123;user == 'yanss'&#125;"&gt;yanss&lt;/span&gt;&lt;/p&gt; 最后要说的是：博客源码 ， 欢迎 star]]></content>
      <categories>
        <category>Front-End</category>
      </categories>
      <tags>
        <tag>Thymeleaf</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Win10推荐软件]]></title>
    <url>%2Farchives%2Fcb859582.html</url>
    <content type="text"><![CDATA[Seer 空格预览 QuickLook更好用 Listary 类Spotlight 设置页，先激活 关键字Web 百度：http://www.baidu.com/s?wd={query} 淘宝：https://s.taobao.com/search?q={query} 京东：https://search.jd.com/Search?keyword={query}&amp;enc=utf-8&amp;qrst=1&amp;rt=1&amp;stop=1&amp;vt=2&amp;wq={query}&amp;psort=3&amp;wtype=1&amp;stock=1&amp;click=2 必应词典：http://cn.bing.com/dict/search?q={query} 关键字自定义 run： 新建文件： GoldenDict： Ditto 剪贴板工具 Bandizip 比7Zzip好 Honeyview 看图超快 potplayer 视频播放器 加皮肤Zune GoldenDict 词典，配合Listary 迅雷极速版 另一地址 FDM 下载工具 SumatraPDF 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768//高级设置# For documentation, see http://www.sumatrapdfreader.org/settings3.2.htmlMainWindowBackground = #80fff200EscToExit = falseReuseInstance = falseUseSysColors = falseRestoreSession = trueFixedPageUI [ TextColor = #000000 BackgroundColor = #ffffff SelectionColor = #f5fc0c WindowMargin = 2 4 2 4 PageSpacing = 4 4]EbookUI [ FontName = 微软雅黑 FontSize = 12.5 TextColor = #5f4b32 BackgroundColor = #fbf0d9 UseFixedPageUI = false GradientColors = #7aa1d2 #dbd4b4 #cc95c0]ComicBookUI [ WindowMargin = 0 0 0 0 PageSpacing = 4 4 CbxMangaMode = false]ChmUI [ UseFixedPageUI = false]ExternalViewers []ShowMenubar = trueReloadModifiedDocuments = trueFullPathInTitle = falseZoomLevels = 8.33 12.5 18 25 33.33 50 66.67 75 100 125 150 200 300 400 600 800 1000 1200 1600 2000 2400 3200 4800 6400ZoomIncrement = 0PrinterDefaults [ PrintScale = shrink]ForwardSearch [ HighlightOffset = 0 HighlightWidth = 15 HighlightColor = #6581ff HighlightPermanent = false]CustomScreenDPI = 0RememberStatePerDocument = trueUiLanguage = cnShowToolbar = trueShowFavorites = falseAssociateSilently = falseCheckForUpdates = trueRememberOpenedFiles = trueEnableTeXEnhancements = falseDefaultDisplayMode = single pageDefaultZoom = fit pageWindowState = 1WindowPos = 695 80 970 1010ShowToc = trueSidebarDx = 196TocDy = 0ShowStartPage = trueUseTabs = true IObit 卸载工具 CCleaner 清理工具 Anaconda python包管理 Sublime 插件下载 1234567//OmniMarkupPreviewer快捷键设置[ &#123; &quot;keys&quot;: [&quot;alt+p&quot;], &quot;command&quot;: &quot;omni_markup_preview&quot;, &quot;context&quot;: [&#123;&quot;key&quot;: &quot;omnimarkup_is_enabled&quot;, &quot;operator&quot;: &quot;equal&quot;, &quot;operand&quot;: &quot;&quot;&#125;] &#125;] ShadowsocksR 下载地址一 地址二 github 自己搭建vps 方案]]></content>
      <categories>
        <category>Solution</category>
      </categories>
      <tags>
        <tag>win10</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop入门]]></title>
    <url>%2Farchives%2F67914616.html</url>
    <content type="text"><![CDATA[Understanding Hadoop Clusters and the NetworkAuthor: Brad HedlundLink: original textTranslator: Yanss This article is Part 1 in series that will take a closer look at the architecture and methods of a Hadoop cluster, and how it relates to the network and server infrastructure. The content presented here is largely based on academic work and conversations I’ve had with customers running real production clusters. If you run production Hadoop clusters in your data center, I’m hoping you’ll provide your valuable insight in the comments below. Subsequent articles to this will cover the server and network architecture options in closer detail. Before we do that though, lets start by learning some of the basics about how a Hadoop cluster works. OK, let’s get started! 本文是系列的第1部分，将带你详细了解Hadoop集群的架构和方法，以及它如何将网络和服务器基础设施相关联。这里介绍的内容主要是基于学术研究和我与在实际产品中运行集群的客户的交流。如果你在你的数据中心中运行Hadoop集群生产，我期待你在下面的评论中提供有价值的见解。接下来的文章将会包含服务器和网络结构的详细细节。然而在此之前，让我了解一些Hadoop集群工作的基础。 The three major categories of machine roles in a Hadoop deployment are Client machines, Masters nodes, and Slave nodes. The Master nodes oversee the two key functional pieces that make up Hadoop: storing lots of data (HDFS), and running parallel computations on all that data (Map Reduce). The Name Node oversees and coordinates the data storage function (HDFS), while the Job Tracker oversees and coordinates the parallel processing of data using Map Reduce. Slave Nodes make up the vast majority of machines and do all the dirty work of storing the data and running the computations. Each slave runs both a Data Node and Task Tracker daemon that communicate with and receive instructions from their master nodes. The Task Tracker daemon is a slave to the Job Tracker, the Data Node daemon a slave to the Name Node. Hadoop部署的三个主要分类分别是Client machines、Masters nodes和Slave nodes。主节点监督两个重要的功能块形成Hadoop：存储大量数据(HDFS)，在所有数据上并行计算(Map Reduce)。Name Node监督协调数据存储功能(HDFS)，同时Job Tracker监督协调使用Map Reduce进行数据的并行处理。Slave Nodes形成大多数的机构，做着所有的存储数据和运行计算的脏活。每个slave同时运行着Data Node和Task Tracker的后台程序——用以传递和接收来自他们的master nodes的命令。Task Tracker后台程序是Job Tracker的slave，Data node后台程序是Name Node的slave。 Client machines have Hadoop installed with all the cluster settings, but are neither a Master or a Slave. Instead, the role of the Client machine is to load data into the cluster, submit Map Reduce jobs describing how that data should be processed, and then retrieve or view the results of the job when its finished. In smaller clusters (~40 nodes) you may have a single physical server playing multiple roles, such as both Job Tracker and Name Node. With medium to large clusters you will often have each role operating on a single server machine. Client machines的Hadoop安装了所有的集群设置，但不包含Master或Slave。相应的，Client machine的作用是加载数据到集群，提交Map Reduce工作，描述数据应该怎么处理，然后在公众完成时取回或查看结果。在小一些的集群（约40个节点）中，你可能只有一个实体服务器运行多任务，例如Job Tracker和Name Node一样。在中大型集群你可能会在单个服务器中进行单个任务运转。 In real production clusters there is no server virtualization, no hypervisor layer. That would only amount to unnecessary overhead impeding performance. Hadoop runs best on Linux machines, working directly with the underlying hardware. That said, Hadoop does work in a virtual machine. That’s a great way to learn and get Hadoop up and running fast and cheap. I have a 6-node cluster up and running in VMware Workstation on my Windows 7 laptop. 在实际生产集群中没有服务器虚拟化，没有虚拟机监视器。那只会产生大量不必要的性能开支。Hadoop在Linux机器上运行得最好，直接在底层硬件上工作。也就是说，Hadoop在虚拟机上工作。那是了解和搭建Hadoop的好办法，并且运行的又快又便宜。我有一个6节点的集群，运行在我的Windows 7笔记本的VMware工作台上。 This is the typical architecture of a Hadoop cluster. You will have rack servers (not blades) populated in racks connected to a top of rack switch usually with 1 or 2 GE boned links. 10GE nodes are uncommon but gaining interest as machines continue to get more dense with CPU cores and disk drives. The rack switch has uplinks connected to another tier of switches connecting all the other racks with uniform bandwidth, forming the cluster. The majority of the servers will be Slave nodes with lots of local disk storage and moderate amounts of CPU and DRAM. Some of the machines will be Master nodes that might have a slightly different configuration favoring more DRAM and CPU, less local storage. In this post, we are not going to discuss various detailed network design options. Let’s save that for another discussion (stay tuned). First, lets understand how this application works… 这是一个Hadoop集群的典型结构。你将使用机架服务器(不是刀锋服务器)，搭建在机架中，连接一个顶部机架开关，通常使用1或2 GE(Gigabit Ethernet千兆以太网)。10 GE节点是不常有的，但当机器使用CPU核心和磁盘驱动获取更大的密度时收益更多。机架开关上行传输被连接到连接所有其他相同带宽机架的另一层开关，构成集群。大多数服务器是Slave nodes，使用大量的本地磁盘存储和中量的CPU和DRAM。一些机器是Master nodes，可能有轻微不同的配置，使用更多的DRAM和CPU，较少的本地存储。在这片文章中，我们不讨论许多详细的网络设计选择，让我们将它保留到另一个讨论(在调试中)中。首先，让我们理解这个应用怎么工作的。 Why did Hadoop come to exist? What problem does it solve? Simply put, businesses and governments have a tremendous amount of data that needs to be analyzed and processed very quickly. If I can chop that huge chunk of data into small chunks and spread it out over many machines, and have all those machines processes their portion of the data in parallel – I can get answers extremely fast – and that, in a nutshell, is what Hadoop does. In our simple example, we’ll have a huge data file containing emails sent to the customer service department. I want a quick snapshot to see how many times the word “Refund” was typed by my customers. This might help me to anticipate the demand on our returns and exchanges department, and staff it appropriately. It’s a simple word count exercise. The Client will load the data into the cluster (File.txt), submit a job describing how to analyze that data (word count), the cluster will store the results in a new file (Results.txt), and the Client will read the results file. Hadoop为何诞生？它解决了什么问题？简言之，商业和政府有一个极大量的数据需要非常快地分析和处理。如果我可以分离这个巨大的数据到很多小的部分，铺开到大量的机器中，让这些机器并行处理它们各自的那一部分——我就可以极快的获取结果——这就是Hadoop做的事情。在我们的简单例子中，我们将用一个巨大的数据文件，它包含了发送到客户服务部门的邮件。我想要一个数据快照，来查看单词“Refund”被客户输入了多少次。这将有助于我预测退还和交换部门的需求，并且合理地安排职员。这是一个简单的词条计数训练。Clients将会加载数据到集群(File.txt)，提交一个工作描述，如何分析数据(单词计数)，集群会存储结果到一个新的文件(Results.txt)，然后Clients会读取结果文件。 Your Hadoop cluster is useless until it has data, so we’ll begin by loading our huge File.txt into the cluster for processing. The goal here is fast parallel processing of lots of data. To accomplish that I need as many machines as possible working on this data all at once. To that end, the Client is going to break the data file into smaller “Blocks”, and place those blocks on different machines throughout the cluster. The more blocks I have, the more machines that will be able to work on this data in parallel. At the same time, these machines may be prone to failure, so I want to insure that every block of data is on multiple machines at once to avoid data loss. So each block will be replicated in the cluster as its loaded. The standard setting for Hadoop is to have (3) copies of each block in the cluster. This can be configured with the dfs.replication parameter in the file hdfs-site.xml. 你的Hadoop集群直到有数据才有用，所以我们开始于加载超大的File.txt到集群中处理。这里的目标是快速并行处理大量数据。为此我需要尽可能多的机器同时处理这些数据。在那结束后，Client将会打断这个数据文件为许多小的块，将这些块放到遍及集群的不同的机器上。分成的块越多，能并行工作的机器就越多。在同一时间，这些机器可能容易失败，所以为了避免数据丢失，我会确信每个数据块在多台机器上存在。所以每个块会在加载到集群时复制。Hadoop的标准设置是集群中每个块有3个复制。这个可以在hdfs-site.xml文件的dfs.replication参数中设置。 The Client breaks File.txt into (3) Blocks. For each block, the Client consults the Name Node (usually TCP 9000) and receives a list of (3) Data Nodes that should have a copy of this block. The Client then writes the block directly to the Data Node (usually TCP 50010). The receiving Data Node replicates the block to other Data Nodes, and the cycle repeats for the remaining blocks. The Name Node is not in the data path. The Name Node only provides the map of where data is and where data should go in the cluster (file system metadata). Client将File.txt拆分为3个块。对每个块，Client查看Name Node(通常用TCP 9000)并接收一个3个Data Nodes的list，每个Data Node都是一个块的复制。Client将块直接写入到Data Node(通常用TCP 50010)。收到的Data Node将块复制到其他Data Nodes，剩下的块也循环这个重复过程。Name Node不是数据路径。在集群中(文件系统云数据)Name Node只提供数据的位置和数据应该去哪。 Hadoop has the concept of “Rack Awareness”. As the Hadoop administrator you can manually define the rack number of each slave Data Node in your cluster. Why would you go through the trouble of doing this? There are two key reasons for this: Data loss prevention, and network performance. Remember that each block of data will be replicated to multiple machines to prevent the failure of one machine from losing all copies of data. Wouldn’t it be unfortunate if all copies of data happened to be located on machines in the same rack, and that rack experiences a failure? Such as a switch failure or power failure. That would be a mess. So to avoid this, somebody needs to know where Data Nodes are located in the network topology and use that information to make an intelligent decision about where data replicas should exist in the cluster. That “somebody” is the Name Node. Hadoop有“机架感知”的概念。作为Hadoop管理员，你可以手动定义集群中每个slave Data Noded的机架数量。为什么你要做这个麻烦的事情呢？有两个关键原因：防止数据丢失和网络性能。记住每个块的数据需要复制到多个机器以防止一个机器失败是丢失所有的数据。如果所有的数据复制碰巧位于同意机架的机器上，并且机架发生失败，会发生这样的事情吗？比如一个开关失败或者是供电问题。那将会一团糟。所以为了避免这样，“某物”需要知道网络拓扑中Data Nodes在哪，以此做一个关于数据复制品应该存放在集群何处的智能的决定。这个“某物”是Name Node。 There is also an assumption that two machines in the same rack have more bandwidth and lower latency between each other than two machines in two different racks. This is true most of the time. The rack switch uplink bandwidth is usually (but not always) less than its downlink bandwidth. Furthermore, in-rack latency is usually lower than cross-rack latency (but not always). If at least one of those two basic assumptions are true, wouldn’t it be cool if Hadoop can use the same Rack Awareness that protects data to also optimally place work streams in the cluster, improving network performance? Well, it does! Cool, right? 有一个假设关于在同一机架的两个机器之间比起不同机架的两个机器有更多的带宽和更低的等待时间。大多是时候这是正确的。机架开关的上行带宽通常(不总是)比下行带宽小。而且，在机架内的等待时间通常比机架见的等待时间低(不总是)。如果这两个基础假设中至少一个是对的，如果Hadoop能用同一个机架感知，保护数据到工作流也就是集群中最适宜的位置，提高网络性能，不会更好吗？当然会。 What is NOT cool about Rack Awareness at this point is the manual work required to define it the first time, continually update it, and keep the information accurate. If the rack switch could auto-magically provide the Name Node with the list of Data Nodes it has, that would be cool. Or vice versa, if the Data Nodes could auto-magically tell the Name Node what switch they’re connected to, that would be cool too. 在这一点关于机架感知不好的地方是，manual work required to define it the first time，持续的更新它，保持信息准确。如果机架可以自动提供Data Nodes list的Name Node，那将会很好。反过来也是，如果Data Nodes可以自动告诉Name Node它们连接的什么开关，也会很好。 Even more interesting would be a OpenFlow network, where the Name Node could query the OpenFlow controller about a Node’s location in the topology. 甚至更有趣的是一个OpenFlow network，Name Node可以在哪查询OpenFlow控制器关于一个Node的拓扑位置。 The Client is ready to load File.txt into the cluster and breaks it up into blocks, starting with Block A. The Client consults the Name Node that it wants to write File.txt, gets permission from the Name Node, and receives a list of (3) Data Nodes for each block, a unique list for each block. The Name Node used its Rack Awareness data to influence the decision of which Data Nodes to provide in these lists. The key rule is that for every block of data, two copies will exist in one rack, another copy in a different rack. So the list provided to the Client will follow this rule. Client准备好加载文件到集群中，将它打断到不同的块，开始块A。Client查询Name Node，想写入File.txt，从Name Node获取许可，接收一个包含每个块的Data Node的list，一个包含每个块独有的list。Name Node用它的机架感知数据去影响这些list中提供哪个Data Nodes的决定。重要规则是对数据的每个块，两个复制存在一个机架中，另一个复制在其他机架中。所以这个list提供Client将遵循这个规则。 Before the Client writes “Block A” of File.txt to the cluster it wants to know that all Data Nodes which are expected to have a copy of this block are ready to receive it. It picks the first Data Node in the list for Block A (Data Node 1), opens a TCP 50010 connection and says, “Hey, get ready to receive a block, and here’s a list of (2) Data Nodes, Data Node 5 and Data Node 6. Go make sure they’re ready to receive this block too.” Data Node 1 then opens a TCP connection to Data Node 5 and says, “Hey, get ready to receive a block, and go make sure Data Node 6 is ready is receive this block too.” Data Node 5 will then ask Data Node 6, “Hey, are you ready to receive a block?” 在Client写入块到File.txt之前，它想知道所有的Data Nodes哪个期望准备接收一个这个块的复制。它在list中为块A挑选第一个Data Node(Data Node 1)，打开TCP 50010连接，然后说，“嘿，准备好接收一个块，这是两个Data Node的list，Data Node 5和Data Node 6。去确信它们也准备好接收这个块。”然后Data Node 1打开一个TCP连接到Data Node 5然后说，“嘿，准备好接收一个块，去确信Data Node 6也准备好接收这个块。”Data Node 5将问Data Node 6，“嘿，你准备好接收一个块了吗？” The acknowledgments of readiness come back on the same TCP pipeline, until the initial Data Node 1 sends a “Ready” message back to the Client. At this point the Client is ready to begin writing block data into the cluster. 准备就绪的确认通知在同一TCP管道返回，直到初始的Data Node 1发送一个“Ready”信息给Client。这样，Client就准备好开始写入块数据到集群中。 As data for each block is written into the cluster a replication pipeline is created between the (3) Data Nodes (or however many you have configured in dfs.replication). This means that as a Data Node is receiving block data it will at the same time push a copy of that data to the next Node in the pipeline. 当每个块的数据被写入集群时, 三个Data Nodes之间(或者无论多少个你在dfs.replication中设置的)会创建一个复制管道. 这意味着, 当一个Data Node接收数据块时, 它会同时推送一个数据的复制到管道中的下一个Node. Here too is a primary example of leveraging the Rack Awareness data in the Name Node to improve cluster performance. Notice that the second and third Data Nodes in the pipeline are in the same rack, and therefore the final leg of the pipeline does not need to traverse between racks and instead benefits from in-rack bandwidth and low latency. The next block will not be begin until this block is successfully written to all three nodes. 这也是一个借助机架系统的简单例子, Name Node中的数据提升集群性能. 注意管道中的第二个和第三个Data Node位于同一个机架中, 因此管道的最后一步不用穿过机架, 这会带来机架内的带宽和低延迟收益. 下一个数据块会在这一块成功写入到三个Nodes后开始. When all three Nodes have successfully received the block they will send a “Block Received” report to the Name Node. They will also send “Success” messages back up the pipeline and close down the TCP sessions. The Client receives a success message and tells the Name Node the block was successfully written. The Name Node updates it metadata info with the Node locations of Block A in File.txt. The Client is ready to start the pipeline process again for the next block of data. 当所有的三个Node都成功接收了这个块, 它们会发送一个”Block Received”报告给Name Node. 它们也会给管道返回一个”Success”消息并关闭TCP协议. Client接收了一个Success消息, 通知Name Node块已经成功写入. Name Node更新File.txt中块A的Node位置的元数据信息. Client准备好开始下一个数据块的管道处理. As the subsequent blocks of File.txt are written, the initial node in the pipeline will vary for each block, spreading around the hot spots of in-rack and cross-rack traffic for replication. 当File.txt中随后的块都被写入, 管道中初始的node会为每一个块做相应的变化, 在机架内的热点间传播, 在机架间复制. Hadoop uses a lot of network bandwidth and storage. We are typically dealing with very big files, Terabytes in size. And each file will be replicated onto the network and disk (3) times. If you have a 1TB file it will consume 3TB of network traffic to successfully load the file, and 3TB disk space to hold the file. Hadoop使用大量的网络带宽和存储空间. 特别是当我们处理非常大的文件时, TB量级的. 每个文件将3倍地复制到网络和磁盘上. 如果你有一个1TB的文件, 它将消耗3TB的网络来成功地加载文件, 以及3TB的磁盘空间来保存这个文件. After the replication pipeline of each block is complete the file is successfully written to the cluster. As intended the file is spread in blocks across the cluster of machines, each machine having a relatively small part of the data. The more blocks that make up a file, the more machines the data can potentially spread. The more CPU cores and disk drives that have a piece of my data mean more parallel processing power and faster results. This is the motivation behind building large, wide clusters. To process more data, faster. When the machine count goes up and the cluster goes wide, our network needs to scale appropriately. 在每个块的复制管道都完成后, 文件就成功地写入集群了. 为了文件在集群机器的块之间传播, 每个机器有相对小的一部分数据. 文件分割的块越多, 数据可能传播的机器就越多. 一块数据有更多的CPU核心和磁盘驱动意味着更好的并行处理能力和更快的获得结果. 这是建立更大更宽的集群的背后的动机. 为了更快处理更多的数据. 当机器数增长, 集群变宽, 我们的网络需要合适的规模. Another approach to scaling the cluster is to go deep. This is where you scale up the machines with more disk drives and more CPU cores. Instead of increasing the number of machines you begin to look at increasing the density of each machine. In scaling deep, you put yourself on a trajectory where more network I/O requirements may be demanded of fewer machines. In this model, how your Hadoop cluster makes the transition to 10GE nodes becomes an important consideration. 纵向发展是规模化集群的另一种方法. 这即是你用更多的磁盘驱动和更多的CPU核数纵向扩展机器. 相对增加机器数量, 取而代之的是增加每个机器的密度. 在纵向化时, 你将趋向于更多的网络I/O需求使用更少的机器. 在这种模式下, 你的Hasoop集群怎样变迁到10GE nodes成为一个重点. The Name Node holds all the file system metadata for the cluster and oversees the health of Data Nodes and coordinates access to data. The Name Node is the central controller of HDFS. It does not hold any cluster data itself. The Name Node only knows what blocks make up a file and where those blocks are located in the cluster. The Name Node points Clients to the Data Nodes they need to talk to and keeps track of the cluster’s storage capacity, the health of each Data Node, and making sure each block of data is meeting the minimum defined replica policy. Name Node控制集群的所有的文件系统元数据, 监督Data Nodes的健康和协调数据入口. Name Node时HDFS的控制中心. 它自己不控制任何集群数据. Name Node只知道文件由什么块组成, 和那些块在集群中的位置. Name Node从Client指向Data Nodes, 它们需要交流来保持集群存储能力和每个Data Node健康的轨迹, 并且确认每个数据的块保证最低限度的复制. Data Nodes send heartbeats to the Name Node every 3 seconds via a TCP handshake, using the same port number defined for the Name Node daemon, usually TCP 9000. Every tenth heartbeat is a Block Report, where the Data Node tells the Name Node about all the blocks it has. The block reports allow the Name Node build its metadata and insure (3) copies of the block exist on different nodes, in different racks. Data Node通过TCP握手每3秒发送心跳给Name Node, 为Name Node使用同一个确定的端口号, 通常是TCP 9000. 每第10个心跳是一个块报告, 关于Data Node告诉Name Node所有它有的块. 块报告允许Name Node建立它的元数据和确认块的3个复制存在于不同机架的不同的node上. The Name Node is a critical component of the Hadoop Distributed File System (HDFS). Without it, Clients would not be able to write or read files from HDFS, and it would be impossible to schedule and execute Map Reduce jobs. Because of this, it’s a good idea to equip the Name Node with a highly redundant enterprise class server configuration; dual power supplies, hot swappable fans, redundant NIC connections, etc. Name Node是Hadoop分布式文件系统的一个关键的组件. 没有它, Clients将不能从HDFS写和读文件, 而且也不能规划和执行Map Reduce工作. 归功于它, 用高冗余企业类服务器配置安装Name Node是一个好主意. 双重支持, 热交换, 冗余NIC连接, 等. If the Name Node stops receiving heartbeats from a Data Node it presumes it to be dead and any data it had to be gone as well. Based on the block reports it had been receiving from the dead node, the Name Node knows which copies of blocks died along with the node and can make the decision to re-replicate those blocks to other Data Nodes. It will also consult the Rack Awareness data in order to maintain the two copies in one rack, one copy in another rack replica rule when deciding which Data Node should receive a new copy of the blocks. Consider the scenario where an entire rack of servers falls off the network, perhaps because of a rack switch failure, or power failure. The Name Node would begin instructing the remaining nodes in the cluster to re-replicate all of the data blocks lost in that rack. If each server in that rack had a modest 12TB of data, this could be hundreds of terabytes of data that needs to begin traversing the network. Hadoop has server role called the Secondary Name Node. A common misconception is that this role provides a high availability backup for the Name Node. This is not the case. The Secondary Name Node occasionally connects to the Name Node (by default, ever hour) and grabs a copy of the Name Node’s in-memory metadata and files used to store metadata (both of which may be out of sync). The Secondary Name Node combines this information in a fresh set of files and delivers them back to the Name Node, while keeping a copy for itself. Should the Name Node die, the files retained by the Secondary Name Node can be used to recover the Name Node. In a busy cluster, the administrator may configure the Secondary Name Node to provide this housekeeping service much more frequently than the default setting of one hour. Maybe every minute. When a Client wants to retrieve a file from HDFS, perhaps the output of a job, it again consults the Name Node and asks for the block locations of the file. The Name Node returns a list of each Data Node holding a block, for each block. The Client picks a Data Node from each block list and reads one block at a time with TCP on port 50010, the default port number for the Data Node daemon. It does not progress to the next block until the previous block completes. There are some cases in which a Data Node daemon itself will need to read a block of data from HDFS. One such case is where the Data Node has been asked to process data that it does not have locally, and therefore it must retrieve the data from another Data Node over the network before it can begin processing. This is another key example of the Name Node’s Rack Awareness knowledge providing optimal network behavior. When the Data Node asks the Name Node for location of block data, the Name Node will check if another Data Node in the same rack has the data. If so, the Name Node provides the in-rack location from which to retrieve the data. The flow does not need to traverse two more switches and congested links find the data in another rack. With the data retrieved quicker in-rack, the data processing can begin sooner, and the job completes that much faster. Now that File.txt is spread in small blocks across my cluster of machines I have the opportunity to provide extremely fast and efficient parallel processing of that data. The parallel processing framework included with Hadoop is called Map Reduce, named after two important steps in the model; Map, and Reduce. The first step is the Map process. This is where we simultaneously ask our machines to run a computation on their local block of data. In this case we are asking our machines to count the number of occurrences of the word “Refund” in the data blocks of File.txt. To start this process the Client machine submits the Map Reduce job to the Job Tracker, asking “How many times does Refund occur in File.txt” (paraphrasing Java code). The Job Tracker consults the Name Node to learn which Data Nodes have blocks of File.txt. The Job Tracker then provides the Task Tracker running on those nodes with the Java code required to execute the Map computation on their local data. The Task Tracker starts a Map task and monitors the tasks progress. The Task Tracker provides heartbeats and task status back to the Job Tracker. As each Map task completes, each node stores the result of its local computation in temporary local storage. This is called the “intermediate data”. The next step will be to send this intermediate data over the network to a Node running a Reduce task for final computation. While the Job Tracker will always try to pick nodes with local data for a Map task, it may not always be able to do so. One reason for this might be that all of the nodes with local data already have too many other tasks running and cannot accept anymore. In this case, the Job Tracker will consult the Name Node whose Rack Awareness knowledge can suggest other nodes in the same rack. The Job Tracker will assign the task to a node in the same rack, and when that node goes to find the data it needs the Name Node will instruct it to grab the data from another node in its rack, leveraging the presumed single hop and high bandwidth of in-rack switching. The second phase of the Map Reduce framework is called, you guess it, Reduce. The Map task on the machines have completed and generated their intermediate data. Now we need to gather all of this intermediate data to combine and distill it for further processing such that we have one final result. The Job Tracker starts a Reduce task on any one of the nodes in the cluster and instructs the Reduce task to go grab the intermediate data from all of the completed Map tasks. The Map tasks may respond to the Reducer almost simultaneously, resulting in a situation where you have a number of nodes sending TCP data to a single node, all at once. This traffic condition is often referred to as TCP Incast or “fan-in”. For networks handling lots of Incast conditions, it’s important the network switches have well-engineered internal traffic management capabilities, and adequate buffers (not too big, not too small). Throwing gobs of buffers at a switch may end up causing unwanted collateral damage to other traffic. But that’s a topic for another day. The Reducer task has now collected all of the intermediate data from the Map tasks and can begin the final computation phase. In this case, we are simply adding up the sum total occurrences of the word “Refund” and writing the result to a file called Results.txt The output from the job is a file called Results.txt that is written to HDFS following all of the processes we have covered already; splitting the file up into blocks, pipeline replication of those blocks, etc. When complete, the Client machine can read the Results.txt file from HDFS, and the job is considered complete. Our simple word count job did not result in a lot of intermediate data to transfer over the network. Other jobs however may produce a lot of intermediate data – such as sorting a terabyte of data. Where the output of the Map Reduce job is a new set of data equal to the size of data you started with. How much traffic you see on the network in the Map Reduce process is entirely dependent on the type job you are running at that given time. If you’re a studious network administrator, you would learn more about Map Reduce and the types of jobs your cluster will be running, and how the type of job affects the traffic flows on your network. If you’re a Hadoop networking rock star, you might even be able to suggest ways to better code the Map Reduce jobs so as to optimize the performance of the network, resulting in faster job completion times. Hadoop may start to be a real success in your organization, providing a lot of previously untapped business value from all that data sitting around. When business folks find out about this you can bet that you’ll quickly have more money to buy more racks of servers and network for your Hadoop cluster. When you add new racks full of servers and network to an existing Hadoop cluster you can end up in a situation where your cluster is unbalanced. In this case, Racks 1 &amp; 2 were my existing racks containing File.txt and running my Map Reduce jobs on that data. When I added two new racks to the cluster, my File.txt data doesn’t auto-magically start spreading over to the new racks. All the data stays where it is. The new servers are sitting idle with no data, until I start loading new data into the cluster. Furthermore, if the servers in Racks 1 &amp; 2 are really busy, the Job Tracker may have no other choice but to assign Map tasks on File.txt to the new servers which have no local data. The new servers need to go grab the data over the network. As as result you may see more network traffic and slower job completion times. To fix the unbalanced cluster situation, Hadoop includes a nifty utility called, you guessed it, balancer. Balancer looks at the difference in available storage between nodes and attempts to provide balance to a certain threshold. New nodes with lots of free disk space will be detected and balancer can begin copying block data off nodes with less available space to the new nodes. Balancer isn’t running until someone types the command at a terminal, and it stops when the terminal is canceled or closed. The amount of network traffic balancer can use is very low, with a default setting of 1MB/s. This setting can be changed with the dfs.balance.bandwidthPerSec parameter in the file hdfs-site.xml The Balancer is good housekeeping for your cluster. It should definitely be used any time new machines are added, and perhaps even run once a week for good measure. Given the balancers low default bandwidth setting it can take a long time to finish its work, perhaps days or weeks. Wouldn’t it be cool if cluster balancing was a core part of Hadoop, and not just a utility? I think so. This material is based on studies, training from Cloudera, and observations from my own virtual Hadoop lab of six nodes. Everything discussed here is based on the latest stable release of Cloudera’s CDH3 distribution of Hadoop. There are new and interesting technologies coming to Hadoop such as Hadoop on Demand (HOD) and HDFS Federations, not discussed here, but worth investigating on your own if so inclined. Download: Slides - PDF Slides and Text - PDF Cheers, Brad 最后要说的是：博客源码 ， 欢迎 star]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Coding动态pages部署WordPress]]></title>
    <url>%2Farchives%2F638b585f.html</url>
    <content type="text"><![CDATA[什么都别说了，先上图 部署好的wordpress效果（我主要是做一个相册集） 总体效果还不错吧，毕竟动态博客还是可操作性强多了。 好了，进入正题。 做这个其实就是前几天在coding的pages服务菜单中发现还有静态和动态两个选项卡，当时就懵逼了，pages服务还能动态？ 然后看了说明，动态是可以，但是限制还是蛮多的。 动态 Pages 是一个动态网页托管和演示服务，支持使用 PHP 语言和 MySQL 数据库，可用于部署开源博客、CMS 等动态应用。 只能使用php语言，然后数据库其实是coding自己的服务器提供的，然后整个服务器后台也是coding提供，所以自己是不可能做什么修改的。当然做一个wordpress博客还是绰绰有余，下面就是我的wordpress仓库文件。 搭建过程也非常简单，就是coding新建一个仓库，然后去wordpress官网上下载最新的wordpress压缩包，解压之后push到coding仓库中。 然后在Pages服务中开启动态Pages，选择部署来源为master分支，稍等一下就自动部署完成了。 打开动态pages运行的url，然后就是5分钟流程了。 所有的连接信息(共5个)都在这里，只有前4个用得上 存在的问题： 使用过程中在wordpress管理后台中下载好了主题和插件，但是在coding仓库中却没有对应的文件增加。所以在偶尔出现数据库连接错误或其他问题需要重新部署时，原本设置好的插件和主题就没有了 由于动态pages使用的是coding自己的服务器，所以个人没法修改服务器的一些设置，比如上传文件的大小限制，图片的分辨率等，所以上传的大图需要自己压缩一下再传。 最后建议： 把连接信息保存到一个文件wp-config.php，放到根目录下。 所有的主题和插件去源网页下载文件，保存到wp-content下的对应文件夹下，然后提交到coding 最后要说的是：博客源码 ， 欢迎 star]]></content>
      <categories>
        <category>Solution</category>
      </categories>
      <tags>
        <tag>WordPress</tag>
        <tag>Coding-pages</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GitBook一键发布到gh-pages]]></title>
    <url>%2Farchives%2Fb2973de0.html</url>
    <content type="text"><![CDATA[背景这段时间在读一本英文书，读的很慢，可以说是逐词翻译了。 然而读的时候总是会忘了前面的生词是什么意思，也没有纸质打印版，所以想到边读边做笔记，主要就是生词注释一下。 于是想到了GitBook。 首先我在GitBook上创建一本书，书名是Hadoop-The Definitive Guide 4th Edition。 然后打开就可以直接编辑。 但是gitbook的编辑器很难用啊，好像原来是直接写markdown的，现在改了编辑模式？特别是插入连接的时候，没法像[]()这么方便啊。 而且重要的是gitbook服务器加载速度不稳定，慢的时候都打不开了，所以想着直接把gitbook的Markdown文件内容编译成静态页面，发布到github仓库中，利用gh-pages直接访问，速度快多了。 连接github仓库现在说说怎么部署到gh-pages。 首先在github创建一个仓库，Hadoop-The-Definitive-Guide-4th，并初始化。 然后到gitbook的书籍Hadoop-The Definitive Guide 4th Edition的设置里找到Github，添加对应Hadoop-The-Definitive-Guide-4th仓库并同步内容 之后可以在github仓库中看到一些文件 这些都是gitbook书的markdown文件。这一步完成后，就可以在gitbook或github任意一端编译文档，提交后都会在两端生成的相应书籍。这相当于书籍在两端都有备份了。 如果不用gh-pages生成页面的话，上述的操作就已经够了。 提交gh-pages分支接下来介绍如何提交静态页面到gh-pages。 由于要生成静态页面的文件，需要在本地安装gitbook的npm包(推荐使用cnpm安装)。1npm install gitbook-cli -g 然后把github仓库clone到本地1git clone git@github.com:fakeYanss/Hadoop-The-Definitive-Guide-4th.git 进入到Hadoop-The-Definitive-Guide-4th文件夹，生成静态页面文件，输出目录在_book中。如果目录文件SUMMARY.md有变化，需要先gitbook init。1gitbook build 然后在本地创建一个gh-pages分支1git checkout --orphan gh-pages 然后清空一下分支下的文件（如果有的话）1rm -rf * 然后将master分支下的_book静态页面文件内容全部复制到gh-pages分支下1git checkout master -- _book 将_book中的子文件全部移到外层，并删除_book12mv _book/* ./rm -rf _book 这时候gh-pages分支下就是全部的静态页面文件了，接下来就是提交到远程gh-pages分支123git add .git commit -m &apos;publish gh-pages&apos;git push origin gh-pages 提交完成后到github仓库的设置中看一下，gh-pages服务是否自动开启，如果没有的话在Source中选择gh-pages branch，保存刷新，等待几分钟就好了。 全部操作已经完成，接下来每次在本地更新书籍内容后，先生成静态页面，然后提交master分支，再提交gh-pages分支就可以了。 之后每次查看线上gitbook书籍，可以直接输入url https://name.github.io/书籍仓库名查看。 最后，为了每次的提交操作不用手打一遍，我写了一个bash脚本publish.sh，点击下载，自行更改第一行的文件夹地址即可。windows系统安装过git环境的可以直接双击运行，要查看日志的话可以在git bash中输入./publish.sh运行。 注意：使用时不能将脚本放在仓库里，不然在切换分支时会出错，最好与仓库文件夹同级。 最后要说的是：博客源码 ， 欢迎 star]]></content>
      <categories>
        <category>Solution</category>
      </categories>
      <tags>
        <tag>GitBook</tag>
        <tag>gh-pages</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ThoughtWorks笔试题]]></title>
    <url>%2Farchives%2Ffd067be5.html</url>
    <content type="text"><![CDATA[2018_SPRING_DEV题目下载 解决思路 首先需要一个逐行读取文件内容的方法， 构造文件输入流，再构造Scanner类输入即可。然后将读取的逐行信息切分为一个数组，保存到ArrayList1中；再以ArrayList2嵌套ArrayList1即可 在main方法中获取键盘输入信息作为消息序号id，然后遍历从第0条到第id条消息，得出第id条消息的输出 步骤 新建一个input.txt文件记录无人机活动信号 12345plane1 1 1 1plane1 1 1 1 1 2 3plane1 2 3 4 1 1 1plane1 3 4 5plane1 1 1 1 1 2 3 新建一个PositionOfPlane.java 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394import java.io.FileInputStream;import java.io.FileNotFoundException;import java.util.ArrayList;import java.util.List;import java.util.Scanner;import java.util.regex.Pattern;/** * Auther: 桂晨 * Date: 2018年1月23日00:14:33 * */public class PositionOfPlane &#123; public static String PLANEID; //无人机编号 public static String PATH = "input.txt"; //记录无人机活动信号的文本文件路径 public static void main(String args[]) &#123; List&lt;List&lt;Integer&gt;&gt; plane; try &#123; plane = ReadFile(PATH); System.out.println("请输入消息序号(自然数):"); Scanner sc = new Scanner(System.in); boolean flag = true; String _id = ""; int id; //判断输入是不是自然数 while(flag) &#123; _id = sc.next(); Pattern pattern = Pattern.compile("[0-9]*"); if(pattern.matcher(_id).matches())&#123; flag = false; &#125;else&#123; System.out.println("请重新输入"); &#125; &#125; id = Integer.valueOf(_id); //将输入消息序号id分为三种情况，0，超出数据集，和在数据集中(不为0) if (id == 0) &#123; System.out.println(PLANEID + " " + id + " " + plane.get(0).get(0) + " " + plane.get(0).get(1) + " " + plane.get(0).get(2)); &#125; else if (id &gt; plane.size() - 1) &#123; System.out.println("Cannot find " + id); &#125; else &#123; Print(id, plane); &#125; &#125; catch (FileNotFoundException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; //输入消息序号id和信号数据plane，打印结果 public static void Print(int id, List&lt;List&lt;Integer&gt;&gt; plane) &#123; int num = 1; int x, y, z, offsetx, offsety, offsetz; x = plane.get(0).get(0); y = plane.get(0).get(1); z = plane.get(0).get(2); while (num &lt;= id) &#123; if (plane.get(num).size() != 6 || x != plane.get(num).get(0) || y != plane.get(num).get(1) || z != plane.get(num).get(2)) &#123; System.out.println("Error: " + id); return; &#125; offsetx = plane.get(num).get(3); offsety = plane.get(num).get(4); offsetz = plane.get(num).get(5); x += offsetx; y += offsety; z += offsetz; num++; &#125; System.out.println(PLANEID + " " + id + " " + x + " " + y + " " + z); &#125; //使用Scanner类nextLine()方法，读取文件每一行的数据，并将每个数据切分，保存到List&lt;List&lt;&gt;&gt;的嵌套集合(动态二维数组)中 public static List&lt;List&lt;Integer&gt;&gt; ReadFile(String path) throws FileNotFoundException &#123; FileInputStream fis = new FileInputStream(path); Scanner scanner = new Scanner(fis); String[] str; List&lt;List&lt;Integer&gt;&gt; plane = new ArrayList&lt;List&lt;Integer&gt;&gt;(); while (scanner.hasNextLine()) &#123; List&lt;Integer&gt; col = new ArrayList&lt;Integer&gt;(); str = scanner.nextLine().split(" "); for (int i = 1; i &lt; str.length; i++) &#123; col.add(Integer.parseInt(str[i])); &#125; PLANEID = str[0]; plane.add(col); &#125; scanner.close(); return plane; &#125;&#125; 运行先编译生成字节码 1javac -encoding utf-8 PositionOfPlane.java 然后运行 1java PositionOfPlane 然后输入ID 12（或其他数字） 2017秋季武汉招聘题目PDF download 思路主要是对字符串的切分处理。其中为了调试方便，将控制台输入数据转为json数据，然后再io流和gson包读取 题解下载全部eclipse项目文件 代码ReadJson.java123456789101112131415161718192021222324252627package pers.yanss.badmintonCourt.utils;import java.io.FileReader;import com.google.gson.*;public class ReadJson &#123; /** * 传入json文件名，解析json数据，将需要的信息保存到一个一维数组str中 * * @param fileName * @return str * @throws Exception */ public String[] Read(String fileName) throws Exception &#123; // 创建json解析器 JsonParser parser = new JsonParser(); String[] str; JsonObject object = (JsonObject) parser.parse(new FileReader("resource/" + fileName + ".json")); JsonArray scanIn = object.getAsJsonArray("scanIn"); str = new String[scanIn.size()]; for (int i = 0; i &lt; scanIn.size(); i++) &#123; JsonObject _scanIn = scanIn.get(i).getAsJsonObject(); str[i] = _scanIn.get("str").toString().replaceAll("\"", ""); &#125; return str; &#125;&#125; 代码DataUtils.java123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106package pers.yanss.badmintonCourt.utils;import java.text.ParseException;import java.text.SimpleDateFormat;import java.util.Calendar;public class DateUtils &#123; /** * 判断日期格式是否合法，合法返回true * * @param str * @return convertSuccess */ public boolean isValidDate(String str) &#123; boolean convertSuccess = true; // 指定日期格式为四位年/两位月份/两位日期，注意yyyy-MM-dd区分大小写 SimpleDateFormat format = new SimpleDateFormat("yyyy-MM-dd"); try &#123; // 设置lenient为false. // 否则SimpleDateFormat会比较宽松地验证日期，比如2007/02/29会被接受，并转换成2007/03/01 format.setLenient(false); format.parse(str); &#125; catch (ParseException e) &#123; // e.printStackTrace(); // 如果throw java.text.ParseException或者NullPointerException，就说明格式不对 convertSuccess = false; &#125; return convertSuccess; &#125; /** * 判断预订时间是否合法，合法返回相应数据，1表示验证时间通过，2表示时间输入不合法，3表示预订时间和之前预订过的有冲突，4表示取消的订单不存在 * * @param str * 预订时间段，如19:00~22:00 * @return convertSuccess */ public int isValidTime(String[][] str, int num) &#123; int convertSuccess = 1; String[] time = str[num][2].split("~"); if (Integer.parseInt(time[0].split(":")[0]) &gt;= Integer.parseInt(time[1].split(":")[0])) &#123; convertSuccess = 2; str[num][5] = "0"; &#125; if (!(time[0].split(":")[1].equals("00") &amp;&amp; time[1].split(":")[1].equals("00"))) &#123; convertSuccess = 2; str[num][5] = "0"; &#125; if (str[num][4].equals(" ")) &#123; // 输入信息是下订单 for (int i = 0; i &lt; num; i++) &#123; if (str[i][1].equals(str[num][1]) &amp;&amp; str[i][3].equals(str[num][3]) &amp;&amp; str[i][4].equals(" ") &amp;&amp; str[i][5].equals("1")) &#123; // 如果预订时间段的起始时间小于之前预订信息的结束时间，或者预订时间段的结束时间大于之前预订信息的起始时间，则冲突 if (Integer.parseInt(str[num][2].split("~")[0].split(":")[0]) &lt; Integer .parseInt(str[i][2].split("~")[1].split(":")[0])) &#123; convertSuccess = 3; str[num][5] = "0"; &#125; if (Integer.parseInt(str[num][2].split("~")[1].split(":")[0]) &gt; Integer .parseInt(str[i][2].split("~")[0].split(":")[0])) &#123; convertSuccess = 3; str[num][5] = "0"; &#125; &#125; &#125; &#125; else &#123; // 输入信息是取消订单 for (int i = 0; i &lt; num; i++) &#123; if (str[i][1].equals(str[num][1]) &amp;&amp; str[i][2].equals(str[num][2]) &amp;&amp; str[i][3].equals(str[num][3]) &amp;&amp; str[i][4].equals(" ") &amp;&amp; str[i][5].equals("1")) &#123; convertSuccess = 1; str[i][5] = "0"; str[i][6] = "1"; str[num][5] = "0"; break; &#125; else &#123; str[num][5] = "0"; convertSuccess = 4; &#125; &#125; &#125; return convertSuccess; &#125; /** * 判断当前日期是星期几 * @param pTime 要判断的时间 * @return dayForWeek 判断结果 * @Exception 发生异常 */ public int dayForWeek(String pTime) throws Exception &#123; SimpleDateFormat format = new SimpleDateFormat("yyyy-MM-dd"); Calendar c = Calendar.getInstance(); c.setTime(format.parse(pTime)); int dayForWeek = 0; if (c.get(Calendar.DAY_OF_WEEK) == 1) &#123; dayForWeek = 7; &#125; else &#123; dayForWeek = c.get(Calendar.DAY_OF_WEEK) - 1; &#125; return dayForWeek; &#125;&#125; 代码PringBill.java123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151package pers.yanss.badmintonCourt.utils;public class PrintBill &#123; static int total = 0; /** * 打印输出账单信息 * * @param str * 保存输入信息的字符串数组 * @param txt * 保存输出信息的字符串 * @return txt * @throws Exception */ public String Print(String[][] str, String txt) throws Exception &#123; System.out.println(); txt += "\r\n"; System.out.println("&gt; 收入汇总"); txt += "&gt; 收入汇总" + "\r\n"; System.out.println("&gt; ---"); txt += "&gt; ---" + "\r\n"; System.out.println("&gt; 场地:A"); txt += "&gt; 场地:A" + "\r\n"; txt = Court("A", str, txt); System.out.println("&gt;"); txt += "&gt;" + "\r\n"; System.out.println("&gt; 场地:B"); txt += "&gt; 场地:B" + "\r\n"; txt = Court("B", str, txt); System.out.println("&gt;"); txt += "&gt;" + "\r\n"; System.out.println("&gt; 场地:C"); txt += "&gt; 场地:C" + "\r\n"; txt = Court("C", str, txt); System.out.println("&gt;"); txt += "&gt;" + "\r\n"; System.out.println("&gt; 场地:D"); txt += "&gt; 场地:D" + "\r\n"; txt = Court("D", str, txt); System.out.println("&gt; ---"); txt += "&gt; ---" + "\r\n"; System.out.println("&gt; 总计: " + total + " 元"); txt += "&gt; 总计: " + total + " 元" + "\r\n"; return txt; &#125; /** * 根据羽毛球场地输出账单信息 * * @param type * 羽毛球场地，可以为A,B,C,D * @param str * 保存输入信息的字符串数组 * @param txt * 保存输出信息的字符串 * @return txt * @throws Exception */ private static String Court(String type, String[][] str, String txt) throws Exception &#123; DateUtils dateUtils = new DateUtils(); int money = 0; int subtotal = 0; for (int i = 0; i &lt; str.length; i++) &#123; if (str[i][3].equals(type)) &#123; int m = Integer.parseInt(str[i][2].split("~")[0].split(":")[0]); int n = Integer.parseInt(str[i][2].split("~")[1].split(":")[0]); if (str[i][5].equals("1")) &#123; money = Bill(dateUtils.dayForWeek(str[i][1]), m, n); subtotal += money; total += subtotal; System.out.println("&gt; " + str[i][1] + " " + str[i][2] + " " + money + " 元"); txt += "&gt; " + str[i][1] + " " + str[i][2] + " " + money + " 元" + "\r\n"; &#125; else if (str[i][5].equals("0") &amp;&amp; str[i][6].equals("1")) &#123; money = Bill(dateUtils.dayForWeek(str[i][1]), m, n) / 2; subtotal += money; System.out.println("&gt; " + str[i][1] + " " + str[i][2] + " 违约金 " + money + " 元"); txt += "&gt; " + str[i][1] + " " + str[i][2] + " 违约金 " + money + " 元" + "\r\n"; &#125; &#125; &#125; System.out.println("&gt; 小计: " + subtotal + " 元"); txt += "&gt; 小计: " + subtotal + " 元" + "\r\n"; return txt; &#125; /** * 根据预订时间，求出消费金额。 首先判断星期几，再对开始时间进行划分，然后对结束时间进行划分，最后根据单位时长金额和时长求出消费金额 * * @param day * 日期对应的星期几 * @param timeBegin * 预订时间的起始时间 * @param timeOver * 预订时间的结束时间 * @return money 消费金额 */ private static int Bill(int day, int timeBegin, int timeOver) &#123; int money = 0; if (day &gt;= 1 &amp;&amp; day &lt;= 5) &#123; if (timeBegin &lt; 12) &#123; if (timeOver &lt; 12) &#123; money = 30 * (timeOver - timeBegin); &#125; else if (timeOver &gt;= 12 &amp;&amp; timeOver &lt; 18) &#123; money = 30 * (12 - timeBegin) + 50 * (timeOver - 12); &#125; else if (timeOver &gt;= 18 &amp;&amp; timeOver &lt; 20) &#123; money = 30 * (12 - timeBegin) + 50 * (18 - 12) + 80 * (timeOver - 18); &#125; else &#123; money = 30 * (12 - timeBegin) + 50 * (18 - 12) + 80 * (20 - 18) + 60 * (timeOver - 20); &#125; &#125; else if (timeBegin &gt;= 12 &amp;&amp; timeBegin &lt; 18) &#123; if (timeOver &lt; 18) &#123; money = 50 * (timeOver - timeBegin); &#125; else if (timeOver &gt;= 18 &amp;&amp; timeOver &lt; 20) &#123; money = 50 * (18 - timeBegin) + 80 * (timeOver - 18); &#125; else &#123; money = 50 * (18 - timeBegin) + 80 * (20 - 18) + 60 * (timeOver - 20); &#125; &#125; else if (timeBegin &gt;= 18 &amp;&amp; timeBegin &lt; 20) &#123; if (timeOver &lt;= 20) &#123; money = 80 * (timeOver - timeBegin); &#125; else &#123; money = 80 * (20 - timeBegin) + 60 * (timeOver - 20); &#125; &#125; else &#123; money = 60 * (timeOver - timeBegin); &#125; &#125; else &#123; if (timeBegin &lt; 12) &#123; if (timeOver &lt; 12) &#123; money = 40 * (timeOver - timeBegin); &#125; else if (timeOver &gt;= 12 &amp;&amp; timeOver &lt; 18) &#123; money = 40 * (12 - timeBegin) + 50 * (timeOver - 12); &#125; else &#123; money = 40 * (12 - timeBegin) + 50 * (18 - 12) + 60 * (timeOver - 18); &#125; &#125; else if (timeBegin &gt;= 12 &amp;&amp; timeBegin &lt; 18) &#123; if (timeOver &lt; 18) &#123; money = 50 * (timeOver - timeBegin); &#125; else &#123; money = 50 * (18 - timeBegin) + 60 * (timeOver - 18); &#125; &#125; else &#123; money = 60 * (timeOver - timeBegin); &#125; &#125; return money; &#125;&#125; 代码TextToFile.java123456789101112131415161718192021222324252627282930package pers.yanss.badmintonCourt.utils;import java.io.File;import java.io.FileWriter;import java.io.IOException;public class TextToFile &#123; /** * 传入文件名以及字符串, 将字符串信息保存到文件中 * * @param strFilename * @param strBuffer */ public void toFile(final String strFilename, final String strBuffer) &#123; try &#123; // 创建文件对象 File fileText = new File(strFilename); // 向文件写入对象写入信息 FileWriter fileWriter = new FileWriter(fileText); // 写文件 fileWriter.write(strBuffer); // 关闭 fileWriter.close(); &#125; catch (IOException e) &#123; // e.printStackTrace(); &#125; &#125;&#125; 代码ChargeSolution.java12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697package pers.yanss.badmintonCourt.charge;import pers.yanss.badmintonCourt.utils.DateUtils;import pers.yanss.badmintonCourt.utils.PrintBill;import pers.yanss.badmintonCourt.utils.ReadJson;import pers.yanss.badmintonCourt.utils.TextToFile;public class ChargeSolution &#123; static String FILE = "data2";// 输入信息储存到了json文件中，这里只需改变json文件名即可改变输入信息 static String txt = "";// 输出信息保存到字符串txt中，最后将txt字符串信息写保存到一个文本文件中 public static void main(String[] args) throws Exception &#123; ReadJson json = new ReadJson(); String[] data = json.Read(FILE); String[][] str = new String[data.length][7];// data每个元素以" // "切分后最大长度为5，加上星期几和两个标志位后长度为7 DateUtils date = new DateUtils(); str = transArray(str, data); // System.out.println(date.dayForWeek("2017-09-10")); // System.out.println(str[2].split(" ")[0]); for (int i = 0; i &lt; str.length; i++) &#123; System.out.println(data[i]); txt += data[i] + "\r\n"; if (str[i][0].startsWith("U")) &#123; if (date.isValidDate(str[i][1])) &#123; if (date.isValidTime(str, i) == 1) &#123; if (str[i][3].equals("A") || str[i][3].equals("B") || str[i][3].equals("C") || str[i][3].equals("D")) &#123; if (str[i][4].equals("C") || str[i][4].equals(" ")) &#123; System.out.println("&gt; Success: the booking is accepted!"); txt += "&gt; Success: the booking is accepted!" + "\r\n"; &#125; else &#123; printInvalid(); &#125; &#125; else &#123; printInvalid(); &#125; &#125; else if (date.isValidTime(str, i) == 2) &#123; printInvalid(); &#125; else if (date.isValidTime(str, i) == 3) &#123; System.out.println("&gt; Error: the booking conflicts with existing bookings!"); txt += "&gt; Error: the booking conflicts with existing bookings!" + "\r\n"; &#125; else &#123; System.out.println("&gt; Error: the booking being cancelled does not exist!"); txt += "&gt; Error: the booking being cancelled does not exist!" + "\r\n"; &#125; &#125; else &#123; str[i][5] = "0"; printInvalid(); &#125; &#125; else &#123; str[i][5] = "0"; printInvalid(); &#125; &#125; PrintBill printBill = new PrintBill(); txt = printBill.Print(str, txt); TextToFile textToFile = new TextToFile(); textToFile.toFile("bill.txt", txt); &#125; /** * 打印不合法的输出语句 */ private static void printInvalid() &#123; System.out.println("&gt; Error: the booking is invalid!"); txt += "&gt; Error: the booking is invalid!" + "\r\n"; &#125; /** * 将一维数组每个元素以" "切分，转化为二维数组,并在每行后面加上两个标志位 第一个标志位，1表示成功预订，0表示预订失败或取消预订 * 第二个标志位，1表示取消订单成功，0表示没有取消订单 * * @param str * 二维数组 * @param data * 一维数组 * @return str 转化后的二维数组 * @throws Exception */ private static String[][] transArray(String[][] str, String[] data) throws Exception &#123; for (int i = 0; i &lt; str.length; i++) &#123; for (int j = 0; j &lt; str[0].length; j++) &#123; if (data[i].split(" ").length &gt; j) &#123; str[i][j] = data[i].split(" ")[j]; &#125; else if (j == 4) &#123; str[i][j] = " "; &#125; else if (j == 5) &#123; str[i][j] = "1"; &#125; else &#123; str[i][j] = "0"; &#125; &#125; &#125; return str; &#125;&#125; 最后要说的是：博客源码 ， 欢迎 star]]></content>
      <categories>
        <category>Inbox</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[面试记录]]></title>
    <url>%2Farchives%2F8e14516.html</url>
    <content type="text"><![CDATA[多酷游戏面试 2018-01-29 23:22:26 自我介绍 Spring注解 AOP 前端发来post请求，后端如何验证 Java常用集合，ArrayList，LinkedList，HashMap，LinkedHashMap ArrayList，LinkedList使用区别 Redis及其使用，支持的数据类型 MySQL数据库引擎，MyISAM，InnoDB 数据库索引，具体使用 多线程 爱奇艺面试 2018-01-22 23:08:29 项目经历，相关问题。 Java创建对象的方法有哪些？ new 反射 clone() 反序列化 Java基类Object的方法有哪些，有什么用？ 有getClass(), hashCode(), equals(), clone(), toString(), notify(), notifyAll(), wiat(), finalize()等 getClass()：获取对象类名 clone()：创建并返回对象的一份拷贝，有深拷贝和浅拷贝 toString()：输出“类的名字@实例”的16进制哈希码 wait()：让当前线程等待直到另一线程调用对象的notify()方法，或者知道超时timeout参数后唤醒 notify()：唤醒一个在此对象监视器上等待的线程 finalize()：在实例被垃圾回收器回收时触发的操作 Error和Exception的区别 谈谈擅长和兴趣，静态博客搭建 前端表格分页的方法 一些sql操作 滴滴Java面试 2018-01-19 23:06:32 自述 项目经历 String，是否继承 面向对象特征 重载和重写的区别 Mysql数据库引擎 LInux系统常用命令 SpringMVC源码有没有读过 摩拜Java面试 2018-01-18 23:09:00 自述 项目经历 Java常用集合，HashMap的常用方法 多线程，问及实际使用 写一个快速排序 百度-智慧客服和金牌销售部-java实习-面试 2017-12-31 11:05:59 聊项目，就项目提问，总结项目中的亮点 ArrayList和LinkedList的区别，分别什么情况使用 多线程：1个action有很多任务要进行，保证所有任务都完成，再返回一个完成信息 Linux 常用命令 查看线程 查看进程 redis的框架，原理，我只答了如何在项目中使用，以及查询数据库的运行机制 数据库：索引，索引分类，具体使用 数据结构：求二叉树中节点的最大长度 跟谁学-java面试 2017-12-31 11:18:24 JVM虚拟机 java垃圾回收机制 java对象存储机制，堆，栈 tomcat，写一个tomcat程序 java引用 maven管理，多个生产环境对应不同配置文件 spring-core源码 面试官的建议：熟读jvm，tomcat，spring源码 小米面试 2017-08-20 17:22:33数据结构 数据结构类型 链表找两个重复元素 二叉树左右对称交换位置 操作系统 进程和线程的区别 linux系统的常用操作 linux文件内查找 网络 TCP UDP区别， 三次握手的细节，数据报文怎么传输 数据库 基本操作 索引 Java arraylist linkedlist map 最后要说的是：博客源码 ， 欢迎 star]]></content>
      <categories>
        <category>Inbox</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Java不用for循环打印数组]]></title>
    <url>%2Farchives%2F8576f9e7.html</url>
    <content type="text"><![CDATA[ArrayList直接打印123456ArrayList&lt;Integer&gt; list = new ArrayList&lt;&gt;();list.add(1);list.add(2);list.add(3);list.add(4);System.out.println(list); 输出1[1, 2, 3, 4] Arrays类打印数组java.util.Arrays的toString()方法1System.out.println(Arrays.toString(new int[] &#123;1, 2, 3, 4&#125;)); 输出1[1, 2, 3, 4] Arrays类打印二维数组java.util.Arrays的deeptoString()方法1System.out.println(Arrays.deepToString(new int[][] &#123;&#123;1, 2&#125;, &#123;3, 4&#125;&#125;)); 输出1[[1, 2], [3, 4]] 最后要说的是：博客源码 ， 欢迎 star]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Array</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[初识机器学习]]></title>
    <url>%2Farchives%2Fcc2e4451.html</url>
    <content type="text"><![CDATA[Machine learningMachine learning definition: Arthur Samuel (1959) . 在没有明确设置的前提下，使机器具有学习能力的研究领域。 Tom Mitchell (1998) . 一个适当的学习问题定义如下：计算机程序从经验E中学习，解决某一任务T，进行某一性能度量P，通过P测试在T上的表现因经验E而提高。 对于跳棋游戏（Samuel设计的一个小游戏，通过数万次跳棋对战学习，获得比Samuel的跳棋水平还高的能力），经验E就是程序与自己下几万次跳棋，任务T就是玩跳棋，性能度量P就是与新对手玩跳棋时赢的概率。 Machine learning algorithms: 目前学习算法主要的两类是监督学习(supervised learning)和无监督学习(unsupervised learning)。 简单来说，监督学习就是我们教计算机做某件事情；在无监督学习中，我们让计算机自己学习。 Others: 强化学习(Reinforcement learning), 推荐系统(recommender systems) Supervised Learning监督学习：我们给算法一个数据集，其中包含了正确答案，算法的目的就是给出更多的正确答案。 回归(Regression)：预测连续的数值输出。 分类(Classification)：预测一个离散值输出。 示例：房子的价格与房子面积的关系(回归问题)；肿瘤是恶性或良性与肿瘤大小，患者年龄，肿瘤块厚度等的关系(分类问题)。 下面一个问题。problem1将要卖的货物数量看成一个连续的值，属于回归问题；problem2输出的值可能为0或1，分别表示两种不同的结果，属于分类问题。 Unsupervised Learning无监督学习：对于数据集中的每一个样本，都具有相同标签或都没有标签，我们不知道要拿数据做什么，也不知道每个数据点究竟是什么，只能在数据集种找到某种结构(簇)，它们具有类似的性质。聚类(clustering)是无监督学习的一种 。 Cocktail party problem 鸡尾酒会问题 编程环境Octave或Matlab 解决代码$$[W,s,v]=svd((repmat(sum(x.^*x,1),size(x,1),1).^*x)^*x’)$$ \(svd\)是奇异值分解的缩写，在Octave中作为一个内置函数。 下面一个问题，哪些选项要使用无监督学习算法？ 最后要说的是：博客源码 ， 欢迎 star]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>supervised learning</tag>
        <tag>unsupervised learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ubantu系统安装]]></title>
    <url>%2Farchives%2F67d3a6a5.html</url>
    <content type="text"><![CDATA[Ubantu系统安装教程 Ubantu Desktop最新版本下载，制作U盘启动盘推荐使用Rufus工具 Ubantu系统分区方案 在windows系统上把原硬盘压缩出50G的free空间，在Ubantu安装时分出2G作为swap分区 剩下的格式化为ext4格式，挂载位置为/ 由于现在PC内存都较大了，所以不必创建swap交换分区 设置安装启动引导器的设备 我的电脑有两块ssd，一块小的全部作为C盘，装的win10系统和开机启动软件；一块大的作为D盘，安装常用软件和存放一些资料。我把ubantu安装在了D盘上的一个50G分区，这样就要把ubantu的引导器放在D盘，也就是sdb （sda对应第一块硬盘，sdb对应第二块硬盘），这样的话电脑开机时会自动进入win10，如果我按F11才会进入grub选择ubantu系统，这样正好符合我的需求。 如果把引导器安装在C盘，每次开机都会手动选择系统。 最后要说的是：博客源码 ， 欢迎 star]]></content>
      <categories>
        <category>Solution</category>
      </categories>
      <tags>
        <tag>Ubantu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo的next主题个性化配置]]></title>
    <url>%2Farchives%2F14046165.html</url>
    <content type="text"><![CDATA[添加RSS首先在博客根目录安装hexo插件： 1$ npm install --save hexo-generator-feed npm安装失败请用cnpm 然后在博客配置文件_config.yml中修改 12345plugins: hexo-generate-feedfeed: type: atom #feed 类型 (atom/rss2) path: atom.xml #rss 路径 limit: 0 #在 rss 中最多生成的文章数(0显示所有) 然后在主题配置文件_config.yml中修改 1rss: /atom.xml 修改作者头像并旋转打开\themes\next\source\css\_common\components\sidebar\sidebar-author.styl，在里面添加如下代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960.site-author-image &#123; display: block; margin: 0 auto; padding: $site-author-image-padding; max-width: $site-author-image-width; height: $site-author-image-height; border: $site-author-image-border-width solid $site-author-image-border-color; /* 头像圆形 */ border-radius: 80px; -webkit-border-radius: 80px; -moz-border-radius: 80px; box-shadow: inset 0 -1px 0 #333sf; /* 设置循环动画 [animation: (play)动画名称 (2s)动画播放时长单位秒或微秒 (ase-out)动画播放的速度曲线为以低速结束 (1s)等待1秒然后开始动画 (1)动画播放次数(infinite为循环播放) ]*/ /* 鼠标经过头像旋转360度 */ -webkit-transition: -webkit-transform 1.0s ease-out; -moz-transition: -moz-transform 1.0s ease-out; transition: transform 1.0s ease-out;&#125;img:hover &#123; /* 鼠标经过停止头像旋转 -webkit-animation-play-state:paused; animation-play-state:paused;*/ /* 鼠标经过头像旋转360度 */ -webkit-transform: rotateZ(360deg); -moz-transform: rotateZ(360deg); transform: rotateZ(360deg);&#125;/* Z 轴旋转动画 */@-webkit-keyframes play &#123; 0% &#123; -webkit-transform: rotateZ(0deg); &#125; 100% &#123; -webkit-transform: rotateZ(-360deg); &#125;&#125;@-moz-keyframes play &#123; 0% &#123; -moz-transform: rotateZ(0deg); &#125; 100% &#123; -moz-transform: rotateZ(-360deg); &#125;&#125;@keyframes play &#123; 0% &#123; transform: rotateZ(0deg); &#125; 100% &#123; transform: rotateZ(-360deg); &#125;&#125; 添加相册我的相册。原理很简单，就是建立一个github仓库存储用于存储图片，然后将每个图片的路径保存到一个json文件里，在hexo博客中解析这个json文件，渲染成html页面后就可以在显示图片了。当然这里肯定要有页面的样式和图片的裁剪压缩，原理简单，实际操作起来有一些坑，我并不懂css样式，还是要感谢litten提供的方法。 相册源 首先在github上新建一个仓库，命名Blog_Album 然后本地新建一个文件夹Blog_Album，进入文件夹新建两个子文件夹photos，min_photos，然后下载这两个文件ImageProcess.py，tool.py到Blog_Album 记住之后要上传的相片就放到photos文件夹内。 在博客根目录运行 1hexo new photos 回到Blog_Album文件夹，将tools.py`中131行 123final_dict = &#123;&quot;list&quot;: list_info&#125; with open(&quot;D:/Blog/source/photos/data.json&quot;,&quot;w&quot;) as fp: json.dump(final_dict, fp) 将路径改为你的博客photos文件夹的相应位置 添加一些图片到Blog_Album的photos文件夹中 将本地Blog_Album与github仓库Blog_Album关联 运行tool.py脚本（因为脚本中有上传到github的函数，所以不用手动git push），如果不能运行请看文章下面的说明 将本地Blog_Album上传到github仓库Blog_Album 博客相册页 回到yourblog\source\photos目录下，将index.md内容修改为 123456789101112131415161718192021222324252627---title: 我的相册date: 2017-12-29 22:32:22type: "photos"---&lt;link rel="stylesheet" href="./ins.css"&gt; &lt;link rel="stylesheet" href="./photoswipe.css"&gt; &lt;link rel="stylesheet" href="./default-skin/default-skin.css"&gt; &lt;div class="photos-btn-wrap"&gt; &lt;a class="photos-btn active" href="javascript:void(0)"&gt;Photos&lt;/a&gt;&lt;/div&gt;&lt;div class="instagram itemscope"&gt; &lt;a href="http://yanss.top" target="_blank" class="open-ins"&gt;图片正在加载中…&lt;/a&gt;&lt;/div&gt; &lt;script&gt; (function() &#123; var loadScript = function(path) &#123; var $script = document.createElement('script') document.getElementsByTagName('body')[0].appendChild($script) $script.setAttribute('src', path) &#125; setTimeout(function() &#123; loadScript('./ins.js') &#125;, 0) &#125;)()&lt;/script&gt; 第13行链接为自己的博客url 然后在photos文件夹下添加这些内容 文件这里下载，data.json是图片的数据信息，运行python脚本后会生成 ins.js中的114行的render()函数需要修改这两个变量 12var minSrc = 'https://raw.githubusercontent.com/fakeYanss/Blog_Album/master/min_photos/' + data.link[i];var src = 'https://raw.githubusercontent.com/fakeYanss/Blog_Album/master/photos/' + data.link[i]; 如果你的仓库名和我相同，只用把这里的fakeYanss改为你自己的github name即可 在yourBlog/themes/next/source/js/src下加入两个js文件photoswipe.min.js 和photoswipe-ui-default.min.js 在yourBlog/themes/next/layout/_scripts/pages/post-details.swig中添加 12&lt;script type="text/javascript" src="&#123;&#123; url_for(theme.js) &#125;&#125;/src/photoswipe.min.js?v=&#123;&#123; theme.version &#125;&#125;"&gt;&lt;/script&gt;&lt;script type="text/javascript" src="&#123;&#123; url_for(theme.js) &#125;&#125;/src/photoswipe-ui-default.min.js?v=&#123;&#123; theme.version &#125;&#125;"&gt;&lt;/script&gt; 在yourBlog/themes/next/layout/_layout.swig中 head内插入 12&lt;script src="&#123;&#123; url_for(theme.js) &#125;&#125;/src/photoswipe.min.js?v=&#123;&#123; theme.version &#125;&#125;"&gt;&lt;/script&gt;&lt;script src="&#123;&#123; url_for(theme.js) &#125;&#125;/src/photoswipe-ui-default.min.js?v=&#123;&#123; theme.version &#125;&#125;"&gt;&lt;/script&gt; body内插入 1234567891011121314151617181920212223242526272829303132333435363738394041&#123;% if page.type === "photos" %&#125;&lt;!-- Root element of PhotoSwipe. Must have class pswp. --&gt;&lt;div class="pswp" tabindex="-1" role="dialog" aria-hidden="true"&gt; &lt;div class="pswp__bg"&gt;&lt;/div&gt; &lt;div class="pswp__scroll-wrap"&gt; &lt;div class="pswp__container"&gt; &lt;div class="pswp__item"&gt;&lt;/div&gt; &lt;div class="pswp__item"&gt;&lt;/div&gt; &lt;div class="pswp__item"&gt;&lt;/div&gt; &lt;/div&gt; &lt;div class="pswp__ui pswp__ui--hidden"&gt; &lt;div class="pswp__top-bar"&gt; &lt;div class="pswp__counter"&gt;&lt;/div&gt; &lt;button class="pswp__button pswp__button--close" title="Close (Esc)"&gt;&lt;/button&gt; &lt;button class="pswp__button pswp__button--share" title="Share"&gt;&lt;/button&gt; &lt;button class="pswp__button pswp__button--fs" title="Toggle fullscreen"&gt;&lt;/button&gt; &lt;button class="pswp__button pswp__button--zoom" title="Zoom in/out"&gt;&lt;/button&gt; &lt;!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR --&gt; &lt;!-- element will get class pswp__preloader--active when preloader is running --&gt; &lt;div class="pswp__preloader"&gt; &lt;div class="pswp__preloader__icn"&gt; &lt;div class="pswp__preloader__cut"&gt; &lt;div class="pswp__preloader__donut"&gt;&lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"&gt; &lt;div class="pswp__share-tooltip"&gt;&lt;/div&gt; &lt;/div&gt; &lt;button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"&gt; &lt;/button&gt; &lt;button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"&gt; &lt;/button&gt; &lt;div class="pswp__caption"&gt; &lt;div class="pswp__caption__center"&gt;&lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt;&lt;/div&gt;&#123;% endif %&#125; 重新生成博客内容即可看到相册内容。 如果py脚本不能运行，先安装python环境，再安装Pillow库pip install Pillow 相册图片的命名请遵循yyyy-mm-dd_abc.jpg格式，虽然脚本里写了其他格式的处理，但实际情况似乎只能对jpg裁剪压缩 最后的不足是，相片的裁剪算法，可能会把一张图片中的人体头部裁剪掉 next主题源码是不支持相册的，如果有不懂的地方，可以看看评论的相似情况，或者查一下yilia主题的issue，然后再来问我 2018.1.20修改：由于从github仓库读取图片，在html页面中会发生ios手机竖持拍照的照片90度旋转问题（图片的EXIF的orientaion信息在裁剪压缩后发生改变），找了一些办法都没效果，所以将相片源仓库转移到七牛云，利用七牛云外链后加上?imageMogr2/auto-orient的方式，可以将照片正常角度显示。如果要用七牛云做图床，可以看我的设置。 2018.2.23修改：发现ios设备拍摄的正方形照片的压缩图上传后仍然有orientation值为6即偏转90度问题，索性修改了整个裁剪切割和生成json的脚本。在压缩同时修改偏转值，并且在生成json时获取图片尺寸，在前端显示图片原图时在data-size中设置json中的对应图片尺寸，这样可以不切割原图，修改后的设置方法在 生成缩略图和json数据并上传七牛云， 对于修改后的相册的ins.js，需要做一些修改，以适应修改后的脚本生成的json数据，在render()方法中，注意内层&lt;img&gt;标签为缩略图，&lt;a&gt;标签为原图，在对应的属性处设为对应值。 12345678910111213141516var minSrc = 'http://p1ju2a9a7.bkt.clouddn.com/min_photos/' + data.link[i];//var src = 'https://raw.githubusercontent.com/fakeYanss/Blog_Album/master/photos/' + data.link[i];var src = 'http://p1ju2a9a7.bkt.clouddn.com/' + data.link[i];var type = data.type[i];var target = src + '.' +type;var size = data.size[i];src = src + (type === 'mp4' ? '.jpg' : '.' + type) + '?imageMogr2/auto-orient';minSrc = minSrc + (type === 'mp4' ? '.jpg' : '.' + type);type = (type === 'mp4' ? 'video' : 'image')liTmpl += '&lt;figure class="thumb" itemprop="associatedMedia" itemscope="" itemtype="http://schema.org/ImageObject"&gt;\ &lt;a href="' + src + '" itemprop="contentUrl" data-size="' + size + '" data-type="' + type + '" data-target="' + target + '"&gt;\ &lt;img class="reward-img" data-type="' + type + '" data-src="' + minSrc + '" src="./assets/empty.png" itemprop="thumbnail" onload="lzld(this)"&gt;\ &lt;/a&gt;\ &lt;figcaption style="display:none" itemprop="caption description"&gt;' + data.text[i] + '&lt;/figcaption&gt;\ &lt;/figure&gt;'; 所有文件的下载 其实啊，相册源文件和博客相册只是用一个json文件关联起来了，可以在实际处理中把相册源文件和博客分离开。也就是说我hexo博客不用管你图片怎么处理的，你的图片裁不裁剪压不压缩都没关系的，存在哪里也没关系的，我只要有一个json数据，并且json数据的格式和ins.js中的处理能对应上就行。 添加Gitment评论原本是用的livere评论，后来总是加载速度太慢，上了梯子也一样，索性改成了Gitment评论。 感谢作者imsun，Gitment评论源自github仓库的issue，所以在将博客地址关联了某个github仓库后，在博客下评论其实就是在对应仓库的issue中评论，这创意真是太好了。 我之前的next主题一直是5.1.0版本，本来是想在主题中添加gitment的js和css文件，结果没成功。然后在next的官方文档中看到已经发行到5.1.4了，而且已经集成了gitment评论。这下可方便，干脆直接升级了next主题，然后就改config文件就好啦。 以前主题中配置了一些设置项，时间久了还忘了改了哪些文件！！！所以升级版本很痛苦，用的Sublime的一个插件Sublimerge，可以对比两个文件的代码差异，就是这样就像git pull操作之后改动时一样，这样子把每个有可能改过的文件都对比了一遍，然后升级到了5.1.4，发现集成了很多新功能，其他的有时间再试吧，这里就只说gitment。 首先在这里注册一个OAuth Application，Homepage URL和Authorization callback URL填写博客首页地址，也就是站点配置文件中的url，其他随意填写即可。 然后会得到一个Client ID和Client Secret，把这两个值填到主题配置文件的gitment对应位置12345678910111213gitment: enable: true mint: true # gitment仓库有两个，这里填true是引用第一个，false引用第二个，具体在layout下的conment文件中可以找到 count: true # 评论计数 lazy: false # 如果要点击按钮再显示评论就填true cleanly: true # 隐藏底部信息 language: # Force language, or auto switch by theme github_user: 填github ID github_repo: 填保存issue的仓库名，一般就用博客发布的仓库名 client_id: 刚才的值 client_secret: 刚才的值 proxy_gateway: # 设置代理，不用填 redirect_protocol: # 没搞懂，不用填 然后重新部署博客(本地调试是没用的，因为url不同)，再打开博客，这时候需要在每一个有评论的页面上使用自己的github长航登录并初始化一遍评论，之后就不用了。文章多的话会有点麻烦，不知道gitment作者有没有做好自动初始化？好像查到了这个，不过我还没试过。这里是成功的样子 但是，这个鼠标放上有两条横线什么鬼啊！！！ 还有这里头像下面为什么有一条横线！！！！ 强迫症忍不了，查看了gitment的css定义，没发现什么问题啊，然后在浏览器中调试，发现了这个123456a&#123; color: #555; border-bottom: 1px solid #999; text-decoration: none; word-wrap: break-word;&#125; 这里的border-bottom: 1px solid #999就是a标签下有一条横线的意思，但是这个属性是主题的属性main.css啊，显然是不能改的，于是只有在themes\next\source\css\_common\components\third-party\gitment.styl下改动了，在这里可以重写前面定义的属性，我是这样改的，在最后面加上123456789101112.gitment-comment-main a&#123; color: #555; border-bottom: none; text-decoration: none; word-wrap: break-word;&#125;.gitment-editor-avatar&#123; color: #555; border-bottom: none; text-decoration: none; word-wrap: break-word;&#125; 第一个就是修改的ID下的横线，显示为none就好了；第二个是修改编辑框头像下的横线，也是显示为none。 这样，算是完成了Gitment的配置了。 设置自定义页面不显示Sidebar主题配置文件中是这样的1234567toc: enable: true number: true wrap: falsesidebar: position: left display: post 讲道理这样就是是没有问题的，但是我发现自定义的页面里如果写了太多的#或者&lt;h1&gt;，就会被识别为post类型而不是page，也就是博客文章，会被自动加载目录，这就很蛋疼了不是，毕竟有的页面不想要目录啊尴尬！！！ 一般这种样式问题都在layout文件夹中找原因。 在themes\next\layout\_macro\sidebar.swig，找到开头的12345678&#123;% macro render(is_post) %&#125; &lt;div class="sidebar-toggle"&gt; &lt;div class="sidebar-toggle-line-wrap"&gt; &lt;span class="sidebar-toggle-line sidebar-toggle-line-first"&gt;&lt;/span&gt; &lt;span class="sidebar-toggle-line sidebar-toggle-line-middle"&gt;&lt;/span&gt; &lt;span class="sidebar-toggle-line sidebar-toggle-line-last"&gt;&lt;/span&gt; &lt;/div&gt; &lt;/div&gt; 在下面加上1&#123;% if page.toc and theme.toc.enable %&#125; 然后在倒数第二行加上1&#123;% endif %&#125; 发现这样修改有bug，重新改。在themes\next\layout\_macro\sidebar.swig找到这一句1&#123;% set display_toc = is_post and theme.toc.enable or is_page and theme.toc.enable %&#125; 改为1&#123;% set display_toc = is_post and theme.toc.enable or is_page and page.toc or is_page and theme.toc.enable and page.toc %&#125; 其实就是多加一个判断，判断页面的开头有没有toc属性 最后，在需要有sidebar目录的文章前加上toc: true即可。 为了以后的方便，可以在scaffolds\post.md中加上toc: true。 最后要说的是：博客源码 ， 欢迎 star]]></content>
      <categories>
        <category>Solution</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Java垃圾回收浅析]]></title>
    <url>%2Farchives%2F52a381d.html</url>
    <content type="text"><![CDATA[GC的三种收集方法标记清除标记清除算法是最基础的收集算法，其他收集算法都是基于这种思想。 标记清除算法分为“标记”和“清除”两个阶段：首先标记出需要回收的对象，标记完成之后统一清除对象。 主要缺点： 效率问题，标记和清除过程效率不高 。 空间问题，标记清除之后会产生大量不连续的内存碎片。 标记整理标记整理，主要用于回收老年代。 标记操作和“标记-清除”算法一致，后续操作不只是直接清理对象，而是在清理无用对象完成后让所有存活的对象都向一端移动，并更新引用其对象的指针。 主要缺点：在标记-清除的基础上还需进行对象的移动，成本相对较高，好处则是不会产生内存碎片。 复制算法复制算法，主要用于回收新生代。 它将可用内存容量划分为大小相等的两块，每次只使用其中的一块。当这一块用完之后，就将还存活的对象复制到另外一块上面，然后在把已使用过的内存空间一次理掉。这样使得每次都是对其中的一块进行内存回收，不会产生碎片等情况，只要移动堆订的指针，按顺序分配内存即可，实现简单，运行高效。 主要缺点：内存缩小为原来的一半。 分代的垃圾回收策略分代的垃圾回收策略是基于这样一个事实：不同的对象的生命周期是不一样的。因此，不同生命周期的对象可以采取不同的回收算法，以便提高回收效率。 年轻代（Young Generation） 所有新生成的对象首先都是放在年轻代的。年轻代的目标就是尽可能快速的收集掉那些生命周期短的对象。 新生代内存按照8:1:1的比例分为一个eden区和两个survivor(survivor0,survivor1)区。一个Eden区，两个 Survivor区(一般而言)。大部分对象在Eden区中生成。 回收时先将eden区存活对象复制到一个survivor0区，然后清空eden区； 当这个survivor0区也存放满了时，则将eden区和survivor0区存活对象复制到另一个survivor1区，然后清空eden和这个survivor0区； 此时survivor0区是空的，然后将survivor0区和survivor1区交换，即保持survivor1区为空。 如此往复循环。 当survivor1区不足以存放 eden和survivor0的存活对象时，就将存活对象直接存放到老年代。若是老年代也满了就会触发一次Full GC，也就是新生代、老年代都进行回收 新生代发生的GC也叫做Minor GC，MinorGC发生频率比较高(不一定等Eden区满了才触发) 年老代（Old Generation） 在年轻代中经历了N次垃圾回收后仍然存活的对象，就会被放到年老代中。因此，可以认为年老代中存放的都是一些生命周期较长的对象。 内存比新生代也大很多(大概比例是1:2)，当老年代内存满时触发Major GC即Full GC，Full GC发生频率比较低，老年代对象存活时间比较长，存活率标记高。 持久代（Permanent Generation）用于存放静态文件，如Java类、方法等。持久代对垃圾回收没有显著影响，但是有些应用可能动态生成或者调用一些class，例如Hibernate 等，在这种时候需要设置一个比较大的持久代空间来存放这些运行过程中新增的类。 Oracle JDK8的HotSpot VM去掉“持久代”，以“元数据区”（Metaspace）替代之。 最后要说的是：博客源码 ， 欢迎 star]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>GC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018年应届硕士毕业生如何拿到知名互联网公司深度学习offer？]]></title>
    <url>%2Farchives%2F70b5d312.html</url>
    <content type="text"><![CDATA[本文作者：熊风 HKUST CS MPhil , 计算机视觉、机器学习 543 人赞同了该回答 最近投了一堆机器学习/深度学习/计算机视觉方向的公司，分享一下自己的经验，希望对大家有帮助。 个人背景： 华科本科 + 港科大硕士（MPhil） 拿到的offer有腾讯优图，阿里AI lab，今日头条，滴滴研究院，商汤科技，旷视（face++），大疆，快手。绝大部分是ssp（super special），给到了普通硕士能给到的最高档，例如滴滴的offer是滴滴新锐，腾讯的offer是技术大咖等等。这些offer大部分待遇在40W-50W之间，个别公司算上期权能到60W 写在前面的话这个回答的适用对象主要还是本科和硕士。PhD找工作的套路跟硕士还是很不一样的，所以这个回答的经验对于手握几篇一作顶会的PhD大神并没啥参考意义。 我也和我们实验室几个找工作的PhD学长学姐聊过，他们的面试主要是讲自己的research，有的甚至就是去公司给个talk，跟本科硕士的校招流程完全不同。现在也是AI方向PhD的黄金时代，没毕业就被各大公司主动联系，待遇也比我这种硕士高很多很多。 一. 整体建议一定要找内推。内推一般有两种，第一种力度比较弱，在公司的内推系统上填一下你的名字，加快一下招聘流程；第二种力度比较强，直接把简历送到部门负责人手上。个人比较建议第二种，会省事很多。 原因如下： （1）现在做机器学习的人实在太多了，在不找内推的情况下，流程会特别特别慢。即使你的简历比较优秀，也可能淹没在茫茫大海中，不一定能被懂行的人看到。 （2）现在很多公司的笔试其实挺有难度的，就算是大神也有翻车的可能性。 （3）对于大公司而言，即使通过了简历筛选、笔试那一关，你也很难保证你的简历被合适的部门挑中。很可能过关斩将后，发现给你安排的面试官并不是太对口。尤其是深度学习这样比较新的领域，一般部门的面试官多半也是近期自学的，对这个也是一知半解。所以如果是想去BAT这些大公司里面专门做AI的部门，按照正常校招流程走是不合适的，一定要找到那些部门的员工内推。在我看来，如果是跪在简历筛选、笔试这些上面，连面试官都没见到，就实在太可惜了。为了避免这一点，请认真找内推。最好能联系到你想去的公司部门里的负责人，直接安排面试。 二. 面试经验面试遇到的题目，可以分为几个大类： （1）代码题（leetcode类型）主要考察数据结构和基础算法，以及代码基本功。 虽然这部分跟机器学习，深度学习关系不大，但也是面试的重中之重。基本每家公司的面试都问了大量的算法题和代码题，即使是商汤、face++这样的深度学习公司，考察这部分的时间也占到了我很多轮面试的60%甚至70%以上。我去face++面试的时候，面试官是residual net，shuffle net的作者；但他们的面试中，写代码题依旧是主要的部分。 大部分题目都不难，基本是leetcode medium的难度。但是要求在现场白板编程，思路要流畅，能做到一次性Bug-free. 并且，一般都是要给出时间复杂度和空间复杂度最优的做法。对于少数难度很大的题，也不要慌张。一般也不会一点思路也没有，尽力给面试官展现自己的思考过程。面试官也会引导你，给一点小提示，沿着提示把题目慢慢做出来也是可以通过面试的。 以下是我所遇到的一些需要当场写出完整代码的题目： 二分查找。分别实现C++中的lower_bound和upper_bound. 排序。 手写快速排序，归并排序，堆排序都被问到过。 给你一个数组，求这个数组的最大子段积 时间复杂度可以到O(n) 给你一个数组，在这个数组中找出不重合的两段，让这两段的字段和的差的绝对值最大。 时间复杂度可以到O(n) 给你一个数组，求一个k值，使得前k个数的方差 + 后面n-k个数的方差最小 时间复杂度可以到O(n) 给你一个只由0和1组成的字符串，找一个最长的子串，要求这个子串里面0和1的数目相等。 时间复杂度可以到O(n) 给你一个数组以及一个数K， 从这个数组里面选择三个数，使得三个数的和小于等于K， 问有多少种选择的方法？ 时间复杂度可以到O(n^2) 给你一个只由0和1组成的矩阵，找出一个最大的子矩阵，要求这个子矩阵是方阵，并且这个子矩阵的所有元素为1 时间复杂度可以到O(n^2) 求一个字符串的最长回文子串 时间复杂度可以到O(n) (Manacher算法) 在一个数轴上移动，初始在0点，现在要到给定的某一个x点， 每一步有三种选择，坐标加1，坐标减1，坐标乘以2，请问最少需要多少步从0点到x点。 给你一个集合，输出这个集合的所有子集。 给你一个长度为n的数组，以及一个k值（k &lt; n) 求出这个数组中每k个相邻元素里面的最大值。其实也就是一个一维的max pooling 时间复杂度可以到O(n) 写一个程序，在单位球面上随机取点，也就是说保证随机取到的点是均匀的。 给你一个长度为n的字符串s，以及m个短串（每个短串的长度小于10）， 每个字符串都是基因序列，也就是说只含有A,T,C,G这四个字母。在字符串中找出所有可以和任何一个短串模糊匹配的子串。模糊匹配的定义，两个字符串长度相等，并且至多有两个字符不一样，那么我们就可以说这两个字符串是模糊匹配的。 其它一些描述很复杂的题这里就不列了。 （2）数学题或者”智力”题不会涉及特别高深的数学知识，一般就是工科数学（微积分，概率论，线性代数）和一些组合数学的问题。 下面是我在面试中被问到过的问题： 如果一个女生说她集齐了十二个星座的前男友，她前男友数量的期望是多少？ ps：这道题在知乎上有广泛的讨论，作为知乎重度用户我也看到过。如果一个女生说，她集齐了十二个星座的前男友，我们应该如何估计她前男友的数量？ 两个人玩游戏。有n堆石头，每堆分别有a1, a2, a3…. an个石头，每次一个游戏者可以从任意一堆石头里拿走至少一个石头，也可以整堆拿走，但不能从多堆石头里面拿。无法拿石头的游戏者输，请问这个游戏是否有先手必胜或者后手必胜的策略？ 如果有，请说出这个策略，并证明这个策略能保证必胜。 一个一维数轴，起始点在原点。每次向左或者向右走一步，概率都是0.5. 请问回到原点的步数期望是多少？ 一条长度为1的线段，随机剪两刀，求有一根大于0.5的概率。 讲一下你理解的矩阵的秩。低秩矩阵有什么特点？ 在图像处理领域，这些特点有什么应用？ 讲一下你理解的特征值和特征向量。 为什么负梯度方向是使函数值下降最快的方向？简单数学推导一下 （3）机器学习基础这部分建议参考周志华老师的《机器学习》。 下面是我在面试中被问到过的问题： https://www.nowcoder.com/discuss/65323 逻辑回归和线性回归对比有什么优点？ 逻辑回归可以处理非线性问题吗？ 分类问题有哪些评价指标？每种的适用场景。 讲一下正则化，L1和L2正则化各自的特点和适用场景。 讲一下常用的损失函数以及各自的适用场景。 讲一下决策树和随机森林 讲一下GBDT的细节，写出GBDT的目标函数。 GBDT和Adaboost的区别与联系 手推softmax loss公式 讲一下SVM, SVM与LR有什么联系。 讲一下PCA的步骤。PCA和SVD的区别和联系 讲一下ensemble 偏差和方差的区别。ensemble的方法中哪些是降低偏差，哪些是降低方差？ …… 这部分问得太琐碎了，我能记起来的问题就这么多了。我的感觉，这部分问题大多数不是问得很深，所以不至于被问得哑口无言，总有得扯；但是要想给出一个特别深刻的回答，还是需要对机器学习的基础算法了解比较透彻。 （4）深度学习基础这部分的准备，我推荐花书（Bengio的Deep learning）和 @魏秀参 《解析卷积神经网络-深度学习实践手册》 手推BP 手推RNN和LSTM结构 LSTM中每个gate的作用是什么，为什么跟RNN比起来，LSTM可以防止梯度消失 讲一下pooling的作用， 为什么max pooling要更常用？哪些情况下，average pooling比max pooling更合适？ 梯度消失和梯度爆炸的原因是什么？ 有哪些解决方法？ CNN和RNN的梯度消失是一样的吗？ 有哪些防止过拟合的方法？ 讲一下激活函数sigmoid，tanh，relu. 各自的优点和适用场景？ relu的负半轴导数都是0，这部分产生的梯度消失怎么办？ batch size对收敛速度的影响。 讲一下batch normalization CNN做卷积运算的复杂度。如果一个CNN网络的输入channel数目和卷积核数目都减半，总的计算量变为原来的多少？ 讲一下AlexNet的具体结构，每层的作用 讲一下你怎么理解dropout，分别从bagging和正则化的角度 data augmentation有哪些技巧？ 讲一下你了解的优化方法，sgd, momentum, rmsprop, adam的区别和联系 如果训练的神经网络不收敛，可能有哪些原因？ 说一下你理解的卷积核， 1x1的卷积核有什么作用？ …….. 同上，这部分的很多问题也是每个人都或多或少能回答一点，但要答得很好还是需要功底的。 （5）科研上的开放性问题这部分的问题没有固定答案，也没法很好地针对性准备。功在平时，多读paper多思考，注意培养自己的insight和intuition 下面是我在面试中被问到过的问题： 选一个计算机视觉、深度学习、机器学习的子领域，讲一下这个领域的发展脉络，重点讲出各种新方法提出时的motivation，以及谈谈这个领域以后会怎么发展。 讲一下你最近看的印象比较深的paper 讲一下经典的几种网络结构， AlexNet， VGG，GoogleNet， Residual Net等等，它们各自最重要的contribution 你看过最近很火的XXX paper吗? 你对这个有什么看法？ …… （6） 编程语言、操作系统等方面的一些问题。 C++， Python， 操作系统，Linux命令等等。这部分问得比较少，但还是有的，不具体列了 （7）针对简历里项目/论文 / 实习的一些问题。这部分因人而异，我个人的对大家也没参考价值，也不列了。 三. 平时应该怎么准备在大多数情况下，你能拿到什么样的offer，其实已经被你的简历决定了。如果平时没有积累相关的经历和成果，很难只靠面试表现就拿到非常好的offer。所以建议大家平时积累算法岗所看重的一些干货。 下面几点算是找AI相关工作的加分项： （1）一作的顶级会议论文 （2）AI领域知名公司的实习经历（长期实习更好） （3）相关方向有含金量的项目经历 （4）计算机视觉竞赛，数据挖掘竞赛的获奖或者优秀名次。现在这类竞赛太多了，就不具体列了。 （5）程序设计竞赛的获奖（例如OI/ACM/topcoder之类的） 当然，名校、高GPA这些是针对所有领域都有用的加分项，同样也是适用于这个领域的。 所以我的建议就是，如果自己所在的实验室很厉害，资源丰富，就专心做科研，发paper； 如果所在的实验室一般，没法产出相关的优秀成果，可以考虑自己做比赛和找实习。有一份知名公司的实习经历之后，找工作难度会下降很多。 最后，祝有志于AI这个领域的人都能拿到满意的offer. 最后要说的是：博客源码 ， 欢迎 star]]></content>
      <categories>
        <category>Inbox</category>
      </categories>
      <tags>
        <tag>AI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HashMap源码剖析]]></title>
    <url>%2Farchives%2Fa73d4680.html</url>
    <content type="text"><![CDATA[本文转发为了方便查看，原文地址 HashMap简介HashMap是基于哈希表实现的，每一个元素都是一个key-value对，其内部通过单链表解决冲突问题，容量不足（超过了阈值）时，同样会自动增长。 HashMap是非线程安全的，只是用于单线程环境下，多线程环境下可以采用concurrent并发包下的concurrentHashMap。 HashMap实现了Serializable接口，因此它支持序列化，实现了Cloneable接口，能被克隆。 HashMap源码剖析HashMap的源码如下（加入了比较详细的注释）： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599600601602603604605606607608609610611612613614615616617618619620621622623624625626627628629630631632633634635636637638639640641642643644645646647648649650651652653654655656657658659660661662663664665666667668669670671672673674675676677678679680681682683684685686687688689690691692693694695696697698699700701702703704705706707708709710711712713714715716717718719720721722723724725726727728729730731732733734735736737738739740741742743744745746747748749750751752753754package java.util; import java.io.*; public class HashMap&lt;K,V&gt; extends AbstractMap&lt;K,V&gt; implements Map&lt;K,V&gt;, Cloneable, Serializable &#123; // 默认的初始容量（容量为HashMap中槽的数目）是16，且实际容量必须是2的整数次幂。 static final int DEFAULT_INITIAL_CAPACITY = 16; // 最大容量（必须是2的幂且小于2的30次方，传入容量过大将被这个值替换） static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30; // 默认加载因子为0.75 static final float DEFAULT_LOAD_FACTOR = 0.75f; // 存储数据的Entry数组，长度是2的幂。 // HashMap采用链表法解决冲突，每一个Entry本质上是一个单向链表 transient Entry[] table; // HashMap的底层数组中已用槽的数量 transient int size; // HashMap的阈值，用于判断是否需要调整HashMap的容量（threshold = 容量*加载因子） int threshold; // 加载因子实际大小 final float loadFactor; // HashMap被改变的次数 transient volatile int modCount; // 指定“容量大小”和“加载因子”的构造函数 public HashMap(int initialCapacity, float loadFactor) &#123; if (initialCapacity &lt; 0) throw new IllegalArgumentException("Illegal initial capacity: " + initialCapacity); // HashMap的最大容量只能是MAXIMUM_CAPACITY if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; //加载因此不能小于0 if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException("Illegal load factor: " + loadFactor); // 找出“大于initialCapacity”的最小的2的幂 int capacity = 1; while (capacity &lt; initialCapacity) capacity &lt;&lt;= 1; // 设置“加载因子” this.loadFactor = loadFactor; // 设置“HashMap阈值”，当HashMap中存储数据的数量达到threshold时，就需要将HashMap的容量加倍。 threshold = (int)(capacity * loadFactor); // 创建Entry数组，用来保存数据 table = new Entry[capacity]; init(); &#125; // 指定“容量大小”的构造函数 public HashMap(int initialCapacity) &#123; this(initialCapacity, DEFAULT_LOAD_FACTOR); &#125; // 默认构造函数。 public HashMap() &#123; // 设置“加载因子”为默认加载因子0.75 this.loadFactor = DEFAULT_LOAD_FACTOR; // 设置“HashMap阈值”，当HashMap中存储数据的数量达到threshold时，就需要将HashMap的容量加倍。 threshold = (int)(DEFAULT_INITIAL_CAPACITY * DEFAULT_LOAD_FACTOR); // 创建Entry数组，用来保存数据 table = new Entry[DEFAULT_INITIAL_CAPACITY]; init(); &#125; // 包含“子Map”的构造函数 public HashMap(Map&lt;? extends K, ? extends V&gt; m) &#123; this(Math.max((int) (m.size() / DEFAULT_LOAD_FACTOR) + 1, DEFAULT_INITIAL_CAPACITY), DEFAULT_LOAD_FACTOR); // 将m中的全部元素逐个添加到HashMap中 putAllForCreate(m); &#125; //求hash值的方法，重新计算hash值 static int hash(int h) &#123; h ^= (h &gt;&gt;&gt; 20) ^ (h &gt;&gt;&gt; 12); return h ^ (h &gt;&gt;&gt; 7) ^ (h &gt;&gt;&gt; 4); &#125; // 返回h在数组中的索引值，这里用&amp;代替取模，旨在提升效率 // h &amp; (length-1)保证返回值的小于length static int indexFor(int h, int length) &#123; return h &amp; (length-1); &#125; public int size() &#123; return size; &#125; public boolean isEmpty() &#123; return size == 0; &#125; // 获取key对应的value public V get(Object key) &#123; if (key == null) return getForNullKey(); // 获取key的hash值 int hash = hash(key.hashCode()); // 在“该hash值对应的链表”上查找“键值等于key”的元素 for (Entry&lt;K,V&gt; e = table[indexFor(hash, table.length)]; e != null; e = e.next) &#123; Object k; //判断key是否相同 if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) return e.value; &#125; //没找到则返回null return null; &#125; // 获取“key为null”的元素的值 // HashMap将“key为null”的元素存储在table[0]位置，但不一定是该链表的第一个位置！ private V getForNullKey() &#123; for (Entry&lt;K,V&gt; e = table[0]; e != null; e = e.next) &#123; if (e.key == null) return e.value; &#125; return null; &#125; // HashMap是否包含key public boolean containsKey(Object key) &#123; return getEntry(key) != null; &#125; // 返回“键为key”的键值对 final Entry&lt;K,V&gt; getEntry(Object key) &#123; // 获取哈希值 // HashMap将“key为null”的元素存储在table[0]位置，“key不为null”的则调用hash()计算哈希值 int hash = (key == null) ? 0 : hash(key.hashCode()); // 在“该hash值对应的链表”上查找“键值等于key”的元素 for (Entry&lt;K,V&gt; e = table[indexFor(hash, table.length)]; e != null; e = e.next) &#123; Object k; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; return null; &#125; // 将“key-value”添加到HashMap中 public V put(K key, V value) &#123; // 若“key为null”，则将该键值对添加到table[0]中。 if (key == null) return putForNullKey(value); // 若“key不为null”，则计算该key的哈希值，然后将其添加到该哈希值对应的链表中。 int hash = hash(key.hashCode()); int i = indexFor(hash, table.length); for (Entry&lt;K,V&gt; e = table[i]; e != null; e = e.next) &#123; Object k; // 若“该key”对应的键值对已经存在，则用新的value取代旧的value。然后退出！ if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) &#123; V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; &#125; &#125; // 若“该key”对应的键值对不存在，则将“key-value”添加到table中 modCount++; //将key-value添加到table[i]处 addEntry(hash, key, value, i); return null; &#125; // putForNullKey()的作用是将“key为null”键值对添加到table[0]位置 private V putForNullKey(V value) &#123; for (Entry&lt;K,V&gt; e = table[0]; e != null; e = e.next) &#123; if (e.key == null) &#123; V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; &#125; &#125; // 如果没有存在key为null的键值对，则直接题阿见到table[0]处! modCount++; addEntry(0, null, value, 0); return null; &#125; // 创建HashMap对应的“添加方法”， // 它和put()不同。putForCreate()是内部方法，它被构造函数等调用，用来创建HashMap // 而put()是对外提供的往HashMap中添加元素的方法。 private void putForCreate(K key, V value) &#123; int hash = (key == null) ? 0 : hash(key.hashCode()); int i = indexFor(hash, table.length); // 若该HashMap表中存在“键值等于key”的元素，则替换该元素的value值 for (Entry&lt;K,V&gt; e = table[i]; e != null; e = e.next) &#123; Object k; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) &#123; e.value = value; return; &#125; &#125; // 若该HashMap表中不存在“键值等于key”的元素，则将该key-value添加到HashMap中 createEntry(hash, key, value, i); &#125; // 将“m”中的全部元素都添加到HashMap中。 // 该方法被内部的构造HashMap的方法所调用。 private void putAllForCreate(Map&lt;? extends K, ? extends V&gt; m) &#123; // 利用迭代器将元素逐个添加到HashMap中 for (Iterator&lt;? extends Map.Entry&lt;? extends K, ? extends V&gt;&gt; i = m.entrySet().iterator(); i.hasNext(); ) &#123; Map.Entry&lt;? extends K, ? extends V&gt; e = i.next(); putForCreate(e.getKey(), e.getValue()); &#125; &#125; // 重新调整HashMap的大小，newCapacity是调整后的容量 void resize(int newCapacity) &#123; Entry[] oldTable = table; int oldCapacity = oldTable.length; //如果就容量已经达到了最大值，则不能再扩容，直接返回 if (oldCapacity == MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return; &#125; // 新建一个HashMap，将“旧HashMap”的全部元素添加到“新HashMap”中， // 然后，将“新HashMap”赋值给“旧HashMap”。 Entry[] newTable = new Entry[newCapacity]; transfer(newTable); table = newTable; threshold = (int)(newCapacity * loadFactor); &#125; // 将HashMap中的全部元素都添加到newTable中 void transfer(Entry[] newTable) &#123; Entry[] src = table; int newCapacity = newTable.length; for (int j = 0; j &lt; src.length; j++) &#123; Entry&lt;K,V&gt; e = src[j]; if (e != null) &#123; src[j] = null; do &#123; Entry&lt;K,V&gt; next = e.next; int i = indexFor(e.hash, newCapacity); e.next = newTable[i]; newTable[i] = e; e = next; &#125; while (e != null); &#125; &#125; &#125; // 将"m"的全部元素都添加到HashMap中 public void putAll(Map&lt;? extends K, ? extends V&gt; m) &#123; // 有效性判断 int numKeysToBeAdded = m.size(); if (numKeysToBeAdded == 0) return; // 计算容量是否足够， // 若“当前阀值容量 &lt; 需要的容量”，则将容量x2。 if (numKeysToBeAdded &gt; threshold) &#123; int targetCapacity = (int)(numKeysToBeAdded / loadFactor + 1); if (targetCapacity &gt; MAXIMUM_CAPACITY) targetCapacity = MAXIMUM_CAPACITY; int newCapacity = table.length; while (newCapacity &lt; targetCapacity) newCapacity &lt;&lt;= 1; if (newCapacity &gt; table.length) resize(newCapacity); &#125; // 通过迭代器，将“m”中的元素逐个添加到HashMap中。 for (Iterator&lt;? extends Map.Entry&lt;? extends K, ? extends V&gt;&gt; i = m.entrySet().iterator(); i.hasNext(); ) &#123; Map.Entry&lt;? extends K, ? extends V&gt; e = i.next(); put(e.getKey(), e.getValue()); &#125; &#125; // 删除“键为key”元素 public V remove(Object key) &#123; Entry&lt;K,V&gt; e = removeEntryForKey(key); return (e == null ? null : e.value); &#125; // 删除“键为key”的元素 final Entry&lt;K,V&gt; removeEntryForKey(Object key) &#123; // 获取哈希值。若key为null，则哈希值为0；否则调用hash()进行计算 int hash = (key == null) ? 0 : hash(key.hashCode()); int i = indexFor(hash, table.length); Entry&lt;K,V&gt; prev = table[i]; Entry&lt;K,V&gt; e = prev; // 删除链表中“键为key”的元素 // 本质是“删除单向链表中的节点” while (e != null) &#123; Entry&lt;K,V&gt; next = e.next; Object k; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) &#123; modCount++; size--; if (prev == e) table[i] = next; else prev.next = next; e.recordRemoval(this); return e; &#125; prev = e; e = next; &#125; return e; &#125; // 删除“键值对” final Entry&lt;K,V&gt; removeMapping(Object o) &#123; if (!(o instanceof Map.Entry)) return null; Map.Entry&lt;K,V&gt; entry = (Map.Entry&lt;K,V&gt;) o; Object key = entry.getKey(); int hash = (key == null) ? 0 : hash(key.hashCode()); int i = indexFor(hash, table.length); Entry&lt;K,V&gt; prev = table[i]; Entry&lt;K,V&gt; e = prev; // 删除链表中的“键值对e” // 本质是“删除单向链表中的节点” while (e != null) &#123; Entry&lt;K,V&gt; next = e.next; if (e.hash == hash &amp;&amp; e.equals(entry)) &#123; modCount++; size--; if (prev == e) table[i] = next; else prev.next = next; e.recordRemoval(this); return e; &#125; prev = e; e = next; &#125; return e; &#125; // 清空HashMap，将所有的元素设为null public void clear() &#123; modCount++; Entry[] tab = table; for (int i = 0; i &lt; tab.length; i++) tab[i] = null; size = 0; &#125; // 是否包含“值为value”的元素 public boolean containsValue(Object value) &#123; // 若“value为null”，则调用containsNullValue()查找 if (value == null) return containsNullValue(); // 若“value不为null”，则查找HashMap中是否有值为value的节点。 Entry[] tab = table; for (int i = 0; i &lt; tab.length ; i++) for (Entry e = tab[i] ; e != null ; e = e.next) if (value.equals(e.value)) return true; return false; &#125; // 是否包含null值 private boolean containsNullValue() &#123; Entry[] tab = table; for (int i = 0; i &lt; tab.length ; i++) for (Entry e = tab[i] ; e != null ; e = e.next) if (e.value == null) return true; return false; &#125; // 克隆一个HashMap，并返回Object对象 public Object clone() &#123; HashMap&lt;K,V&gt; result = null; try &#123; result = (HashMap&lt;K,V&gt;)super.clone(); &#125; catch (CloneNotSupportedException e) &#123; // assert false; &#125; result.table = new Entry[table.length]; result.entrySet = null; result.modCount = 0; result.size = 0; result.init(); // 调用putAllForCreate()将全部元素添加到HashMap中 result.putAllForCreate(this); return result; &#125; // Entry是单向链表。 // 它是 “HashMap链式存储法”对应的链表。 // 它实现了Map.Entry 接口，即实现getKey(), getValue(), setValue(V value), equals(Object o), hashCode()这些函数 static class Entry&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final K key; V value; // 指向下一个节点 Entry&lt;K,V&gt; next; final int hash; // 构造函数。 // 输入参数包括"哈希值(h)", "键(k)", "值(v)", "下一节点(n)" Entry(int h, K k, V v, Entry&lt;K,V&gt; n) &#123; value = v; next = n; key = k; hash = h; &#125; public final K getKey() &#123; return key; &#125; public final V getValue() &#123; return value; &#125; public final V setValue(V newValue) &#123; V oldValue = value; value = newValue; return oldValue; &#125; // 判断两个Entry是否相等 // 若两个Entry的“key”和“value”都相等，则返回true。 // 否则，返回false public final boolean equals(Object o) &#123; if (!(o instanceof Map.Entry)) return false; Map.Entry e = (Map.Entry)o; Object k1 = getKey(); Object k2 = e.getKey(); if (k1 == k2 || (k1 != null &amp;&amp; k1.equals(k2))) &#123; Object v1 = getValue(); Object v2 = e.getValue(); if (v1 == v2 || (v1 != null &amp;&amp; v1.equals(v2))) return true; &#125; return false; &#125; // 实现hashCode() public final int hashCode() &#123; return (key==null ? 0 : key.hashCode()) ^ (value==null ? 0 : value.hashCode()); &#125; public final String toString() &#123; return getKey() + "=" + getValue(); &#125; // 当向HashMap中添加元素时，绘调用recordAccess()。 // 这里不做任何处理 void recordAccess(HashMap&lt;K,V&gt; m) &#123; &#125; // 当从HashMap中删除元素时，绘调用recordRemoval()。 // 这里不做任何处理 void recordRemoval(HashMap&lt;K,V&gt; m) &#123; &#125; &#125; // 新增Entry。将“key-value”插入指定位置，bucketIndex是位置索引。 void addEntry(int hash, K key, V value, int bucketIndex) &#123; // 保存“bucketIndex”位置的值到“e”中 Entry&lt;K,V&gt; e = table[bucketIndex]; // 设置“bucketIndex”位置的元素为“新Entry”， // 设置“e”为“新Entry的下一个节点” table[bucketIndex] = new Entry&lt;K,V&gt;(hash, key, value, e); // 若HashMap的实际大小 不小于 “阈值”，则调整HashMap的大小 if (size++ &gt;= threshold) resize(2 * table.length); &#125; // 创建Entry。将“key-value”插入指定位置。 void createEntry(int hash, K key, V value, int bucketIndex) &#123; // 保存“bucketIndex”位置的值到“e”中 Entry&lt;K,V&gt; e = table[bucketIndex]; // 设置“bucketIndex”位置的元素为“新Entry”， // 设置“e”为“新Entry的下一个节点” table[bucketIndex] = new Entry&lt;K,V&gt;(hash, key, value, e); size++; &#125; // HashIterator是HashMap迭代器的抽象出来的父类，实现了公共了函数。 // 它包含“key迭代器(KeyIterator)”、“Value迭代器(ValueIterator)”和“Entry迭代器(EntryIterator)”3个子类。 private abstract class HashIterator&lt;E&gt; implements Iterator&lt;E&gt; &#123; // 下一个元素 Entry&lt;K,V&gt; next; // expectedModCount用于实现fast-fail机制。 int expectedModCount; // 当前索引 int index; // 当前元素 Entry&lt;K,V&gt; current; HashIterator() &#123; expectedModCount = modCount; if (size &gt; 0) &#123; // advance to first entry Entry[] t = table; // 将next指向table中第一个不为null的元素。 // 这里利用了index的初始值为0，从0开始依次向后遍历，直到找到不为null的元素就退出循环。 while (index &lt; t.length &amp;&amp; (next = t[index++]) == null) ; &#125; &#125; public final boolean hasNext() &#123; return next != null; &#125; // 获取下一个元素 final Entry&lt;K,V&gt; nextEntry() &#123; if (modCount != expectedModCount) throw new ConcurrentModificationException(); Entry&lt;K,V&gt; e = next; if (e == null) throw new NoSuchElementException(); // 注意！！！ // 一个Entry就是一个单向链表 // 若该Entry的下一个节点不为空，就将next指向下一个节点; // 否则，将next指向下一个链表(也是下一个Entry)的不为null的节点。 if ((next = e.next) == null) &#123; Entry[] t = table; while (index &lt; t.length &amp;&amp; (next = t[index++]) == null) ; &#125; current = e; return e; &#125; // 删除当前元素 public void remove() &#123; if (current == null) throw new IllegalStateException(); if (modCount != expectedModCount) throw new ConcurrentModificationException(); Object k = current.key; current = null; HashMap.this.removeEntryForKey(k); expectedModCount = modCount; &#125; &#125; // value的迭代器 private final class ValueIterator extends HashIterator&lt;V&gt; &#123; public V next() &#123; return nextEntry().value; &#125; &#125; // key的迭代器 private final class KeyIterator extends HashIterator&lt;K&gt; &#123; public K next() &#123; return nextEntry().getKey(); &#125; &#125; // Entry的迭代器 private final class EntryIterator extends HashIterator&lt;Map.Entry&lt;K,V&gt;&gt; &#123; public Map.Entry&lt;K,V&gt; next() &#123; return nextEntry(); &#125; &#125; // 返回一个“key迭代器” Iterator&lt;K&gt; newKeyIterator() &#123; return new KeyIterator(); &#125; // 返回一个“value迭代器” Iterator&lt;V&gt; newValueIterator() &#123; return new ValueIterator(); &#125; // 返回一个“entry迭代器” Iterator&lt;Map.Entry&lt;K,V&gt;&gt; newEntryIterator() &#123; return new EntryIterator(); &#125; // HashMap的Entry对应的集合 private transient Set&lt;Map.Entry&lt;K,V&gt;&gt; entrySet = null; // 返回“key的集合”，实际上返回一个“KeySet对象” public Set&lt;K&gt; keySet() &#123; Set&lt;K&gt; ks = keySet; return (ks != null ? ks : (keySet = new KeySet())); &#125; // Key对应的集合 // KeySet继承于AbstractSet，说明该集合中没有重复的Key。 private final class KeySet extends AbstractSet&lt;K&gt; &#123; public Iterator&lt;K&gt; iterator() &#123; return newKeyIterator(); &#125; public int size() &#123; return size; &#125; public boolean contains(Object o) &#123; return containsKey(o); &#125; public boolean remove(Object o) &#123; return HashMap.this.removeEntryForKey(o) != null; &#125; public void clear() &#123; HashMap.this.clear(); &#125; &#125; // 返回“value集合”，实际上返回的是一个Values对象 public Collection&lt;V&gt; values() &#123; Collection&lt;V&gt; vs = values; return (vs != null ? vs : (values = new Values())); &#125; // “value集合” // Values继承于AbstractCollection，不同于“KeySet继承于AbstractSet”， // Values中的元素能够重复。因为不同的key可以指向相同的value。 private final class Values extends AbstractCollection&lt;V&gt; &#123; public Iterator&lt;V&gt; iterator() &#123; return newValueIterator(); &#125; public int size() &#123; return size; &#125; public boolean contains(Object o) &#123; return containsValue(o); &#125; public void clear() &#123; HashMap.this.clear(); &#125; &#125; // 返回“HashMap的Entry集合” public Set&lt;Map.Entry&lt;K,V&gt;&gt; entrySet() &#123; return entrySet0(); &#125; // 返回“HashMap的Entry集合”，它实际是返回一个EntrySet对象 private Set&lt;Map.Entry&lt;K,V&gt;&gt; entrySet0() &#123; Set&lt;Map.Entry&lt;K,V&gt;&gt; es = entrySet; return es != null ? es : (entrySet = new EntrySet()); &#125; // EntrySet对应的集合 // EntrySet继承于AbstractSet，说明该集合中没有重复的EntrySet。 private final class EntrySet extends AbstractSet&lt;Map.Entry&lt;K,V&gt;&gt; &#123; public Iterator&lt;Map.Entry&lt;K,V&gt;&gt; iterator() &#123; return newEntryIterator(); &#125; public boolean contains(Object o) &#123; if (!(o instanceof Map.Entry)) return false; Map.Entry&lt;K,V&gt; e = (Map.Entry&lt;K,V&gt;) o; Entry&lt;K,V&gt; candidate = getEntry(e.getKey()); return candidate != null &amp;&amp; candidate.equals(e); &#125; public boolean remove(Object o) &#123; return removeMapping(o) != null; &#125; public int size() &#123; return size; &#125; public void clear() &#123; HashMap.this.clear(); &#125; &#125; // java.io.Serializable的写入函数 // 将HashMap的“总的容量，实际容量，所有的Entry”都写入到输出流中 private void writeObject(java.io.ObjectOutputStream s) throws IOException &#123; Iterator&lt;Map.Entry&lt;K,V&gt;&gt; i = (size &gt; 0) ? entrySet0().iterator() : null; // Write out the threshold, loadfactor, and any hidden stuff s.defaultWriteObject(); // Write out number of buckets s.writeInt(table.length); // Write out size (number of Mappings) s.writeInt(size); // Write out keys and values (alternating) if (i != null) &#123; while (i.hasNext()) &#123; Map.Entry&lt;K,V&gt; e = i.next(); s.writeObject(e.getKey()); s.writeObject(e.getValue()); &#125; &#125; &#125; private static final long serialVersionUID = 362498820763181265L; // java.io.Serializable的读取函数：根据写入方式读出 // 将HashMap的“总的容量，实际容量，所有的Entry”依次读出 private void readObject(java.io.ObjectInputStream s) throws IOException, ClassNotFoundException &#123; // Read in the threshold, loadfactor, and any hidden stuff s.defaultReadObject(); // Read in number of buckets and allocate the bucket array; int numBuckets = s.readInt(); table = new Entry[numBuckets]; init(); // Give subclass a chance to do its thing. // Read in size (number of Mappings) int size = s.readInt(); // Read the keys and values, and put the mappings in the HashMap for (int i=0; i&lt;size; i++) &#123; K key = (K) s.readObject(); V value = (V) s.readObject(); putForCreate(key, value); &#125; &#125; // 返回“HashMap总的容量” int capacity() &#123; return table.length; &#125; // 返回“HashMap的加载因子” float loadFactor() &#123; return loadFactor; &#125; &#125; 几点总结首先要清楚HashMap的存储结构，如下图所示：(图) 图中，紫色部分即代表哈希表，也称为哈希数组，数组的每个元素都是一个单链表的头节点，链表是用来解决冲突的，如果不同的key映射到了数组的同一位置处，就将其放入单链表中。 首先看链表中节点的数据结构：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071// Entry是单向链表。 // 它是 “HashMap链式存储法”对应的链表。 // 它实现了Map.Entry 接口，即实现getKey(), getValue(), setValue(V value), equals(Object o), hashCode()这些函数 static class Entry&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final K key; V value; // 指向下一个节点 Entry&lt;K,V&gt; next; final int hash; // 构造函数。 // 输入参数包括"哈希值(h)", "键(k)", "值(v)", "下一节点(n)" Entry(int h, K k, V v, Entry&lt;K,V&gt; n) &#123; value = v; next = n; key = k; hash = h; &#125; public final K getKey() &#123; return key; &#125; public final V getValue() &#123; return value; &#125; public final V setValue(V newValue) &#123; V oldValue = value; value = newValue; return oldValue; &#125; // 判断两个Entry是否相等 // 若两个Entry的“key”和“value”都相等，则返回true。 // 否则，返回false public final boolean equals(Object o) &#123; if (!(o instanceof Map.Entry)) return false; Map.Entry e = (Map.Entry)o; Object k1 = getKey(); Object k2 = e.getKey(); if (k1 == k2 || (k1 != null &amp;&amp; k1.equals(k2))) &#123; Object v1 = getValue(); Object v2 = e.getValue(); if (v1 == v2 || (v1 != null &amp;&amp; v1.equals(v2))) return true; &#125; return false; &#125; // 实现hashCode() public final int hashCode() &#123; return (key==null ? 0 : key.hashCode()) ^ (value==null ? 0 : value.hashCode()); &#125; public final String toString() &#123; return getKey() + "=" + getValue(); &#125; // 当向HashMap中添加元素时，绘调用recordAccess()。 // 这里不做任何处理 void recordAccess(HashMap&lt;K,V&gt; m) &#123; &#125; // 当从HashMap中删除元素时，绘调用recordRemoval()。 // 这里不做任何处理 void recordRemoval(HashMap&lt;K,V&gt; m) &#123; &#125; &#125; 它的结构元素除了key、value、hash外，还有next，next指向下一个节点。另外，这里覆写了equals和hashCode方法来保证键值对的独一无二。 HashMap共有四个构造方法。构造方法中提到了两个很重要的参数：初始容量和加载因子。这两个参数是影响HashMap性能的重要参数，其中容量表示哈希表中槽的数量（即哈希数组的长度），初始容量是创建哈希表时的容量（从构造函数中可以看出，如果不指明，则默认为16），加载因子是哈希表在其容量自动增加之前可以达到多满的一种尺度，当哈希表中的条目数超出了加载因子与当前容量的乘积时，则要对该哈希表进行 resize 操作（即扩容）。 下面说下加载因子，如果加载因子越大，对空间的利用更充分，但是查找效率会降低（链表长度会越来越长）；如果加载因子太小，那么表中的数据将过于稀疏（很多空间还没用，就开始扩容了），对空间造成严重浪费。如果我们在构造方法中不指定，则系统默认加载因子为0.75，这是一个比较理想的值，一般情况下我们是无需修改的。 另外，无论我们指定的容量为多少，构造方法都会将实际容量设为不小于指定容量的2的次方的一个数，且最大值不能超过2的30次方 HashMap中key和value都允许为null。put和get要重点分析下HashMap中用的最多的两个方法put和get。 get 先从比较简单的get方法着手，源码如下： 12345678910111213141516171819202122232425262728// 获取key对应的value public V get(Object key) &#123; if (key == null) return getForNullKey(); // 获取key的hash值 int hash = hash(key.hashCode()); // 在“该hash值对应的链表”上查找“键值等于key”的元素 for (Entry&lt;K,V&gt; e = table[indexFor(hash, table.length)]; e != null; e = e.next) &#123; Object k; /判断key是否相同 if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) return e.value; &#125; 没找到则返回null return null; &#125; // 获取“key为null”的元素的值 // HashMap将“key为null”的元素存储在table[0]位置，但不一定是该链表的第一个位置！ private V getForNullKey() &#123; for (Entry&lt;K,V&gt; e = table[0]; e != null; e = e.next) &#123; if (e.key == null) return e.value; &#125; return null; &#125; 首先，如果key为null，则直接从哈希表的第一个位置table[0]对应的链表上查找。记住，key为null的键值对永远都放在以table[0]为头结点的链表中，当然不一定是存放在头结点table[0]中。 如果key不为null，则先求的key的hash值，根据hash值找到在table中的索引，在该索引对应的单链表中查找是否有键值对的key与目标key相等，有就返回对应的value，没有则返回null。 put put方法稍微复杂些，代码如下： 12345678910111213141516171819202122232425 // 将“key-value”添加到HashMap中 public V put(K key, V value) &#123; // 若“key为null”，则将该键值对添加到table[0]中。 if (key == null) return putForNullKey(value); // 若“key不为null”，则计算该key的哈希值，然后将其添加到该哈希值对应的链表中。 int hash = hash(key.hashCode()); int i = indexFor(hash, table.length); for (Entry&lt;K,V&gt; e = table[i]; e != null; e = e.next) &#123; Object k; // 若“该key”对应的键值对已经存在，则用新的value取代旧的value。然后退出！ if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) &#123; V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; &#125; &#125; // 若“该key”对应的键值对不存在，则将“key-value”添加到table中 modCount++; //将key-value添加到table[i]处 addEntry(hash, key, value, i); return null; &#125; 如果key为null，则将其添加到table[0]对应的链表中，putForNullKey的源码如下：123456789101112131415// putForNullKey()的作用是将“key为null”键值对添加到table[0]位置 private V putForNullKey(V value) &#123; for (Entry&lt;K,V&gt; e = table[0]; e != null; e = e.next) &#123; if (e.key == null) &#123; V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; &#125; &#125; // 如果没有存在key为null的键值对，则直接题阿见到table[0]处! modCount++; addEntry(0, null, value, 0); return null; &#125; 如果key不为null，则同样先求出key的hash值，根据hash值得出在table中的索引，而后遍历对应的单链表，如果单链表中存在与目标key相等的键值对，则将新的value覆盖旧的value，比将旧的value返回，如果找不到与目标key相等的键值对，或者该单链表为空，则将该键值对插入到改单链表的头结点位置（每次新插入的节点都是放在头结点的位置），该操作是有addEntry方法实现的，它的源码如下：1234567891011// 新增Entry。将“key-value”插入指定位置，bucketIndex是位置索引。 void addEntry(int hash, K key, V value, int bucketIndex) &#123; // 保存“bucketIndex”位置的值到“e”中 Entry&lt;K,V&gt; e = table[bucketIndex]; // 设置“bucketIndex”位置的元素为“新Entry”， // 设置“e”为“新Entry的下一个节点” table[bucketIndex] = new Entry&lt;K,V&gt;(hash, key, value, e); // 若HashMap的实际大小 不小于 “阈值”，则调整HashMap的大小 if (size++ &gt;= threshold) resize(2 * table.length); &#125; 注意这里倒数第三行的构造方法，将key-value键值对赋给table[bucketIndex]，并将其next指向元素e，这便将key-value放到了头结点中，并将之前的头结点接在了它的后面。该方法也说明，每次put键值对的时候，总是将新的该键值对放在table[bucketIndex]处（即头结点处）。 两外注意最后两行代码，每次加入键值对时，都要判断当前已用的槽的数目是否大于等于阀值（容量*加载因子），如果大于等于，则进行扩容，将容量扩为原来容量的2倍。 关于扩容。上面我们看到了扩容的方法，resize方法，它的源码如下： 12345678910111213141516// 重新调整HashMap的大小，newCapacity是调整后的单位 void resize(int newCapacity) &#123; Entry[] oldTable = table; int oldCapacity = oldTable.length; if (oldCapacity == MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return; &#125; // 新建一个HashMap，将“旧HashMap”的全部元素添加到“新HashMap”中， // 然后，将“新HashMap”赋值给“旧HashMap”。 Entry[] newTable = new Entry[newCapacity]; transfer(newTable); table = newTable; threshold = (int)(newCapacity * loadFactor); &#125; 很明显，是新建了一个HashMap的底层数组，而后调用transfer方法，将就HashMap的全部元素添加到新的HashMap中（要重新计算元素在新的数组中的索引位置）。transfer方法的源码如下：123456789101112131415161718// 将HashMap中的全部元素都添加到newTable中 void transfer(Entry[] newTable) &#123; Entry[] src = table; int newCapacity = newTable.length; for (int j = 0; j &lt; src.length; j++) &#123; Entry&lt;K,V&gt; e = src[j]; if (e != null) &#123; src[j] = null; do &#123; Entry&lt;K,V&gt; next = e.next; int i = indexFor(e.hash, newCapacity); e.next = newTable[i]; newTable[i] = e; e = next; &#125; while (e != null); &#125; &#125; &#125; 很明显，扩容是一个相当耗时的操作，因为它需要重新计算这些元素在新的数组中的位置并进行复制处理。因此，我们在用HashMap的时，最好能提前预估下HashMap中元素的个数，这样有助于提高HashMap的性能。 注意containsKey方法和containsValue方法。前者直接可以通过key的哈希值将搜索范围定位到指定索引对应的链表，而后者要对哈希数组的每个链表进行搜索。 求hash值和索引值我们重点来分析下求hash值和索引值的方法，这两个方法便是HashMap设计的最为核心的部分，二者结合能保证哈希表中的元素尽可能均匀地散列。 计算哈希值的方法如下：1234static int hash(int h) &#123; h ^= (h &gt;&gt;&gt; 20) ^ (h &gt;&gt;&gt; 12); return h ^ (h &gt;&gt;&gt; 7) ^ (h &gt;&gt;&gt; 4); &#125; 它只是一个数学公式，IDK这样设计对hash值的计算，自然有它的好处，至于为什么这样设计，我们这里不去追究，只要明白一点，用的位的操作使hash值的计算效率很高。 由hash值找到对应索引的方法如下：123static int indexFor(int h, int length) &#123; return h &amp; (length-1); &#125; 这个我们要重点说下，我们一般对哈希表的散列很自然地会想到用hash值对length取模（即除法散列法），Hashtable中也是这样实现的，这种方法基本能保证元素在哈希表中散列的比较均匀，但取模会用到除法运算，效率很低，HashMap中则通过h&amp;(length-1)的方法来代替取模，同样实现了均匀的散列，但效率要高很多，这也是HashMap对Hashtable的一个改进。 接下来，我们分析下为什么哈希表的容量一定要是2的整数次幂。首先，length为2的整数次幂的话，h&amp;(length-1)就相当于对length取模，这样便保证了散列的均匀，同时也提升了效率；其次，length为2的整数次幂的话，为偶数，这样length-1为奇数，奇数的最后一位是1，这样便保证了h&amp;(length-1)的最后一位可能为0，也可能为1（这取决于h的值），即与后的结果可能为偶数，也可能为奇数，这样便可以保证散列的均匀性，而如果length为奇数的话，很明显length-1为偶数，它的最后一位是0，这样h&amp;(length-1)的最后一位肯定为0，即只能为偶数，这样任何hash值都只会被散列到数组的偶数下标位置上，这便浪费了近一半的空间，因此，length取2的整数次幂，是为了使不同hash值发生碰撞的概率较小，这样就能使元素在哈希表中均匀地散列。 最后要说的是：博客源码 ， 欢迎 star]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>HashMap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[css实现图片轮播]]></title>
    <url>%2Farchives%2F64ac0229.html</url>
    <content type="text"><![CDATA[本文转自知乎 由于css无法做到js一样的精准操控，所有某些效果是无法实现的，比如在轮播的同时支持用户左右滑动，所以使用css只能实现基本的效果。下面列出来的内容就是我们实现的： 在固定区域中，内部内容自行滑动切换形成播放的效果 当切换到最后一张内容时，会反向播放或者回到起点重播 每张内容会停留一段时间，让用户能够看清楚 内容可以点击/进行操作 dom结构搭建首先要有一个容器作为轮播图的容器，同时由于要实现滑动切换，所以内部需要有一个装载所有待切换内容的子容器 如果子容器中的内容是左右切换的，则需要将内容左右排列开 下面以轮播图片为例，上代码123456789&lt;div class="loop-wrap"&gt; &lt;div class="loop-images-container"&gt; &lt;img src="darksky.jpg" alt="" class="loop-image"&gt; &lt;img src="starsky.jpg" alt="" class="loop-image"&gt; &lt;img src="whiteland.jpg" alt="" class="loop-image"&gt; &lt;img src="darksky.jpg" alt="" class="loop-image"&gt; &lt;img src="starsky.jpg" alt="" class="loop-image"&gt; &lt;/div&gt;&lt;/div&gt; .loop-wrap 是主容器 .loop-images-container 是承载内部图片的子容器 .loop-image 是图片内容，如果需要显示其他内容，可以自定义 css实现静态效果轮播图内每一页内容的宽高应该相同，且等于主容器.loop-wrap宽高 .loop-images-container的宽高必然有一个大于外部主容器，overflow属性应该设置为hidden。那为什么不设置为auto呢？我不告诉你，你可以自己试试看(这里原因是auto属性会在内容超出时自动加载出容器的下拉条)1234567891011121314151617181920.loop-wrap &#123; position: relative; width: 500px; height: 300px; margin: 100px auto; overflow: hidden;&#125;.loop-images-container&#123; position: absolute; left: 0; top: 0; width: 500%; /* 横向排列 5张图片 宽度应为主容器5倍 */ height: 100%; font-size: 0;&#125;.loop-image&#123; width: 500px; height: 300px;&#125; css实现轮播效果轮播效果说到底就是一个动画效果，而通过css3的新属性 animation 我们就可以自定义一个动画来达到轮播图效果。下面先来了解一下 animation 这个属性12345678910/*animation: name duration timing-function delay iteration-count directionname: 动画名duration： 动画持续时间 设置为0则不执行timing-function：动画速度曲线delay：动画延迟开始时间 设置为0则不延迟iteration-count：动画循环次数 设置为infinite则无限次循环direction：是否应该轮流反向播放动画 normal 否 alternate 是*/ animation 的 name 值是动画名，动画名可以通过 @keyframes 创建自定义动画规则 分析动画要实现轮播，本质上是使内部承载内容的子容器 .loop-images-container 进行位移，从而使不同位置的内容一次展示在用户眼前 共有五张图片需要展示，如果轮播总耗时10s，那么每张图片应该有2s的时间(20%)，而每张图片耗时的构成是切换耗时+展示耗时，如果切换耗时500ms(5%)，展示耗时就应该是1500ms(15%) 于是这样改造css123456789101112131415161718192021222324252627282930313233.loop-images-container&#123; position: absolute; left: 0; top: 0; width: 500%; height: 100%; font-size: 0; transform: translate(0,0); /* 初始位置位移 */ animation: loop 10s linear infinite;&#125;/* 创建loop动画规则 *//* 轮播5张，总耗时10s，单张应为2s(20%) 单张切换动画耗时500ms，停留1500ms*/@keyframes loop &#123; 0% &#123;transform: translate(0,0);&#125; 15% &#123;transform: translate(0,0);&#125; /* 停留1500ms */ 20% &#123;transform: translate(-20%,0);&#125; /* 切换500ms 位移-20% */ 35% &#123;transform: translate(-20%,0);&#125; 40% &#123;transform: translate(-40%,0);&#125; 55% &#123;transform: translate(-40%,0);&#125; 60% &#123;transform: translate(-60%,0);&#125; 75% &#123;transform: translate(-60%,0);&#125; 80% &#123;transform: translate(-80%,0);&#125; 95% &#123;transform: translate(-80%,0);&#125; 100% &#123;transform: translate(0,0);&#125; /* 复位到第一张图片 */&#125; 这是一个方向的轮播效果，想要实现往返方向的轮播效果，小伙伴们可以试试direction的alternate，但是自定义动画规则的时间间隔也要重新计算了哦！ 以下是所有代码：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;title&gt;&lt;/title&gt;&lt;style type="text/css"&gt;.loop-wrap &#123; position: relative; width: 500px; height: 300px; margin: 100px auto; overflow: hidden;&#125;.loop-images-container&#123; position: absolute; left: 0; top: 0; width: 500%; /* 横向排列 5张图片 宽度应为主容器5倍 */ height: 100%; font-size: 0; transform: translate(0,0); /* 初始位置位移 */ animation: loop 10s linear infinite;&#125;.loop-image&#123; width: 500px; height: 300px;&#125;@keyframes loop &#123; 0% &#123;transform: translate(0,0);&#125; 15% &#123;transform: translate(0,0);&#125; /* 停留1500ms */ 20% &#123;transform: translate(-20%,0);&#125; /* 切换500ms 位移-20% */ 35% &#123;transform: translate(-20%,0);&#125; 40% &#123;transform: translate(-40%,0);&#125; 55% &#123;transform: translate(-40%,0);&#125; 60% &#123;transform: translate(-60%,0);&#125; 75% &#123;transform: translate(-60%,0);&#125; 80% &#123;transform: translate(-80%,0);&#125; 95% &#123;transform: translate(-80%,0);&#125; 100% &#123;transform: translate(0,0);&#125; /* 复位到第一张图片 */&#125;&lt;/style&gt;&lt;/head&gt;&lt;body&gt; &lt;div class="loop-wrap"&gt; &lt;div class="loop-images-container"&gt; &lt;img src="1.jpg" alt="" class="loop-image"&gt; &lt;img src="2.jpg" alt="" class="loop-image"&gt; &lt;img src="3.jpg" alt="" class="loop-image"&gt; &lt;img src="4.jpg" alt="" class="loop-image"&gt; &lt;img src="5.jpg" alt="" class="loop-image"&gt; &lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 总结虽然css也能实现轮播效果，但是相对于js实现来说，功能性就弱了很多，无法控制暂停与播放，无法与用户产生交互，无法监听到状态的而变化等等，但是好处也很明显嘛！那就是简单。 最后要说的是：博客源码 ， 欢迎 star]]></content>
      <categories>
        <category>Front-End</category>
      </categories>
      <tags>
        <tag>css3</tag>
        <tag>animation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从尾到头打印链表]]></title>
    <url>%2Farchives%2Fb724efa.html</url>
    <content type="text"><![CDATA[从尾到头打印链表题目描述输入一个链表，从尾到头打印链表每个节点的值。思路2：递归1234567891011import java.util.ArrayList;public class Solution &#123; ArrayList&lt;Integer&gt; arrayList = new ArrayList&lt;Integer&gt;(); public ArrayList&lt;Integer&gt; printListFromTailToHead(ListNode listNode) &#123; if(listNode != null)&#123; this.printListFromTailToHead(listNode.next); arrayList.add(listNode.val); &#125; return arrayList; &#125;&#125; 思路2：利用栈1234567891011121314151617import java.util.Stack;import java.util.ArrayList;public class Solution &#123; public ArrayList&lt;Integer&gt; printListFromTailToHead(ListNode listNode) &#123; Stack&lt;Integer&gt; stack = new Stack&lt;&gt;(); while (listNode != null) &#123; stack.push(listNode.val); listNode = listNode.next; &#125; ArrayList&lt;Integer&gt; list = new ArrayList&lt;&gt;(); while (!stack.isEmpty()) &#123; list.add(stack.pop()); &#125; return list; &#125;&#125; 最后要说的是：博客源码 ， 欢迎 star]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>剑指Offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[替换空格]]></title>
    <url>%2Farchives%2Fcd96fb91.html</url>
    <content type="text"><![CDATA[替换空格题目描述请实现一个函数，将一个字符串中的空格替换成“%20”。例如，当字符串为We Are Happy.则经过替换之后的字符串为We%20Are%20Happy。 思路看到问题第一反应是用replaceAll()方法，但这样挺没意思的。所以自己写解决方法，这样的话，就有了两种思路。 从前往后替换，当遇到第一个空格时，要移动第一个空格后所有的字符一次；当遇到第二个空格时，要移动第二个空格后所有的字符一次；以此类推。 从后往前，先计算需要多少空间，然后从后往前移动，则每个字符只为移动一次，这样效率更高一点。 这里提供第二种方法的代码1234567891011121314151617181920212223public class Solution &#123; public String replaceSpace(StringBuffer str) &#123; int spacenum = 0;//spacenum为计算空格数 for(int i=0;i&lt;str.length();i++)&#123; if(str.charAt(i)==' ') spacenum++; &#125; int indexold = str.length()-1; //indexold为为替换前的str下标 int newlength = str.length() + spacenum*2;//计算空格转换成%20之后的str长度 int indexnew = newlength-1;//indexnew为把空格替换为%20后的str下标 str.setLength(newlength);//使str的长度扩大到转换成%20之后的长度,防止下标越界 for(;indexold&gt;=0 &amp;&amp; indexold&lt;newlength;--indexold)&#123; if(str.charAt(indexold) == ' ')&#123; // str.setCharAt(indexnew--, '0'); str.setCharAt(indexnew--, '2'); str.setCharAt(indexnew--, '%'); &#125;else&#123; str.setCharAt(indexnew--, str.charAt(indexold)); &#125; &#125; return str.toString(); &#125;&#125; 最后要说的是：博客源码 ， 欢迎 star]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>剑指Offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[二维数组中的查找]]></title>
    <url>%2Farchives%2F2aa4f70.html</url>
    <content type="text"><![CDATA[二维数组中的查找题目描述在一个二维数组中，每一行都按照从左到右递增的顺序排序，每一列都按照从上到下递增的顺序排序。请完成一个函数，输入这样的一个二维数组和一个整数，判断数组中是否含有该整数。 解题思路利用二维数组由上到下，由左到右递增的规律，那么选取右上角或者左下角的元素a[row][col]与target进行比较，当target小于元素a[row][col]时，那么target必定在元素a所在行的左边,即col–；当target大于元素a[row][col]时，那么target必定在元素a所在列的下边,即row++；时间复杂度是O(2n)123456789101112131415public class Solution &#123; public boolean Find(int target, int [][] array) &#123; int row=0; int col=array[0].length-1; while(row&lt;=array.length-1&amp;&amp;col&gt;=0)&#123; if(target==array[row][col]) return true; else if(target&gt;array[row][col]) row++; else col--; &#125; return false; &#125;&#125; 另一种思路是把每一行看成有序递增的数组，利用二分查找，通过遍历每一行得到答案，时间复杂度是O(nlogn)1234567891011121314151617181920public class Solution &#123; public boolean Find(int [][] array,int target) &#123; for(int i=0;i&lt;array.length;i++)&#123; int low=0; int high=array[i].length-1; while(low&lt;=high)&#123; int mid=(low+high)/2; if(target&gt;array[i][mid]) low=mid+1; else if(target&lt;array[i][mid]) high=mid-1; else return true; &#125; &#125; return false; &#125;&#125; 最后要说的是：博客源码 ， 欢迎 star]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>剑指Offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java抽象类和接口]]></title>
    <url>%2Farchives%2Ff3ed7a5d.html</url>
    <content type="text"><![CDATA[InterruptedException中断故障(异常) Java抽象类和接口抽象类抽象类是用来描述抽象行为的，比如Animal，我们不知道Animal具体有会有什么样的行为，只有具体的动物类，如Dog，Cat才有具体的行为，才能够被实例化。抽象类是实现多态的一种机制，它可以包含具体方法（有具体实现的方法），也可以包含抽象方法，而继承它的子类必须实现这些方法，下面总结了一下抽象类的特性： 抽象类不能被实例化，但可以有构造函数 抽象方法必须由子类进行重写 只要包含一个抽象方法的类，就必须定义为抽象类，不管是否还包含其他方法 抽象类中可以包含具体的方法，也可以不包含抽象方法 抽象类可以包含普通成员变量，其访问类型可以任意 抽象类也可以包含静态成员变量，其访问类型可以任意 子类中的抽象方法不能与父类的抽象方法同名 abstract不能与private、static、final或native并列修饰同一个方法 下面通过一个实例类来说明抽象类的使用 123456789101112131415161718192021222324252627282930313233// 抽象类Animal，包含了一个抽象方法cryabstract class Animal&#123; public abstract void cry(); &#125;// 子类Dog继承的抽象类Animal，必须实现其抽象方法cryclass Dog extends Animal&#123; public void cry() &#123; System.out.println("Dog cry"); &#125;&#125;// 同样，子类Cat继承的抽象类Animal，必须实现其抽象方法cryclass Cat extends Animal&#123; public void cry() &#123; System.out.println("Cat cry"); &#125;&#125;public class Test &#123; public static void main(String[] args) &#123; Animal a1 = new Dog(); // 抽象类引用指向子类实例 Animal a2 = new Cat(); a1.cry(); a2.cry(); &#125;&#125; 输出结果如下：12Dog cryCat cry 由输出结果可以知道，使用a1,a2调用cry方法调用的是子类的cry方法，这是动态绑定，是实现多态的一种机制。 接口接口在Java当中是通过关键字interface来实现，接口不是类，所以也不能被实例化，接口是用来建立类与类之间的协议，它的提供的只是一种形式，而没有具体的实现。实现类实现(implements)接口，必须实现接口的全部方法 接口是抽象类的延伸，Java不允许多重继承（即不能有多个父类，只能有一个），但可以实现多个接口。在使用接口的过程中，就注意以下几个问题： 接口中不能有构造方法。 接口的所有方法自动被声明为public，而且只能为public，如果使用protected、private，会导致编译错误。 接口可以定义”成员变量”，而且会自动转为public final static，即常量，而且必须被显式初始化。 接口中的所有方法都是抽象方法，不能包含实现的方法，也不能包含静态方法 实现接口的非抽象类必须实现接口的所有方法，而抽象类不需要 不能使用new来实现化接口，但可以声明一个接口变量，它必须引用一个实现该接口的类的对象，可以使用instanceOf来判断一个类是否实现了某个接口，如if (object instanceOf ClassName){doSth()}; 在实现多接口的时候一定要注意方法名的重复 抽象类与接口的区别语法层次抽象类的定义，如下所示： 12345678// 抽象类中可以包含抽象方法与非抽象方法（必须给出实现）public abstract class Demo &#123; abstract void foo1(); void foo2()&#123; //实现 &#125;&#125; 接口的定义，如下所示： 123456interface Demo&#123; // 接口中的方法自动转为public abstract void foo1(); void foo2();&#125; 抽象类方式中，抽象类可以拥有任意范围的成员数据，同时也可以拥有自己的非抽象方法，但是接口方式中，它仅能够有静态、不能修改的成员数据（即final static，但是我们一般是不会在接口中使用成员数据），同时它所有的方法都必须是抽象的。在某种程度上来说，接口是抽象类的特殊化。 设计层次从设计的层面来看，我觉得抽象类与接口有如下几个不同点： 抽象层次不同。可以这样理解，抽象类是对类的抽象，接口是对行为的抽象。抽象类对是类整体进行抽象，包括属性、行为，而接口是对类局部（行为）进行抽象。 跨域不同。抽象类所跨域的是具有相似特点的类，而接口可以跨域不同的类。抽象类所体现的是一种继承关系，要想使得继承关系合理，父类和派生类之间必须 存在”is-a” 关系 ，即父类和派生类在概念本质上应该是相同的。对于接口则不然，并不要求接口的实现者和接口定义在概念本质上是一致的， 仅仅是实现了接口定义的契约而已。 总结 抽象类在java语言中所表示的是一种继承关系，一个子类只能存在一个父类，但是可以存在多个接口。 在抽象类中可以拥有自己的成员变量和非抽象类方法，但是接口中只能存在静态的不可变的成员数据（不过一般都不在接口中定义成员数据），而且它的所有方法都是抽象的。 抽象类和接口所反映的设计理念是不同的，抽象类所代表的是”is-a”的关系，而接口所代表的是”like-a”的关系。抽象类和接口是java语言中两种不同的抽象概念，他们的存在对多态提供了非常好的支持，虽然他们之间存在很大的相似性。但是对于他们的选择往往反应了您对问题域的理解。只有对问题域的本质有良好的理解，才能做出正确、合理的设计。 以上内容源自： 作者：AlvinL链接：http://www.jianshu.com/p/2b5a9bdcd25f來源：简书著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 牛客网：优先选用接口，尽量少用抽象类 接口和抽象类都是继承树的上层，他们的共同点如下： 都是上层的抽象层。 都不能被实例化 都能包含抽象的方法，这些抽象的方法用于描述类具备的功能，但是不比提供具体的实现。 他们的区别如下： 在抽象类中可以写非抽象的方法，从而避免在子类中重复书写他们，这样可以提高代码的复用性，这是抽象类的优势；接口中只能有抽象的方法。 一个类只能继承一个直接父类，这个父类可以是具体的类也可是抽象类；但是一个类可以实现多个接口。 jsp中静态include和动态include 静态导入（include指令）通过file属性指定被包含的文件，并且file属性不支持任何表达式；动态导入（include动作）通过page属性指定被包含的文件，且page属性支持JSP表达式； 使用静态导入（include指令）时，被包含的文件内容会原封不动的插入到包含页中，然后JSP编译器再将合成后的文件最终编译成一个 Java文件；使用动态导入（include动作）包含文件时，当该标识被执行时，程序会将请求转发（不是请求重定向）到被包含的页面，并将执行结果输出 到浏览器中，然后返回包含页继续执行后面的代码。因为服务器执行的是多个文件，所以JSP编译器会分别对这些文件进行编译； 使用include静态指令包含文件时，由于被包含的文件最终会生成一个文件，所以在被包含、包含文件中不能有重名的变量或方法；而include动态包含文件时，由于每个文件是单独编译的，所以在被包含文件和包含文件中重名的变量和方法是不相冲突的。 静态导入是将被导入页面的代码完全融入，两个页面融合成一个整体Servlet，因此被导入页面甚至不需要是一个完整的页面；而动态导入则在Servlet中使用include方法来引入被导入页面的内容； 静态导入时被导入页面的编译指令会起作用；而动态导入时被导入页面的编译指令则失去作用，只是插入被导入页面的body内容。 一个以.java为后缀的源文件，只能有一个与文件名相同的public类，可以包含其他非public类（不考虑内部类）java中true ,false , null在java中不是关键字，也不是保留字，它们只是显式常量值，但是你在程序中不能使用它们作为标识符。其中const和goto是java的保留字。java中所有的关键字都是小写的，还有要注意true,false,null, friendly，sizeof不是java的关键字,但是你不能把它们作为java标识符用。 最后要说的是：博客源码 ， 欢迎 star]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>InterruptedException</tag>
        <tag>抽象类</tag>
        <tag>接口</tag>
        <tag>jsp include</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Coding+DaoCloud持续集成]]></title>
    <url>%2Farchives%2F93b24b01.html</url>
    <content type="text"><![CDATA[在上一篇博客中，按照步骤做完，就可以在本机发布博客到coding了，但这样我们只能用这一台电脑发博客，并且最大的问题是备份问题，如果这台电脑挂了，或者是误删文件，就很可能丢失了blog源文件。如果没有备份，就要gg了。 想要备份源文件，同时在多终端发布博客，最好的实现方式是找一个持续集成工具，在coding的帮助文档里有一些介绍，我试过一些，最后选择DaoCloud来集成coding仓库，原因还是免费。 这里的原理是，在coding的blog仓库中建立两个分支，分别是master和coding-pages。master分支存放blog源文件，即本地的hexo文件内容；coding-pages分支存放博客的全部静态页面，也就是blog\public文件夹中的内容。 操作的过程中，顺序是 git push 提交本地blog源文件到master分支 DaoCloud检测到master分支有提交内容，按照设置好的安装环境，生成博客，部署博客到coding-pages分支 这样在部署过一次之后，如果不再修改样式和主题等其他内容，只是提交博客，可以直接登录coding.net网站，将编辑好的md文件添加到master分支的blog/source/_post文件夹下，就完了。 如果要同步本地备份，也只用把master分支内容clone到本地就行了。 现在开始正式的操作。 创建新分支登录到coding官网中，可以继续保留上一篇博客中创建的仓库，先清空仓库（在项目设置里的仓库设置），然后在分支管理处新建分支coding-pages。 创建SSH Key文件夹由于上一篇博客《github+hexo搭建个人博客》中已经创建了SSH key，所以这里可以直接使用，在/blog/根目录下创建文件夹.daocloud，这里前面有.的文件夹不能直接创建，可以先直接创建aaa，再用命令行修改名字。此处右键打开git bash，1mv aaa .daocloud 然后把之前生成的SSH key复制到这个文件夹下1cp ~/.ssh/id_rsa* .daocloud/ 然后在.daocloud下新建文件ssh_config1touch ssh_config 打开ssh_config,输入12StrictHostKeyChecking noUserKnownHostsFile /dev/null 保存。 编辑Dockerfile在本地仓库blog根目录下新建文件名为Dockerfile的文件（没有后缀）,打开编辑内容如下，原因稍后再说： 1234567891011121314151617# DockerfileFROM node:slimMAINTAINER xxx &lt;xxx@xxx.com&gt;# 安装git、ssh等基本工具RUN apt-get update &amp;&amp; apt-get install -y git ssh-client ca-certificates --no-install-recommends &amp;&amp; rm -r /var/lib/apt/lists/*# 设置时区RUN echo &quot;Asia/Shanghai&quot; &gt; /etc/timezone &amp;&amp; dpkg-reconfigure -f noninteractive tzdataRUN npm install -g cnpm --registry=https://registry.npm.taobao.org# 只安装Hexo命令行工具，其他依赖项根据项目package.json在持续集成过程中安装RUN cnpm install hexo-cli -g# install hexo serverRUN cnpm install hexo-serverEXPOSE 4000 这里只用替换上你自己的coding用户名和邮箱就行，别的不变。 修改本地_config.yml配置因为要把博客部署到coding-pages分支，所以要修改deploy参数，把_config.yml的deploy修改如下1234deploy: type: git repo: coding: git@git.coding.net:xxx/xxx.git,coding-pages coding后面替换为你自己的coding仓库地址，加上coding-pages分支。 DaoCloud创建项目和配置这里是因为DaoCloud系统有过升级改版，网上搜到的DaoCloud操作教程几乎都是去年12月以前的，所以有些对现在的版本不太适用，我自己借助旧的教程和部署AppVeyor的经验改动了一些，适用现在的DaoCloud。 下面先登录DaoCloud官网，用github或者coding账号直接登录，在个人设置中绑定github和coding，然后在控制台新建项目，项目名随便取。 选择成功构建后设置 latest 为镜像标签，然后点击镜像：ci-hexo（这里是我的名字，你就点你自己相应的），复制镜像地址，先记在一边等下要用。这里安利一个剪贴板管理软件Ditto。 然后打开流程定义，点击右侧通过 yaml 快捷编辑，打开后只一个脚本编辑页面，直接把以下内容复制进去： 123456789101112131415161718192021222324252627282930313233343536373839404142version: 3image: ubuntu:16.04stages:- build- test构建任务: stage: build job_type: image_build only: branches: - master build_dir: / cache: true dockerfile_path: /Dockerfile测试任务: stage: test job_type: test only: branches: - master pull_request: false before_script: - mkdir ~/.ssh - mv .daocloud/id_rsa ~/.ssh/id_rsa - mv .daocloud/ssh_config ~/.ssh/config - chmod 600 ~/.ssh/id_rsa - chmod 600 ~/.ssh/config - eval $(ssh-agent) - ssh-add ~/.ssh/id_rsa - rm -rf .daocloud - git config --global user.name &quot;xxxxx&quot; #这里填你的coding用户名 - git config --global user.email &quot;xxxxx@xxxxx.com&quot; #这里填你的coding邮箱 image: xxxxx:latest #这里填你的镜像url，不要覆盖latest install: - cnpm install - cnpm install --save hexo-generator-feed - cnpm install hexo-baidu-url-submit --save script: - hexo clean - hexo g - hexo d - rm -rf ~/.ssh/ 只用修改3处位置，其他的不要动，然后点击更新。 到了这里就快完成了，还差一点，在流程定义这里点击构建任务，修改触发条件为分支-master-执行任务，测试任务也修改触发条件为分支-master-执行任务。 git关联远程库和提交代码先在所有的配置就做完了，可以回到blog文件夹下，将本地仓库push到远程库就行了。现在介绍一下如何关联远程库，以及commit和push操作。 关联远程库，这里后面xxx是你的仓库url 1git remote add coding xxxxxx 添加代码到本地仓库和commit信息，这里xxx是你的commit信息，可以随便写 12git add .git commit -m &quot;xxx&quot; push到远程库的master分支 1git push -u coding master 如果提示有冲突，就把-u改为-f，反正仓库里都是自己的东西，force push也没什么大问题。 结束现在就完成了全部的操作，其实并不复杂，Dockerfile之预编译文件，放在仓库master分支中，每当仓库master分支有新的提交时，DaoCloud会监测到变化，由于触发条件的设置，就开始了构建任务和测试任务，构建任务阶段是编译Dockerfile文件中的脚本，其实大致意思很好懂，就是配置git，node和hexo环境。然后进行测试任务，就是进行刚才编辑的yaml编辑器中的脚本，其中有一段测试任务，逻辑就是安装必要的hexo包，然后复制coding仓库里的ssh密匙，用你配置的账号和邮箱，进行hexo操作，清除缓存，生成文件，部署到coding-pages分支，然后删除ssh密匙（安全性）。 虽然整个操作过程有暴露私有ssh key的风险，但说实话，git的初衷不就是为了分享代码吗，况且我们写的这些真实价值并没有多少，所以放心的使用吧，coding提供的是私有仓库还是可以放心，github进行进行相同操作就改一下配置也可以最后删掉ssh key。 coding+DaoCloud持续集成到此结束，有问题可以下方留言评论，或者很急的话发邮件提醒我也可以，邮箱在about me里。 另外还有 《Coding+Hexo搭建个人博客》 《Github+Appveyor博客云端持续集成》 《hexo博客部署到Github》 最后要说的是：博客源码 ， 欢迎 star]]></content>
      <categories>
        <category>Solution</category>
      </categories>
      <tags>
        <tag>CI</tag>
        <tag>Coding</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Github+Hexo+AppVeyor个人博客搭建和持续集成]]></title>
    <url>%2Farchives%2F3a153972.html</url>
    <content type="text"><![CDATA[Github+Hexo+AppVeyor个人博客搭建和持续集成我们都知道github pages提供了静态网页的自动解析，于是我们想到用github pages展示个人项目，特别是前端工程师，完全可以拿github展示个人项目设计。除此之外，我们也可以拿这个写个人博客，但是有一点，我们很难把博客写成html的静态页面吧。虽然可以强行实现，但是每添加一篇blog，就需要添加很多指向其他页面的链接，还有很多样式，光秃秃的页面总归不美。 有需求就有答案，这里有一种方法是，用一种博客框架来实现将编辑好的博客文本自动生成静态页面，这样我们就只用将html文件加入repository就好。有一个基于Node.js的轻量级的框架Hexo，就可以实现把markdown文件生成静态页面并发布到github仓库的功能，本文的内容从这里开始。 由于有不同的终端机，每次在不同平台去同步博客特别麻烦，所以想到持续集成的方式。在Github上同步管理我的blog源码，由第三方平台进行持续集成构建博客的html文件，并发布到gh-pages。搜一搜解决方案还不少，Travis，Appveyor和DaoCloud是我使用过的免费平台。这里只说Appveyor。 （时隔近一年，继续编辑这篇post）这里只贴出AppVeyor的官方文档和用于hexo构建的yaml脚本。 文档地址：https://www.appveyor.com/docs/ 环境变量设定： 构建脚本：1234567891011121314151617181920212223242526272829303132clone_depth: 5environment: access_token: secure: ***********************************************install: - ps: Install-Product node '' - node --version - npm --version - npm install - npm install hexo-cli -g # and other package you needbuild_script: - hexo generateartifacts: - path: publicon_success: - git config --global credential.helper store - ps: Add-Content "$env:USERPROFILE\.git-credentials" "https://$($env:access_token):x-oauth-basic@github.com`n" - git config --global user.email "%GIT_USER_EMAIL%" - git config --global user.name "%GIT_USER_NAME%" - git clone --depth 5 -q --branch=%TARGET_BRANCH% %STATIC_SITE_REPO% %TEMP%\static-site - cd %TEMP%\static-site - del * /f /q - for /d %%p IN (*) do rmdir "%%p" /s /q - SETLOCAL EnableDelayedExpansion &amp; robocopy "%APPVEYOR_BUILD_FOLDER%\public" "%TEMP%\static-site" /e &amp; IF !ERRORLEVEL! EQU 1 (exit 0) ELSE (IF !ERRORLEVEL! EQU 3 (exit 0) ELSE (exit 1)) - git add -A - if "%APPVEYOR_REPO_BRANCH%"=="master" if not defined APPVEYOR_PULL_REQUEST_NUMBER (git diff --quiet --exit-code --cached || git commit -m "Update Static Site" &amp;&amp; git push origin %TARGET_BRANCH% &amp;&amp; appveyor AddMessage "Static Site Updated") 好的，大致就这么些，如果还有问题，可在评论中提出。]]></content>
      <categories>
        <category>Solution</category>
      </categories>
      <tags>
        <tag>CI</tag>
        <tag>Github</tag>
        <tag>AppVeyor</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Coding+Hexo搭建个人博客]]></title>
    <url>%2Farchives%2F9997094b.html</url>
    <content type="text"><![CDATA[好了，答应了好几个评论的朋友要写一篇教程，不多说，我们直接进入主题。 我搭建博客用的是hexo框架，因为这个框架比较简单轻便，而且依赖于node.js管理包文件，我以前用过也用过一些npm的内容，所以选了这个。这里提一句，基于coding或者github搭建的博客都是静态页面，轻量简洁，相对的功能上不如Wordpress那样强大，但是我们也可以用第三方插件实现文章统计，网站计数，博客评论等功能，看自己喜好加吧。 好了，现在我们开始操作。 step 1 安装环境环境有三，node git hexo，hexo最后装。 git直接去下官网最新版本，安装步骤就不停next就好，不放心的话搜一下百度知道，有几个步骤需要斟酌，不过影响不大。链接在这里git官网。 node也是官网最新版本，node.js中文官网现在好像是8.x了，这个就一直next安装，装好后在桌面打开cmd， npm是node.js集成的包管理工具，现在是直接随node装好了。 查看版本号，成功显示就能用了，如果显示不是可用的命令就需要手动添加node环境变量，这个也简单，百度知道全有。 hexo在桌面右键，git bash，然后输入1npm install -g cnpm --registry=https://registry.npm.taobao.org 这里用cnpm是因为npm连接不太稳定，我用npm也是装了两次hexo才成功。而cnpm是淘宝团队提供的一个npm镜像库，国内访问非常快，以后的npm命令就在前面加一个c，使用方法完全相同。然后 1cnpm install hexo-cli -g 在某个盘下新建一个文件夹，取名随意，我是blog。 然后在这个文件夹下右键，git bash，然后 1hexo init 即在此初始化hexo源文件，需要这个blog文件夹初始为空。然后 1cnpm install 这一步是安装通用的npm包文件，如果有特定的npm包需要额外添加。我们除了通用包，还要一个hexo部署博客的包文件， 1cnpm install hexo-deployer-git --save 现在需要的基础包就安装好了。 step 2 测试本地发布现在新建一个博客，在blog文件夹下右键，git bash，然后输入 1hexo new test 这一步是在生成一篇空博客marksown文档，存在blog\source\_post路径下，可以用编辑器打开它，我用的是sublime，装了markdown editing和markdown preview插件，或者其他md编辑器都行，在test.md中随便写点什么，然后保存。接下来1hexo g 这一步是hexo的核心，把md文件转为静态页面，并添加主题样式和必要的链接，生成的文件在public下。然后 1hexo s 这样，就是在本地预览博客，在浏览器地址栏中输入 1http://localhost:4000 就可以看到结果了。现在只能自己浏览，想要让其他人也能看到，就需要部署到服务器上。租服务器不仅要花费，还要自己搭建web环境，太麻烦了，而且不适合学生党和技术不够的同学们，万幸github和coding都提供了静态页面解析的功能，所以我们把public文件夹下的内容push到一个git远程仓库就可以了。现在我们需要开始发布到coding的步骤。 step 3 本地博客部署到coding首先，去官网登陆你的coding账号，没有就注册一个，然后完善个人信息，升级到银牌会员（才能绑定个人域名）。然后新建一个repository，项目名称就填你的用户名，选择私有，然后创建项目。 现在有了远程仓库，就要把本地仓库和远程仓库关联起来，首先在blog目录下git bash，然后输入1git config -l 查看你的git配置信息，像我的是这样123456789101112131415161718192021222324252627$ git config -lcore.symlinks=falsecore.autocrlf=truecore.fscache=truecolor.diff=autocolor.status=autocolor.branch=autocolor.interactive=truehelp.format=htmlrebase.autosquash=truehttp.sslcainfo=C:/Program Files/Git/mingw64/ssl/certs/ca-bundle.crtdiff.astextplain.textconv=astextplainfilter.lfs.clean=git-lfs clean -- %ffilter.lfs.smudge=git-lfs smudge -- %ffilter.lfs.required=truefilter.lfs.process=git-lfs filter-processcredential.helper=manageruser.name=xxxuser.email=xxx@xxx.comcore.repositoryformatversion=0core.filemode=falsecore.bare=falsecore.logallrefupdates=truecore.symlinks=falsecore.ignorecase=truegui.wmstate=normalgui.geometry=841x483+343+178 189 218 这里你只用关注的是这两行12user.name=xxxuser.email=xxx@xxx.com 如果你没有这两行，那么你需要添加配置：1git config --global user.email &quot;your email&quot; 和1git config --global user.name &quot;your name&quot; 将双引号中内容替换为你自己的coding用户名和邮箱，可以在coding个人设置中查看自己的用户名和邮箱。 然后我们给本地添加一个SSH key，这样的话每次部署就不用输密码。在git bash中输入1ssh-keygen -t rsa -b 4096 -C &quot;your email&quot; 成功会出现以下代码：123# Creates a new ssh key, using the provided email as a label# Generating public/private rsa key pair.Enter file in which to save the key (/Users/you/.ssh/id_rsa): [Press enter] // 推荐使用默认地址,如果使用非默认地址可能需要配置 .ssh/config 然后一直回车，回车，回车，然后在 Coding.net 添加公钥本地打开 id_rsa.pub 文件（一般在c盘用户文件夹下，进入你的用户文件夹，有一个.ssh文件，打开其中的id_rsa.pub ），复制其中全部内容，添加到Coding账户“SSH 公钥”页面 中，公钥名称可以随意起名字。完成后点击“添加”，然后输入密码或动态码即可添加完成。这里要注意是账户的SSH公匙，而不是项目中的设置的部署公匙，切记。 现在验证一下是否添加SSH公匙成功，在git bash中输入1ssh -T git@git.coding.net 如果成功，会出现以下代码 123Are you sure you want to continue connecting (yes/no)? yes Warning: Permanently added ‘git.coding.net,61.146.73.68’ (RSA) to the list of kn own hosts.Enter passphrase for key ‘/c/Users/xxx/.ssh/id_rsa’: Coding.net Tips : [ Hello xxx! You have connected to Coding.net by SSH successfully! ] 现在就已经添加好了公匙，我们离博客部署到coding只差一步。 step 4 部署博客到coding首先打开blog文件夹下的_config.yml文件，这是我的配置，你需要修改的地方我都加了注释，别的不要动，还有就是要注意这里yml文件是用的yaml脚本语言，对语法要求很严格，每个:后面要加上空格，没空格会编译出错。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788# Hexo Configuration## Docs: https://hexo.io/docs/configuration.html## Source: https://github.com/hexojs/hexo/# Sitetitle: Yanss&apos;s Blog #改为你自己的网站名subtitle:description: Winner Winner, Chicken Dinner #改为你自己的描述语句，随便写author: Yanss #改成你自己的名字language: timezone: Asia/Shanghai# URL## If your site is put in a subdirectory, set url as &apos;http://yoursite.com/child&apos; and root as &apos;/child/&apos;url: https://yanss.top #url改成&quot;xxx.coding.me&quot;，xxx是你的仓库名，也是你的用户名root: /permalink: :year/:month/:day/:title/permalink_defaults:# Directorysource_dir: sourcepublic_dir: publictag_dir: tagsarchive_dir: archivescategory_dir: categoriescode_dir: downloads/codei18n_dir: :langskip_render:# Writingnew_post_name: :title.md # File name of new postsdefault_layout: posttitlecase: false # Transform title into titlecaseexternal_link: true # Open external links in new tabfilename_case: 0render_drafts: falsepost_asset_folder: falserelative_link: falsefuture: truehighlight: enable: true line_number: true auto_detect: false tab_replace: # Category &amp; Tagdefault_category: uncategorizedcategory_map:tag_map:# Date / Time format## Hexo uses Moment.js to parse and display date## You can customize the date format as defined in## http://momentjs.com/docs/#/displaying/format/date_format: YYYY-MM-DDtime_format: HH:mm:ss# Pagination## Set per_page to 0 to disable paginationper_page: 10pagination_dir: page# Extensions## Plugins: https://hexo.io/plugins/## Themes: https://hexo.io/themes/theme: next #这里是主题名，你的先不要变，后面换主题再改# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy: type: git repo: git@github.com:yanss/yanss.github.io.git #repo这里写你的仓库地址，下面告诉怎么找 branch: master #就一个分支就不用改，默认master#这个feed是添加RSS订阅用的，这里你没有暂时不用写feed: type: atom path: atom.xml limit: 20 hub: content: #这个search是站内搜索的，也是没有就不用写search: path: search.xml field: post format: html limit: 10000 打开coding网站上你刚才创建的仓库，点击代码，左下角选择SSH方式访问仓库，复制那个链接，把它填到你的_config.yml的repo那里。 现在我们就可以开始部署博客了，记得部署之前最好清理一遍public文件夹,也就是这样 123hexo cleanhexo ghexo d 或者你也可以直接 1hexo d -g coding+hexo的博客部署操作就是这些了，有问题可以下方留言评论，或者很急的话发邮件提醒我也可以，邮箱在about me里。 另外还有 《Coding+DaoCloud持续集成》 《Github+Appveyor博客云端持续集成》 《hexo博客部署到Github》 最后要说的是：博客源码 ， 欢迎 star]]></content>
      <categories>
        <category>Solution</category>
      </categories>
      <tags>
        <tag>Coding</tag>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sql基础语句]]></title>
    <url>%2Farchives%2F929f9824.html</url>
    <content type="text"><![CDATA[选择：1select * from table1 where 范围 插入：1insert into table1(field1,field2) values(value1,value2) 删除：1delete from table1 where 范围 更新：1update table1 set field1=value1 where 范围 查找：12select * from table1 where field1 like ’%value1%’ //like的语法很精妙，查资料!&apos; 排序：1select * from table1 order by field1,field2 [desc] 总数：1select count as totalcount from table1 求和：1select sum(field1) as sumvalue from table1 平均：1select avg(field1) as avgvalue from table1 最大：1select max(field1) as maxvalue from table1 最小：1select min(field1) as minvalue from table1 最后要说的是：博客源码 ， 欢迎 star]]></content>
      <categories>
        <category>SQL</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JS正则表达式]]></title>
    <url>%2Farchives%2Fbf9092f5.html</url>
    <content type="text"><![CDATA[正则表达式（英语：Regular Expression，在代码中常简写为regex、regexp或RE）使用单个字符串来描述、匹配一系列符合某个句法规则的字符串搜索模式。搜索模式可用于文本搜索和文本替换。 语法1/正则表达式主体/修饰符(可选) 实例：1var patt = /abc/i /abc/i是一个正则表达式 abc 是一个正则表达式主体（用于检索） i是一个修饰符（搜索不区分大小写） 使用字符串方法search() 方法 用于检索字符串中指定的子字符串，或检索与正则表达式相匹配的子字符串，并返回子串的起始位置。 实例 使用正则表达式搜索”Runoob”字符串，且不区分大小写：123456789101112131415161718192021&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;meta charset=&quot;utf-8&quot;&gt;&lt;title&gt;菜鸟教程(runoob.com)&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;p&gt;搜索字符串 &quot;runoob&quot;, 并显示匹配的起始位置：&lt;/p&gt;&lt;button onclick=&quot;myFunction()&quot;&gt;点我&lt;/button&gt;&lt;p id=&quot;demo&quot;&gt;&lt;/p&gt;&lt;script&gt;function myFunction() &#123; var str = &quot;Visit Runoob!&quot;; var n = str.search(/Runoob/i); document.getElementById(&quot;demo&quot;).innerHTML = n;&#125;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 输出结果为：16 replace() 方法 用于在字符串中用一些字符替换另一些字符，或替换一个与正则表达式匹配的子串。 实例 使用正则表达式且不区分大小写将字符串中的 Microsoft 替换为 Runoob : 123456789101112131415161718192021&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;meta charset=&quot;utf-8&quot;&gt;&lt;title&gt;菜鸟教程(runoob.com)&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;p&gt;替换 &quot;microsoft&quot; 为 &quot;Runoob&quot; :&lt;/p&gt;&lt;button onclick=&quot;myFunction()&quot;&gt;点我&lt;/button&gt;&lt;p id=&quot;demo&quot;&gt;请访问 Microsoft!&lt;/p&gt;&lt;script&gt;function myFunction() &#123; var str = document.getElementById(&quot;demo&quot;).innerHTML; var txt = str.replace(/microsoft/i,&quot;Runoob&quot;); document.getElementById(&quot;demo&quot;).innerHTML = txt;&#125;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 结果输出为：1Visit Runoob! 使用RegExp对象 在 JavaScript 中，RegExp 对象是一个预定义了属性和方法的正则表达式对象。 test() 方法 用于检测一个字符串是否匹配某个模式。如果字符串中有匹配的值返回 true ，否则返回 false。 语法: 1RegExpObject.test(string) 实例： 1234567891011121314&lt;script&gt;var str=&quot;Hello world!&quot;;//look for &quot;Hello&quot;var patt=/Hello/g;var result=patt.test(str);document.write(&quot;Returned value: &quot; + result); //look for &quot;W3CSchool&quot;patt=/W3CSchool/g;result=patt.test(str);document.write(&quot;&lt;br&gt;Returned value: &quot; + result);&lt;/script&gt; 输出：12Returned value: trueReturned value: false exec() 方法 用于检索字符串中的正则表达式的匹配。如果字符串中有匹配的值返回该匹配值，否则返回 null。 语法：1RegExpObject.exec(string) 实例：1234567891011121314&lt;script&gt;var str=&quot;Hello world!&quot;;//look for &quot;Hello&quot;var patt=/Hello/g;var result=patt.exec(str);document.write(&quot;Returned value: &quot; + result); //look for &quot;W3Schools&quot;patt=/W3Schools/g;result=patt.exec(str);document.write(&quot;&lt;br&gt;Returned value: &quot; + result);&lt;/script&gt; 输出：12Returned value: HelloReturned value: null 正则表达式修饰符 修饰符 描述 i 执行对大小写不敏感的匹配 g 执行全局匹配 m 执行多行匹配 正则表达式模式方括号用于查找某个范围内的字符 表达式 描述 [abc] 查找方括号之间的任何字符 [0-9] 查找任何从0至9的数字 (x\ y) 查找任何以\ 分隔的选项 元字符是拥有特殊含义的字符 元字符 描述 \d 查找数字 \s 查找空白字符 \b 匹配单词边界 \uxxxx 查找以十六进制数xxxx规定的Unicode字符 量词 量词 描述 n+ 匹配任何包含至少一个n的字符串 n* 匹配任何包含零个或多个n的字符串 n? 匹配任何包含零个或一个n的字符串 最后要说的是：博客源码 ， 欢迎 star]]></content>
      <categories>
        <category>Front-End</category>
      </categories>
      <tags>
        <tag>JavaScrit</tag>
        <tag>regexp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SSM框架各层关系]]></title>
    <url>%2Farchives%2F457836a3.html</url>
    <content type="text"><![CDATA[SSM包含持久层Dao和Model，业务层Service和ServiceImpl，表现层Controller和View，本文讲述这些不同层的作用和关系。 持久层：Dao层（mapper） DAO层主要是做数据持久层的工作，负责与数据库进行联络的一些任务都封装在此 DAO层的设计首先是设计DAO的接口 然后在Spring的配置文件中定义此接口的实现类 然后就可在模块中调用此接口来进行数据业务的处理，而不用关心此接口的具体实现类是哪个类，显得结构非常清晰 DAO层的数据源配置，以及有关数据库连接的参数都在Spring的配置文件中进行配置 业务层：Service层 Service层主要负责业务模块的逻辑应用设计 首先设计接口，再设计其实现的类 接着再在Spring的配置文件中配置其实现的关联。这样我们就可以在应用中调用Service接口来进行业务处理 Service层的业务实现，具体要调用到已定义的DAO层的接口 封装Service层的业务逻辑有利于通用的业务逻辑的独立性和重复利用性，程序显得非常简洁 表现层：Controller层（Handler） Controller层负责具体的业务模块流程的控制 在此层里面要调用Service层的接口来控制业务流程 控制的配置也同样是在Spring的配置文件里面进行，针对具体的业务流程，会有不同的控制器，我们具体的设计过程中可以将流程进行抽象归纳，设计出可以重复利用的子单元流程模块，这样不仅使程序结构变得清晰，也大大减少了代码量 View层 View层：此层与控制层结合比较紧密，需要二者结合起来协同工发。View层主要负责前台jsp页面的表示. 各层联系 DAO层，Service层这两个层次都可以单独开发，互相的耦合度很低，完全可以独立进行，这样的一种模式在开发大项目的过程中尤其有优势 Controller，View层因为耦合度比较高，因而要结合在一起开发，但是也可以看作一个整体独立于前两个层进行开发。这样，在层与层之前我们只需要知道接口的定义，调用接口即可完成所需要的逻辑单元应用，一切显得非常清晰简单 Service层是建立在DAO层之上的，建立了DAO层后才可以建立Service层，而Service层又是在Controller层之下的，因而Service层应该既调用DAO层的接口，又要提供接口给Controller层的类来进行调用，它刚好处于一个中间层的位置。每个模型都有一个Service接口，每个接口分别封装各自的业务处理方法 总结：view层：结合control层，显示前台页面 control层：业务模块流程控制，调用service层接口 service层：业务操作实现类，调用dao接口 dao层：数据业务处理，持久化操作 model层：pojo， or mapping，持久层 123456graph TDA[View]--&gt;B[Controller]B--&gt;C[Service]C--&gt;D[ServiceImpl]D--&gt;E[Dao]E--&gt;F[Mapping] 最后要说的是：博客源码， 欢迎 star]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Spring</tag>
        <tag>SpringMVC</tag>
        <tag>MyBatis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[树和二叉树]]></title>
    <url>%2Farchives%2F7b216a3b.html</url>
    <content type="text"><![CDATA[树树的定义 （递归）一棵树是一些节点的集合。这个集合可以是空集；若不是空集，则树由称作根的节点 r 以及 0 个或多个非空的（子）树 $T_1，T_2，···，T_k$ 组成，这些子树中每一棵的根都被来自根 r 的一条有向边所连结。 树的实现1234567//树节点的声明class TreeNode&#123; Object element; TreeNode firstChild; TreeNode netSibling;&#125; 将每个节点的所有儿子都放到树节点的链表中。 树的遍历 先序遍历 后序遍历 中序遍历 二叉树 二叉树（binary tree）是一棵树，其中每个节点都不能有多于两个的儿子。 二叉树平均深度为 $O(\sqrt{N})$，最大深度为 $N$。二叉查找树的平均深度为 $O(log N)$。 12345678//二叉树节点类class BinaryNode&#123; //Friendly data;accessible by other package toutines Object element;//The data in the node BinaryNode left;//Left child BinaryNode right;//right child&#125; 查找树ADT——二叉查找树 使二叉树成为查找树的性质是，对于树中的每个节点 X ，它的左子树中所有项的值小于 X 中的项，而它的右子树中所有项的值大于 X 中的项。 1234567891011121314//BinaryNode类private static class BinaryNode&lt;AnyType&gt;&#123; //Constructors BinaryNode(AnyType theElement) &#123;this(theElement, null, null);&#125; BinaryNode(AnyType theElement, BinaryNode&lt;AnyType&gt; lt, BinaryNode&lt;AnyType&gt; rt) &#123;element = theElement; left = lt; right = rt;&#125; AnyType element;//The data in the node BinaryNode&lt;AnyType&gt; left;//Left child BinaryNode&lt;AnyType&gt; right;//Right child&#125; 二叉查找树架构123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127//二叉查找树架构public class BinarySearchTree&lt;AnyType extends comparable&lt;? super AnyType&gt;&gt;&#123; private static class BinaryNode&lt;AnyType&gt; &#123; //Constructors BinaryNode(AnyType theElement) &#123;this(theElement, null, null);&#125; BinaryNode(AnyType theElement, BinaryNode&lt;AnyType&gt; lt, BinaryNode&lt;AnyType&gt; rt) &#123;element = theElement; left = lt; right = rt;&#125; AnyType element;//The data in the node BinaryNode&lt;AnyType&gt; left;//Left child BinaryNode&lt;AnyType&gt; right;//Right child &#125; private BinaryNode&lt;AnyType&gt; root; public BinarySearchTree() &#123; root = null; &#125; public void makeEmpty() &#123; root = null; &#125; public boolean isEmpty() &#123; return root == null; &#125; public boolean contains( AnyType x ) &#123; return contains( x, root ); &#125; public AnyType findMin() &#123; if (isEmpty()) throw new UnderflowException(); return findMin(root).element; &#125; public AnyType finMax() &#123; if (isEmpty()) throw new UnderflowException(); return finMax(roow).element; &#125; public void insert(AnyType x) &#123; root = insert(x,root); &#125; public void remove(AnyType x) &#123; root = remove(x,root); &#125; public void printTree() &#123; if (isEmpty()) System.out.println("Empty tree"); else printTree(root); &#125; private boolean contains(AnyType x, BinaryNode&lt;AnyType&gt; t) &#123; if (t == null) return false; int compareResult = x.compareTo(t.element); if(compareResult &lt; 0) return contains(x, t.left); else if(compareResult &gt; 0) return contains(x, t.right); else return true; //Match &#125; private BinaryNode&lt;AnyType&gt; findMin(BinaryNode&lt;AnyType&gt; t) &#123; if(t == null) return null; else if(t.left == null) return t; return findMin(t.left); &#125; private BinaryNode&lt;AnyType&gt; finMax(BinaryNode&lt;AnyType&gt; t) &#123; if(t != null) while(t.right != null) t = t.right; return t; &#125; private BinaryNode&lt;AnyType&gt; insert(AnyType x, BinaryNode&lt;AnyType&gt; t) &#123; if(t == null) return new BinaryNode&lt;&gt;(x, null, null); int compareResult = x.compareTo(t.element); if(compareResult &lt; 0) t.left = insert(x, t.left); else if(compareResult &gt; 0) t.right = insert(x, t.right); else ;//Duplicate; do nothing return t; &#125; private BinaryNode&lt;AnyType&gt; remove(AnyType x, BinaryNode&lt;AnyType&gt; t) &#123; if(t == null) return t;//Item not found; do nothing int compareResult = x.compareTo(t.element); if(compareResult &lt; 0) t.left = remove(x, t.left); else if(compareResult &gt; 0) t.right = remove(x, t.right); else if(t.left != null &amp;&amp; t.right != null)//Two children &#123; t.element = findMin(t.right).element; t.right = remove(t.element, t.right); &#125; else t = (t.left != null) ? t.left : t.right; return t; &#125; private void printTree(BinaryNode&lt;AnyType&gt; t) &#123; if (t != null) &#123; printTree(t.left); System.out.println(t.element); printTree(t.right); &#125; &#125;&#125; contains方法 如果树 $T$ 中含有项 $X$ 的节点，那么这个操作需要返回true，如果这样的节点不存在则返回false。树的结构使这种操作很简单。如果 $T$ 是空集，那么久返回false。否则，如果存储在 $T$ 处的项是 $X$ ，那么可以返回true。否则，我们对数 $T$ 的左子树或右子树进行一次递归调用，则依赖于 $X$ 与存储在 $T$ 中的项的关系。 123456789101112131415161718192021/** * Internal method to find an item in a subtree * @param x is item to search for. * @param t the node that roots the subtree. * @return true if the item is found; false otherwise. *///二叉查找树的contains操作private boolean contains(AnyType x, BinaryNode&lt;AnyType&gt; t) &#123; if (t == null) return false; int compareResult = x.compareTo(t.element); if(compareResult &lt; 0) return contains(x, t.left); else if(compareResult &gt; 0) return contains(x, t.right); else return true; //Match &#125; 123456//递归用while循环代替 while(compareResult &lt;0) &#123; t=t.left; compareResult = x.compareTo(t.element); &#125; 算法表达式的简明性是以速度的降低为代价的。 findMin方法和findMax方法 这两个方法分别返回树中包含最小元和最大元的节点的引用。为执行findMin，从根开始并且只要有左儿子就向左进行。 终止点就是最小的元素。findMax除分支朝向右儿子其余过程相同。 123456789101112131415161718192021222324252627//用递归编写findMin，用非递归编写findMax/*** Internal method to find the smallest item in a subtree* @param t the node that roots the subtree.* @return node containing the smallest item*/private BinaryNode&lt;AnyType&gt; findMin(BinaryNode&lt;AnyType&gt; t)&#123; if(t == null) return null; else if(t.left == null) return t; return findMin(t.left);&#125;/*** Internal method to find the largest item in a subtree* @param t the node that roots the subtree.* @return node containing the largest item.*/private BinaryNode&lt;AnyType&gt; finMax(BinaryNode&lt;AnyType&gt; t)&#123; if(t != null) while(t.right != null) t = t.right; return t; &#125; insert方法123456789101112131415161718192021/** * Internal method to insert into a subtree * @param x the item to insert * @param t the node that roots the subtree * @return the new root of the subtree */ private BinaryNode&lt;AnyType&gt; insert(AnyType x, BinaryNode&lt;AnyType&gt; t)&#123; if(t == null) return new BinaryNode&lt;&gt;(x, null, null); int compareResult = x.compareTo(t.element); if(compareResult &lt; 0) t.left = insert(x, t.left); else if(compareResult &gt; 0) t.right = insert(x, t.right); else ;//Duplicate; do nothing return t;&#125; remove方法1234567891011121314151617181920212223242526/** * Internal method to remove from a subtree * @param x the item to remove. * @param t the node that roots the subtree. * @return the new root of the subtree */private BinaryNode&lt;AnyType&gt; remove(AnyType x, BinaryNode&lt;AnyType&gt; t)&#123; if(t == null) return t;//Item not found; do nothing int compareResult = x.compareTo(t.element); if(compareResult &lt; 0) t.left = remove(x, t.left); else if(compareResult &gt; 0) t.right = remove(x, t.right); else if(t.left != null &amp;&amp; t.right != null)//Node that has two children &#123; t.element = findMin(t.right).element;//Find the minimum item of right subtree t.right = remove(t.element, t.right);//Remove the node of minimum item recursively &#125; else t = (t.left != null) ? t.left : t.right;//Node that has one children; parent of the node roots subtree of the node return t;&#125; 如果节点是树叶，可以直接删除。 如果节点有一个儿子，这该节点需要在其父节点调整自己的链以绕过该节点 如果节点有两个儿子，一般的删除策略是用其右子树的最小的数据代替该节点，并在右子树中递归地删除那个最小的节点 另外，如果删除的次数不多，通常使用的策略是懒惰删除（lazy deletion）：当一个元素要被删除时，它仍留在树中，而只是被标记为删除。 AVL树 AVL树是带有平衡条件的二叉查找树。这个平衡条件必须要容易保持，而且它保证树的深度须是 $O(log N)$ 。一个AVL树是其每个节点的左子树和右子树的高度最多差 1 的二叉查找树（空树的高度定义为 -1）。 可以知道，在高度为 $h$ 的AVL树中，最少节点数 $S(h)=S(h-1)+S(h-2)+1$ 给出。对于 $h=0, S(h)=1; h=1, S(h)=2$ 。函数 $S(h)$ 与斐波那契数密切相关。 那么重点来了，对于AVL树的插入操作，有可能破坏树的平衡性。这时候，我们就需要在这一步插入完成之前恢复平衡的性质。 可以知道，从插入的节点往上，逆行到根，若发生平衡信息改变，那么改变的节点一定在这条路径上。我们需要找出这个需要重新平衡的节点 $\alpha$ 。 对于节点 $\alpha$ ，不平衡条件可能出现在一下四种操作中： 对 $\alpha$ 的左儿子的左子树进行一次插入（LL）。 对 $\alpha$ 的左儿子的右子树进行一次插入（LR）。 对 $\alpha$ 的右儿子的左子树进行一次插入（RL）。 对 $\alpha$ 的右儿子的右子树进行一次插入（RR）。 对于1和4，是插入发生在外边的情况，通过对树的一次单旋转而完成调整。对于2和3，是插入发生在内部的情况，通过对树的一次双旋转而完成调整。 这里先对AvlNode类进行定义： 1234567891011121314private static class AvlNode&lt;AnyType&gt;&#123; //Constructors AvlNode(AnyType theElement) &#123;this(theElement, null, null);&#125; AvlNode(AnyType theElement, AvlNode&lt;AnyType&gt; lt, AvlNode&lt;AnyType&gt; rt) &#123;element = theElement; left = lt; right = rt; height = 0;&#125; AnyType element;//The data in the code AvlNode&lt;AnyType&gt; left;//Left child AvlNode&lt;AnyType&gt; right;//Right child int height;//Height&#125; 然后需要一个返回节点高度的方法： ​ 12345678//返回AVL树的节点高度/** * return the height of node t, or -1, if null. */private int height(AvlNode&lt;AnyType&gt; t)&#123; return t == null ? -1 : t.height;&#125; 单旋转 1234567891011121314151617/** * Rotate binary tree node with left child. * For AVL trees, this is a single rotation for case 1. * Update heights, then return new root. */private AvlNode&lt;AnyType&gt; RotationWithLeftChild(AvlNode&lt;AnyType&gt; k2) &#123; AVLTreeNode&lt;AnyType&gt; k1 = k2.left; k2.left = k1.right; k1.right = k2; k2.height = Math.max( height(k2.left), height(k2.right)) + 1; k1.height = Math.max( height(k1.left), k2.height) + 1; return k1; &#125; 1234567891011121314151617/** * Rotate binary tree node with right child. * For AVL trees, this is a single rotation for case 4. * Update heights, then return new root. */private AvlNode&lt;AnyType&gt; RotationWithRightChild(AvlNode&lt;AnyType&gt; k1) &#123; AVLTreeNode&lt;AnyType&gt; k2 = k1.right; k1.right = k2.left; k2.left = k1; k1.height = Math.max( height(k1.left), height(k1.right)) + 1; k1.height = Math.max( height(k2.right), k1.height) + 1; return k2; &#125; 双旋转 1234567891011/** * Double rotate binary tree node: first left child * with its right child; then node k3 with new left child. * For AVL trees, this is a double rotation for case 2. * Update heights, then return new root. */private AvlNode&lt;AnyType&gt; doubleWithLeftChild(AvlNode&lt;AnyType&gt; k3)&#123; k3.left = RotationWithRightChild(k3.left); return RotationWithLeftChild(k3);&#125; 1234567891011/** * Double rotate binary tree node: first right child * with its left child; then node k1 with new right child. * For AVL trees, this is a double rotation for case 3. * Update heights, then return new root. */private AvlNode&lt;AnyType&gt; doubleWithRightChild(AvlNode&lt;AnyType&gt; k1)&#123; k1.right = RotationWithRightChild(k1.right); return RotationWithLeftChild(k1);&#125; AVL树的插入方法 插入方法就是前文中的insert方法，只是在最后一行调用平衡的方法以保持AVL树的平衡性。123456789101112131415161718192021222324252627282930313233343536373839404142434445/** * Internal method to insert into a subtree. * @param x the item to insert. * @param t the node that roots the subtree. * @return the new root of the subtree. */private AvlNode&lt;AnyType&gt; insert(AnyType x, AvlNode&lt;AnyType&gt; t)&#123; if(t == null) return new AvlNode&lt;&gt;(x, null, null); int compareResult = x.compareTo(t.element); if(compareResult &lt; 0) t.left = insert(x, t.left); else if(compareResult &gt; 0) t.right = insert(x, t.right); else ;//Duplicate; do nothing return balance(t);&#125;private static final int ALLOWED_IMBALLANCE = 1;//Assume t is either balanced of within one of being balancedprivate AvlNode&lt;AnyType&gt; balance(AvlNode&lt;AnyType&gt; t)&#123; if(t == null) return t; if(height(t.left) - height(t.right) &gt; ALLOWED_IMBALLANCE) if(height(t.left.left) &gt;= height(t.left.right)) t = RotationWithLeftChild(t); else t = doubleWithLeftChild(t); else if(height(t.right) - height(t.left) &gt; ALLOWED_IMBALLANCE) if(height(t.right.right) &gt;= height(t.right.left)) t = RotationWithRightChild(t); else t = doubleWithRightChild(t); t.height = Math.max(height(t.left), height(t.right)) + 1; return t;&#125; AVL树的删除方法 和AVL树的插入一样，只用在前文的删除方法最后加上一行调用平衡的方法即可。 1234567891011121314151617181920private AvlNode&lt;AnyType&gt; remove(AnyType x, AvlNode&lt;AnyType&gt; t)&#123; if(t == null) return t;//Item not found; do nothing int compareResult = x.compareTo(t.element); if(compareResult &lt; 0) t.left = remove(x, t.left); else if(compareResult &gt; 0) t.right = remove(x, t.right); else if(t.left != null &amp;&amp; t.right != null)//Node that has two children &#123; t.element = findMin(t.right).element;//Find the minimum item of right subtree t.right = remove(t.element, t.right);//Remove the node of minimum item recursively &#125; else t = (t.left != null) ? t.left : t.right;//Node that has one children; parent of the node roots subtree of the node return balance(t);&#125; 最后要说的是：博客源码 ， 欢迎 star]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>BinaryTree</tag>
      </tags>
  </entry>
</search>
